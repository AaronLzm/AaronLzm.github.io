import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,f as n,b as o}from"./app-CYoxshh_.js";const p={};function i(r,e){return o(),t("div",null,[...e[0]||(e[0]=[n('<p>#Bayes</p><h3 id="gpt-content" tabindex="-1"><a class="header-anchor" href="#gpt-content"><span>GPT content:</span></a></h3><p>&quot;Bayes by Backprop&quot;的核心是使用变分推理来近似权重的后验分布。这一过程涉及到一些复杂的数学，但我会尽力将其分解为详细的步骤。</p><p>假设我们的神经网络的权重是 w，我们的数据是 D，那么在贝叶斯推理中，我们希望计算权重的后验分布 p(w|D)。然而，这通常是非常困难的，因为这需要计算一个涉及到所有可能权重值的积分，这在实际中是不可行的。因此，我们使用变分推理来近似这个后验分布。</p><p>在变分推理中，我们引入一个变分分布 q(w|θ)，这个分布是由参数θ描述的，通常我们假设这个分布是高斯分布。我们的目标是找到最优的参数θ*，使得q(w|θ*)最接近真实的后验分布p(w|D)。</p><p>为了找到这个最优的参数θ*，我们需要优化一个叫做变分下界（Variational Lower Bound）或者证据下界（Evidence Lower Bound，ELBO）的量。ELBO定义为：</p><p>ELBO(θ) = E_q[log p(D|w)] - KL(q(w|θ)||p(w))</p><p>其中，E_q[log p(D|w)]是在变分分布q(w|θ)下，模型对数据的对数似然的期望，KL(q(w|θ)||p(w))是变分分布q(w|θ)和先验分布p(w)之间的KL散度（Kullback-Leibler Divergence）。</p><p>我们的目标是最大化这个ELBO。为了实现这个，我们需要计算ELBO关于θ的梯度，然后使用一个优化算法（比如随机梯度下降）来更新θ。计算梯度的过程就是所谓的反向传播。</p><p>在计算了梯度并更新了θ后，我们就得到了新的权重分布q(w|θ)。然后，我们可以在下一次前向传播时，从这个新的分布中采样权重。</p><p>这个过程会反复进行，每次迭代都会使得我们的权重分布更接近真实的后验分布，也就是使得ELBO更大。最终，我们得到的不仅是一组能使得预测性能最优的权重，还有一个描述权重不确定性的分布，这对于理解模型的不确定性，防止过拟合，以及在面对新的、未见过的数据时提供有用的不确定性信息是非常有价值的。</p><p>据时提供有用的不确定性信息是非常有价值的。</p>',12)])])}const l=a(p,[["render",i]]),m=JSON.parse(`{"path":"/posts/Machine_learning/Bayesian/Bayes%20by%20backprop%20-%20naive%20variational%20inference.html","title":"Bayes by backprop - naive variational inference","lang":"en-US","frontmatter":{"title":"Bayes by backprop - naive variational inference","category":"Machine Learning","tags":["Machine Learning","Bayesian"],"description":"#Bayes GPT content: \\"Bayes by Backprop\\"的核心是使用变分推理来近似权重的后验分布。这一过程涉及到一些复杂的数学，但我会尽力将其分解为详细的步骤。 假设我们的神经网络的权重是 w，我们的数据是 D，那么在贝叶斯推理中，我们希望计算权重的后验分布 p(w|D)。然而，这通常是非常困难的，因为这需要计算一个涉及到所有可能...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Bayes by backprop - naive variational inference\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-11-25T05:15:08.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/Machine_learning/Bayesian/Bayes%20by%20backprop%20-%20naive%20variational%20inference.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"Bayes by backprop - naive variational inference"}],["meta",{"property":"og:description","content":"#Bayes GPT content: \\"Bayes by Backprop\\"的核心是使用变分推理来近似权重的后验分布。这一过程涉及到一些复杂的数学，但我会尽力将其分解为详细的步骤。 假设我们的神经网络的权重是 w，我们的数据是 D，那么在贝叶斯推理中，我们希望计算权重的后验分布 p(w|D)。然而，这通常是非常困难的，因为这需要计算一个涉及到所有可能..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-11-25T05:15:08.000Z"}],["meta",{"property":"article:tag","content":"Bayesian"}],["meta",{"property":"article:tag","content":"Machine Learning"}],["meta",{"property":"article:modified_time","content":"2025-11-25T05:15:08.000Z"}]]},"git":{"createdTime":1757508567000,"updatedTime":1764047708000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":5,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":2.11,"words":632},"filePathRelative":"posts/Machine_learning/Bayesian/Bayes by backprop - naive variational inference.md","excerpt":"<p>#Bayes</p>\\n<h3>GPT content:</h3>\\n<p>\\"Bayes by Backprop\\"的核心是使用变分推理来近似权重的后验分布。这一过程涉及到一些复杂的数学，但我会尽力将其分解为详细的步骤。</p>\\n<p>假设我们的神经网络的权重是 w，我们的数据是 D，那么在贝叶斯推理中，我们希望计算权重的后验分布 p(w|D)。然而，这通常是非常困难的，因为这需要计算一个涉及到所有可能权重值的积分，这在实际中是不可行的。因此，我们使用变分推理来近似这个后验分布。</p>","autoDesc":true}`);export{l as comp,m as data};
