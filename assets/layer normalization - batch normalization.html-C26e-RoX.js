import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,f as r,b as e}from"./app-DGckflSM.js";const o={};function n(l,a){return e(),i("div",null,[...a[0]||(a[0]=[r('<p>GPT answer:</p><p>Batch Normalization和Layer Normalization都是深度学习中的一种技术，用来防止梯度消失或者梯度爆炸，从而让神经网络更好地学习。它们的主要区别在于归一化的维度不同。</p><p>Batch Normalization，简称BN，是在每一层的激活函数之前，对每一批样本的输入进行归一化处理。其原理是通过减去均值并除以标准差来做归一化，然后再引入两个可训练的参数，分别对应缩放因子和偏移量，来弥补归一化过程中损失的模型表达能力。BN的主要目的是为了解决深度神经网络训练过程中的内部协变量偏移问题，使得模型训练更加稳定，收敛更快，同时可以使用更大的学习率，有正则化效果，一定程度上防止过拟合。</p><p>而Layer Normalization，简称LN，其主要思想是在每个样本中，对所有神经元的输出进行归一化。与BN不同的是，LN是对单个样本进行归一化，不受batch size的影响，因此在训练和测试阶段的行为是一致的。LN更多的应用在自然语言处理领域，尤其是在处理序列化数据时，如RNN（循环神经网络）和Transformer等模型中。</p><p>总的来说，BN和LN的使用取决于数据和任务的特性。BN更适合于图像类的任务，因为图像在batch内的统计特性较为一致，而LN由于其独立于batch size，更适合处理序列化的任务，以及在小batch size的情况下也能很好的工作。</p><ul><li><p>Batch Normalization<br> Batch Normalization的过程可以分为以下几个步骤：</p><ol><li>计算均值和方差：首先，我们需要对输入的mini-batch进行处理。假设一个mini-batch包含m个样本，对于每个神经元，我们计算这<strong>m个样本</strong>的均值和方差。</li><li>归一化：对于每个神经元，我们用每个样本的值减去该神经元的均值，然后除以其标准差，得到归一化的输出。这样做的目的是消除不同特征值范围对梯度下降优化算法的影响，使得所有特征值都在相同的数值范围内。</li><li>缩放和平移：虽然归一化有助于神经网络的训练，但有时我们可能不希望归一化的数据严格位于0均值和单位标准差。因此，Batch Normalization引入了两个可学习的参数，一个是缩放因子γ，另一个是平移因子β。这两个参数的维度与神经元的数量相同，它们的值通过反向传播和梯度下降来学习。具体的操作是，对于每个神经元，我们用归一化后的值乘以γ，再加上β，得到Batch Normalization的最终输出。</li><li>反向传播：在训练过程中，我们需要计算损失函数对于每个参数的梯度，包括γ和β。Batch Normalization的反向传播比较复杂，需要应用链式法则和一些数学技巧。<br> 总的来说，Batch Normalization的主要作用是在训练深度神经网络时，通过减少内部协变量偏移，提高模型的训练速度和性能。</li></ol></li><li><p>Layer Normalization<br> Layer Normalization是一种常见的深度学习模型中的归一化方法，主要用于处理梯度消失或梯度爆炸问题。它通过归一化层内的每个样本来改善模型的训练，具体步骤如下：</p><ol><li><p>计算输入样本的均值和方差：对于每个样本，我们需要计算其在所有 <strong>特征维度</strong> 上的均值和方差。设输入样本 X ∈ R^(d1 x d2 x ... x dn)，则其均值μ和方差σ²分别为：</p><p>μ = (1/H) * Σ(x)</p><p>σ² = (1/H) * Σ((x-μ)²)</p><p>其中，H 是特征维度的总数，Σ表示求和。</p></li><li><p>对输入样本进行归一化：使用上一步计算得到的均值和方差，我们可以对输入样本进行归一化。归一化的输入样本 y 的计算公式为：</p><p>y = (x - μ) / sqrt(σ² + ε)</p><p>其中，ε 是一个很小的数，用于防止除零错误。</p></li><li><p>重新调整和平移归一化的样本：归一化后的样本将被缩放和平移，以便能够学习更复杂的模式。调整后的输出 z 的计算公式为：</p><p>z = γ*y + β</p><p>其中，γ 和 β 是可学习的参数，分别用于缩放和平移。</p></li></ol><p>通过上述步骤，Layer Normalization能够在单个样本级别上进行归一化，使得模型在训练过程中能够更好地学习和适应不同的数据分布，提高模型的泛化性能。</p></li></ul><p>Zhihu answer:<br><a href="https://zhuanlan.zhihu.com/p/54171297" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/54171297</a><br><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/54530247</a></p><p>在文章的最后，我们指出BN并不适用于RNN等动态网络和batchsize较小的时候效果不好。Layer Normalization（LN）的提出有效的解决BN的这两个问题。LN和BN不同点是归一化的维度是互相垂直的，如图1所示。在图1中N表示样本轴, C表示通道轴, F是每个通道的特征数量。BN如右侧所示，它是取不同样本的同一个通道的特征做归一化；LN则是如左侧所示，它取的是同一个样本的不同通道做归一化。</p><p>![[Pasted image 20240414163730.png]]</p><h2 id="reference" tabindex="-1"><a class="header-anchor" href="#reference"><span>Reference</span></a></h2><p>[1] Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.</p><p>[2] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015. covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</p>',12)])])}const m=t(o,[["render",n]]),h=JSON.parse(`{"path":"/posts/Machine_learning/trick_layers/tricks/layer%20normalization%20-%20batch%20normalization.html","title":"layer normalization - batch normalization","lang":"en-US","frontmatter":{"title":"layer normalization - batch normalization","category":"Machine Learning","tags":["Machine Learning","tricks"],"description":"GPT answer: Batch Normalization和Layer Normalization都是深度学习中的一种技术，用来防止梯度消失或者梯度爆炸，从而让神经网络更好地学习。它们的主要区别在于归一化的维度不同。 Batch Normalization，简称BN，是在每一层的激活函数之前，对每一批样本的输入进行归一化处理。其原理是通过减去均值并...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"layer normalization - batch normalization\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-11-25T05:15:08.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/Machine_learning/trick_layers/tricks/layer%20normalization%20-%20batch%20normalization.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"layer normalization - batch normalization"}],["meta",{"property":"og:description","content":"GPT answer: Batch Normalization和Layer Normalization都是深度学习中的一种技术，用来防止梯度消失或者梯度爆炸，从而让神经网络更好地学习。它们的主要区别在于归一化的维度不同。 Batch Normalization，简称BN，是在每一层的激活函数之前，对每一批样本的输入进行归一化处理。其原理是通过减去均值并..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-11-25T05:15:08.000Z"}],["meta",{"property":"article:tag","content":"tricks"}],["meta",{"property":"article:tag","content":"Machine Learning"}],["meta",{"property":"article:modified_time","content":"2025-11-25T05:15:08.000Z"}]]},"git":{"createdTime":1757508567000,"updatedTime":1764047708000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":4,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":4.8,"words":1440},"filePathRelative":"posts/Machine_learning/trick&layers/tricks/layer normalization - batch normalization.md","excerpt":"<p>GPT answer:</p>\\n<p>Batch Normalization和Layer Normalization都是深度学习中的一种技术，用来防止梯度消失或者梯度爆炸，从而让神经网络更好地学习。它们的主要区别在于归一化的维度不同。</p>\\n<p>Batch Normalization，简称BN，是在每一层的激活函数之前，对每一批样本的输入进行归一化处理。其原理是通过减去均值并除以标准差来做归一化，然后再引入两个可训练的参数，分别对应缩放因子和偏移量，来弥补归一化过程中损失的模型表达能力。BN的主要目的是为了解决深度神经网络训练过程中的内部协变量偏移问题，使得模型训练更加稳定，收敛更快，同时可以使用更大的学习率，有正则化效果，一定程度上防止过拟合。</p>","autoDesc":true}`);export{m as comp,h as data};
