import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as i,a as l,f as r,w as p,r as m,b as o,g as a,d as s}from"./app-BPhnXKse.js";const c={};function g(h,n){const e=m("think");return o(),i("div",null,[l(e,null,{default:p(()=>[...n[0]||(n[0]=[a(` We are summarizing Chapter 18: "VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA" from the provided text. The chapter has several sections and fine points. We'll structure the summary by sections and include the technical points as described. `,-1),s("p",null,"Note: The text includes references to figures (e.g., Figure 18.1, 18.2, etc.) but we don't have the images. We'll describe the content based on the text and context.",-1),s("p",null,[a("We break down the chapter into:"),s("br"),a(" 18.1 The different goals of variable selection"),s("br"),a(" 18.2 Variables that induce or amplify bias"),s("br"),a(" 18.3 Causal inference and machine learning"),s("br"),a(" 18.4 Doubly robust machine learning estimators"),s("br"),a(" 18.5 Variable selection is a difficult problem")],-1),s("p",null,"Also, there are Fine Points and Technical Points that we will integrate.",-1),s("p",null,"Let's start with the chapter introduction and then each section.",-1)])]),_:1}),n[1]||(n[1]=r(' ### Chapter 18: VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA *This chapter addresses variable selection challenges in causal inference, contrasting predictive modeling goals with causal objectives. It identifies bias-inducing variables, introduces machine learning solutions for high-dimensional data, and emphasizes domain knowledge for valid causal estimates.* <h4 id="_18-1-the-different-goals-of-variable-selection" tabindex="-1"><a class="header-anchor" href="#_18-1-the-different-goals-of-variable-selection"><span>18.1 The different goals of variable selection</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Core theory</strong>: Causal inference requires adjustment for confounders (L) to achieve conditional exchangeability ((A \\perp!!!\\perp Y^a | L)), unlike predictive modeling which quantifies associations without causal interpretation.</li><li><strong>Important concept</strong>: Confounding is irrelevant in predictive models (e.g., smoking cessation (A) and weight gain (Y) association), as parameters lack causal meaning.</li><li><strong>Application scenario</strong>: Clinical prediction models (e.g., heart failure risk) classify patients but cannot guide interventions.</li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Predictive algorithms</strong>: Use cross-validation to optimize tuning parameters (e.g., lasso/ridge regularization, neural net depth/width).</li><li><strong>Black-box limitations</strong>: Lack interpretability for clinical decisions and may fail in new settings due to reliance on noncausal features.</li></ul><hr><h4 id="_18-2-variables-that-induce-or-amplify-bias" tabindex="-1"><a class="header-anchor" href="#_18-2-variables-that-induce-or-amplify-bias"><span>18.2 Variables that induce or amplify bias</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Bias mechanisms</strong>: <ul><li><strong>Colliders</strong> (e.g., Figure 18.1): Adjustment induces <em>selection bias under the null</em> (bias when true effect is zero).</li><li><strong>Mediators</strong> (e.g., Figure 18.4): Adjustment causes <em>overadjustment</em>, blocking causal paths (e.g., direct vs. total effects).</li><li><strong>Instruments</strong> (e.g., Figure 18.7): Adjustment may <em>amplify bias</em> from unmeasured confounders (U) (<em>Z-bias</em>).</li></ul></li><li><strong>Key formula</strong>: G-formula contrast for (E[Y^a]) becomes biased if (L) is a collider:<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><munder><mo>∑</mo><mi>l</mi></munder><mi>E</mi><mo stretchy="false">[</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo>=</mo><mn>1</mn><mo separator="true">,</mo><mi>L</mi><mo>=</mo><mi>l</mi><mo stretchy="false">]</mo><mi>Pr</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>L</mi><mo>=</mo><mi>l</mi><mo stretchy="false">)</mo><mo>−</mo><munder><mo>∑</mo><mi>l</mi></munder><mi>E</mi><mo stretchy="false">[</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo>=</mo><mn>0</mn><mo separator="true">,</mo><mi>L</mi><mo>=</mo><mi>l</mi><mo stretchy="false">]</mo><mi>Pr</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>L</mi><mo>=</mo><mi>l</mi><mo stretchy="false">)</mo><mo mathvariant="normal">≠</mo><mi>E</mi><mo stretchy="false">[</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo>=</mo><mn>1</mn><mo stretchy="false">]</mo><mo>−</mo><mi>E</mi><mo stretchy="false">[</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo>=</mo><mn>0</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\sum_l E[Y|A=1,L=l] \\Pr(L=l) - \\sum_l E[Y|A=0,L=l] \\Pr(L=l) \\neq E[Y|A=1] - E[Y|A=0] </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">Pr</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.3521em;vertical-align:-1.3021em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8479em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3021em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">Pr</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">0</span><span class="mclose">]</span></span></span></span></span></p></li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Temporal order fallacy</strong>: Post-treatment variables (e.g., Figure 18.5) can be valid adjusters if not affected by treatment.</li><li><strong>M-bias</strong> (Figure 18.6): Adjusting for pre-treatment colliders introduces bias; external knowledge is needed to distinguish confounders from colliders.</li><li><strong>Bias amplification</strong>: Adjusting for instruments (e.g., (Z) in Figure 18.7) can inflate bias from (U), but reduction is also possible.</li></ul><hr><h4 id="_18-3-causal-inference-and-machine-learning" tabindex="-1"><a class="header-anchor" href="#_18-3-causal-inference-and-machine-learning"><span>18.3 Causal inference and machine learning</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Estimation challenge</strong>: In high-dimensional (X), parametric models (e.g., GLMs) for (b(X) = E[Y|A=1,X]) or (\\pi(X) = \\Pr(A=1|X)) are misspecified.</li><li><strong>Machine learning solution</strong>: Use flexible models (e.g., splines, lasso, neural nets) to estimate (b(X)) and (\\pi(X)), but require integration with doubly robust estimators.</li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Model specification</strong>: High-dimensional transformations (s(X)) (e.g., cubic splines) improve flexibility but risk non-convergence if (\\dim(s(X)) &gt; n).</li><li><strong>Algorithm selection</strong>: Tree-based methods (random forests) or deep learning outperform parametric models for conditional expectations.</li></ul><hr><h4 id="_18-4-doubly-robust-machine-learning-estimators" tabindex="-1"><a class="header-anchor" href="#_18-4-doubly-robust-machine-learning-estimators"><span>18.4 Doubly robust machine learning estimators</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Doubly robust (DR) bias</strong>: Bias of DR estimator (\\hat{E}[Y^{a=1}]) depends on product of errors (\\left(\\frac{1}{\\pi(x)} - \\frac{1}{\\hat{\\pi}(x)}\\right) \\times (b(x) - \\hat{b}(x))), allowing small bias if errors are (o(n^{-1/4})).</li><li><strong>Sample splitting</strong>: Randomly split data into training (estimate (\\hat{b}(x)), (\\hat{\\pi}(x))) and estimation (compute DR effect) samples to avoid correlation-induced bias.</li><li><strong>Cross-fitting</strong>: Swap samples and average estimates to recover efficiency (e.g., for (\\psi = E[Y^{a=1}])):<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mover accent="true"><mi>ψ</mi><mo>^</mo></mover><mtext>cross-fit</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mi>M</mi></mfrac><munderover><mo>∑</mo><mrow><mi>m</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></munderover><msup><mover accent="true"><mi>ψ</mi><mo>^</mo></mover><mrow><mo stretchy="false">(</mo><mi>m</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\\hat{\\psi}_{\\text{cross-fit}} = \\frac{1}{M} \\sum_{m=1}^M \\hat{\\psi}^{(m)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1523em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ψ</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1389em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">cross-fit</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0954em;vertical-align:-1.2671em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8829em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9579em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">ψ</span></span><span style="top:-3.2634em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1389em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">m</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span></p></li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Variance reduction</strong>: Cross-fitting achieves semiparametric efficiency with standard error (\\sqrt{\\text{Var}\\left(b(X) + \\frac{A}{\\pi(X)}[Y - b(X)]\\right)/\\sqrt{n}).</li><li><strong>Implementation</strong>: Use bootstrapping for 95% confidence intervals.</li></ul><hr><h4 id="_18-5-variable-selection-is-a-difficult-problem" tabindex="-1"><a class="header-anchor" href="#_18-5-variable-selection-is-a-difficult-problem"><span>18.5 Variable selection is a difficult problem</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Limitations of DR-ML</strong>: <ol><li>Requires domain knowledge to exclude bias-inducing variables.</li><li>High computational cost for time-varying treatments.</li><li>Large variance if (\\pi(X) \\approx 0/1) for some (X).</li></ol></li><li><strong>Variance-bias tradeoff</strong>: Excluding variables to reduce variance invalidates confidence intervals.</li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Practical advice</strong>: Conduct sensitivity analyses with multiple methods to assess result robustness.</li><li><strong>Philosophical challenge</strong>: Confidence intervals assume all confounders are measured; unmeasured confounders undermine interpretability.</li></ul><hr><p><strong>Figures Summary</strong> (based on contextual descriptions):</p><ul><li><strong>Figures 18.1–18.7</strong>: Illustrate causal diagrams for colliders, mediators, instruments, and backdoor paths. Key takeaway: Adjustment decisions require causal knowledge, not temporal order.</li><li><strong>Fine Points 18.1–18.2</strong>: Describe variable selection methods (e.g., lasso, cross-validation) and overfitting solutions.</li><li><strong>Technical Points 18.1–18.2</strong>: Detail AIPW estimator algorithms and asymptotic properties of cross-fit estimators.</li></ul>',33))])}const y=t(c,[["render",g]]),f=JSON.parse(`{"path":"/posts/Causal%20inference/What-if%20book%20reading%20notes/response_md/DeepSeek-R1-0528/Chapter_18.html","title":"Chapter_18","lang":"en-US","frontmatter":{"title":"Chapter_18","category":"Causal inference","tags":["DeepSeek-R1-0528","response_md"],"description":"### Chapter 18: VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA *This chapter addresses variable selection challenges in causal inference, contrasting predictive modeling goals wit...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Chapter_18\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-17T03:19:05.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/Causal%20inference/What-if%20book%20reading%20notes/response_md/DeepSeek-R1-0528/Chapter_18.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"Chapter_18"}],["meta",{"property":"og:description","content":"### Chapter 18: VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA *This chapter addresses variable selection challenges in causal inference, contrasting predictive modeling goals wit..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-17T03:19:05.000Z"}],["meta",{"property":"article:tag","content":"response_md"}],["meta",{"property":"article:tag","content":"DeepSeek-R1-0528"}],["meta",{"property":"article:modified_time","content":"2025-09-17T03:19:05.000Z"}]]},"git":{"createdTime":1758079145000,"updatedTime":1758079145000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":1,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":2.83,"words":850},"filePathRelative":"posts/Causal inference/What-if book reading notes/response_md/DeepSeek-R1-0528/Chapter_18.md","excerpt":"\\n### Chapter 18: VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA  \\n*This chapter addresses variable selection challenges in causal inference, contrasting predictive modeling goals with causal objectives. It identifies bias-inducing variables, introduces machine learning solutions for high-dimensional data, and emphasizes domain knowledge for valid causal estimates.*\\n","autoDesc":true}`);export{y as comp,f as data};
