import{_ as s}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,f as a,b as n}from"./app-p5c8ryjK.js";const e={};function l(r,i){return n(),t("div",null,[...i[0]||(i[0]=[a(`<p>这是一个非常棒的问题，它触及了传统机器学习与深度学习之间核心思想的传承。</p><p>简单答案是：<strong>岭回归（L2正则化）和Lasso回归（L1正则化）的基本思想——正则化（Regularization），不仅适用于深度学习，而且是其不可或缺的核心技术之一。</strong> 不过，它们通常不再被直接称为“岭回归”或“Lasso回归”，而是以更一般化的“L1/L2正则化”或“权重衰减”的形式出现。</p><p>下面我们来一步步详细解释：</p><h3 id="_1-核心思想的传承-对抗过拟合" tabindex="-1"><a class="header-anchor" href="#_1-核心思想的传承-对抗过拟合"><span>1. 核心思想的传承：对抗过拟合</span></a></h3><p>无论是线性模型还是深度神经网络，机器学习模型都有一个共同的敌人：<strong>过拟合（Overfitting）</strong>。即模型过于复杂，完美地记住了训练数据中的噪声和细节，导致在新数据上的泛化能力很差。</p><ul><li><strong>岭回归(L2)的核心</strong>：通过在损失函数中增加模型权重（参数）的<strong>L2范数</strong>（平方和）作为惩罚项，迫使所有权重都趋向于变小、变得分散，而不是依赖少数几个巨大的权重。这会让模型变得更简单、更平滑，从而减轻过拟合。</li><li><strong>Lasso回归(L1)的核心</strong>：通过在损失函数中增加模型权重（参数）的<strong>L1范数</strong>（绝对值和）作为惩罚项，它倾向于将一些不重要的特征的权重<strong>直接压缩到零</strong>，从而实现特征选择，产生稀疏模型。</li></ul><p>深度学习完全继承了这一思想，并将其应用在规模更大、结构更复杂的模型上。</p><hr><h3 id="_2-在深度学习中的具体应用形式" tabindex="-1"><a class="header-anchor" href="#_2-在深度学习中的具体应用形式"><span>2. 在深度学习中的具体应用形式</span></a></h3><p>在深度学习框架（如PyTorch, TensorFlow）中，正则化被直接集成到了优化器里，使用起来非常方便。</p><h4 id="l2正则化-权重衰减-weight-decay" tabindex="-1"><a class="header-anchor" href="#l2正则化-权重衰减-weight-decay"><span>L2正则化 -&gt; “权重衰减”（Weight Decay）</span></a></h4><p>这是最直接、最广泛的应用。在深度学习中，L2正则化几乎等同于“权重衰减”这个概念。</p><ul><li><strong>是什么</strong>：在每次参数更新时，除了根据梯度调整参数外，还会额外将参数值向零缩小一点点（乘以一个小于1的因子）。</li><li><strong>为什么有效</strong>：防止权重变得过大，鼓励模型使用所有特征一点点，而不是极度依赖少数特征，从而提高泛化能力。这<strong>极大地缓解了过拟合</strong>。</li><li><strong>如何使用</strong>：在定义优化器（如 <code>Adam</code>, <code>SGD</code>）时，直接设置 <code>weight_decay</code> 参数。<div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 在PyTorch中，使用L2正则化（权重衰减）非常简单</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">optimizer </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.optim.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">Adam</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">parameters</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(), </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">lr</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1e-3</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">weight_decay</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1e-5</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># \`weight_decay\` 参数就是L2正则化的强度系数（λ）</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><strong>几乎所有成功的深度学习模型</strong>（从ResNet到BERT）的训练配置中，你都能找到权重衰减的身影。它是深度学习训练的标配。</li></ul><h4 id="l1正则化" tabindex="-1"><a class="header-anchor" href="#l1正则化"><span>L1正则化</span></a></h4><p>L1在深度学习中也同样可用，但不如L2普遍。</p><ul><li><strong>是什么</strong>：在损失函数中增加模型权重的绝对值和作为惩罚项。</li><li><strong>为什么有效</strong>：和Lasso回归一样，它倾向于产生<strong>稀疏权重</strong>，即让很多权重值直接变为零。这在某些场景下非常有用： <ol><li><strong>模型压缩（Model Compression）</strong>：通过L1正则化，可以“剪枝”掉大量不重要的神经元连接（权重为0），得到一个稀疏模型。这个稀疏模型可以大幅减少内存占用和计算量，便于部署在手机、嵌入式设备等资源受限的环境中。</li><li><strong>特征选择</strong>：虽然深度学习自动学习特征，但有时我们仍想解释哪些输入特征更重要。L1正则化可以帮助凸显出最重要的输入路径。</li></ol></li><li><strong>如何使用</strong>：通常需要手动将其添加到损失函数中，因为主流优化器没有像 <code>weight_decay</code> 那样的直接参数。<div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-python"><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># PyTorch中手动添加L1正则化的示例</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">l1_lambda </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 1e-6</span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  # L1正则化强度</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">l1_loss </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> 0</span></span>
<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">for</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> param </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">in</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> model.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">parameters</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">():</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">    l1_loss </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+=</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> torch.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">norm</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(param, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">p</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">1</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)  </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 计算所有参数的L1范数</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">total_loss </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> your_original_loss_function</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(</span><span style="--shiki-light:#383A42;--shiki-dark:#D19A66;">...</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">) </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">+</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> l1_lambda </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">*</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> l1_loss</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">optimizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">zero_grad</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">total_loss.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">backward</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">() </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 反向传播</span></span>
<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">optimizer.</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;">step</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">() </span><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 更新参数</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></li></ul><hr><h3 id="_3-深度学习中的其他正则化-利器" tabindex="-1"><a class="header-anchor" href="#_3-深度学习中的其他正则化-利器"><span>3. 深度学习中的其他正则化“利器”</span></a></h3><p>深度学习的成功不仅仅依赖于L1/L2。由于其模型极其灵活和复杂，研究者们发明了更多样、更强大的正则化技术，这些技术与L1/L2相辅相成，共同对抗过拟合：</p><ol><li><strong>Dropout</strong>：在训练过程中，随机地“丢弃”（暂时屏蔽）一部分神经元。这迫使网络不能过度依赖任何单个神经元，必须学习到更加鲁棒的特征。这是深度学习中最标志性的正则化方法之一。</li><li><strong>批量归一化（Batch Normalization）</strong>：虽然其主要目的是稳定训练、加速收敛，但通过引入训练和测试时的不一致性（噪声），它同时也起到了轻微的正则化效果。</li><li><strong>数据增强（Data Augmentation）</strong>：对输入数据（如图像）进行随机旋转、裁剪、变色等操作，人工扩大训练数据集。这是计算机视觉领域<strong>最强大</strong>的正则化手段。</li><li><strong>早停（Early Stopping）</strong>：在训练过程中持续监控验证集性能，一旦性能不再提升就停止训练，防止模型过度记忆训练集。</li></ol><h3 id="总结与对比" tabindex="-1"><a class="header-anchor" href="#总结与对比"><span>总结与对比</span></a></h3><table><thead><tr><th style="text-align:left;">技术</th><th style="text-align:left;">源于</th><th style="text-align:left;">在深度学习中的角色</th><th style="text-align:left;">主要目的</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>L2正则化</strong></td><td style="text-align:left;">岭回归</td><td style="text-align:left;"><strong>至关重要，标配</strong>（以<strong>权重衰减</strong>的形式集成）</td><td style="text-align:left;">防止权重过大，提高泛化能力</td></tr><tr><td style="text-align:left;"><strong>L1正则化</strong></td><td style="text-align:left;">Lasso回归</td><td style="text-align:left;"><strong>有选择地使用</strong></td><td style="text-align:left;">产生稀疏权重，用于<strong>模型压缩</strong>和特征选择</td></tr><tr><td style="text-align:left;"><strong>Dropout/BN/数据增强</strong></td><td style="text-align:left;">深度学习原生</td><td style="text-align:left;"><strong>与L1/L2同等重要，甚至更常用</strong></td><td style="text-align:left;">通过各种方式增加模型鲁棒性，防止过拟合</td></tr></tbody></table><p><strong>结论：</strong></p><p><strong>不要说“岭回归/Lasso回归适用于深度学习”，而应该说“L1和L2正则化的思想是深度学习正则化技术的基石”。</strong></p><ul><li>对于绝大多数日常训练深度网络的任务，<strong>你应该默认使用L2正则化（权重衰减）</strong>。</li><li>如果你有明确的模型轻量化、压缩或部署需求，可以尝试<strong>L1正则化</strong>。</li><li><strong>千万不要忽视</strong>Dropout、数据增强等其他正则化技术，它们通常能带来比单纯调大权重衰减系数更显著的效果。</li></ul><p>希望这个解释帮你理清了传统机器学习与深度学习之间美妙的联系！��器学习与深度学习之间美妙的联系！</p>`,26)])])}const p=s(e,[["render",l]]),g=JSON.parse(`{"path":"/posts/machine_learning/mathematics/Regularization%20for%20Deep%20Learning.html","title":"Regularization for Deep Learning","lang":"en-US","frontmatter":{"title":"Regularization for Deep Learning","category":"Machine Learning","tags":["Machine Learning","mathematics"],"description":"这是一个非常棒的问题，它触及了传统机器学习与深度学习之间核心思想的传承。 简单答案是：岭回归（L2正则化）和Lasso回归（L1正则化）的基本思想——正则化（Regularization），不仅适用于深度学习，而且是其不可或缺的核心技术之一。 不过，它们通常不再被直接称为“岭回归”或“Lasso回归”，而是以更一般化的“L1/L2正则化”或“权重衰减”...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Regularization for Deep Learning\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-16T12:31:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/machine_learning/mathematics/Regularization%20for%20Deep%20Learning.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"Regularization for Deep Learning"}],["meta",{"property":"og:description","content":"这是一个非常棒的问题，它触及了传统机器学习与深度学习之间核心思想的传承。 简单答案是：岭回归（L2正则化）和Lasso回归（L1正则化）的基本思想——正则化（Regularization），不仅适用于深度学习，而且是其不可或缺的核心技术之一。 不过，它们通常不再被直接称为“岭回归”或“Lasso回归”，而是以更一般化的“L1/L2正则化”或“权重衰减”..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-16T12:31:49.000Z"}],["meta",{"property":"article:tag","content":"mathematics"}],["meta",{"property":"article:tag","content":"Machine Learning"}],["meta",{"property":"article:modified_time","content":"2025-09-16T12:31:49.000Z"}]]},"git":{"createdTime":1757508567000,"updatedTime":1758025909000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":3,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":5.58,"words":1674},"filePathRelative":"posts/machine_learning/mathematics/Regularization for Deep Learning.md","excerpt":"<p>这是一个非常棒的问题，它触及了传统机器学习与深度学习之间核心思想的传承。</p>\\n<p>简单答案是：<strong>岭回归（L2正则化）和Lasso回归（L1正则化）的基本思想——正则化（Regularization），不仅适用于深度学习，而且是其不可或缺的核心技术之一。</strong> 不过，它们通常不再被直接称为“岭回归”或“Lasso回归”，而是以更一般化的“L1/L2正则化”或“权重衰减”的形式出现。</p>","autoDesc":true}`);export{p as comp,g as data};
