import{_ as n}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as a,a as s,f as o,w as r,r as l,b as c,g as u,d as e}from"./app-CyRTpUnn.js";const d={};function g(m,t){const i=l("think");return c(),a("div",null,[s(i,null,{default:r(()=>[...t[0]||(t[0]=[u(' We are summarizing Chapter 18: "VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA" from the provided text. The chapter has several sections and fine points. We will structure the summary by the sections and include the fine points as sub-sections. ',-1),e("p",null,"Important: We must base the summary solely on the provided text. For images, we note that they are referenced in the text (e.g., Figure 18.1, 18.2, etc.) but we don't have the images. We will describe the information based on the text that refers to them.",-1),e("p",null,"Steps:",-1),e("ol",null,[e("li",null,"Start with the introduction of the chapter (Page 1)."),e("li",null,"Then section 18.1: The different goals of variable selection."),e("li",null,"Section 18.2: Variables that induce or amplify bias (with embedded Fine Points 18.1 and 18.2)."),e("li",null,"Section 18.3: Causal inference and machine learning."),e("li",null,"Section 18.4: Doubly robust machine learning estimators (with Technical Points 18.1 and 18.2)."),e("li",null,"Section 18.5: Variable selection is a difficult problem.")],-1),e("p",null,"We note that the text includes several figures (18.1 to 18.7) and fine points (18.1, 18.2) and technical points (18.1, 18.2). We will summarize the content as described in the text.",-1),e("p",null,"Since the text is long, we break the summary by the major sections and include the fine points and technical points as sub-sections.",-1),e("p",null,"Format: Use the example output format provided.",-1),e("p",null,"Note: The text on Page 13 and 14 is just the start of Part III and is not part of Chapter 18, so we ignore it.",-1),e("p",null,"Let's begin.",-1)])]),_:1}),t[1]||(t[1]=o(' ### Chapter 18: VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA **Introduction** This chapter addresses variable selection for causal inference, contrasting it with predictive modeling. All causal adjustment methods (stratification, outcome regression, standardization, IP weighting, g-estimation) require a set of adjustment variables \\(L\\) that ensure conditional exchangeability. The chapter outlines guidelines for selecting \\(L\\) in causal analyses, emphasizing that criteria differ from predictive modeling due to the risk of introducing bias. <h4 id="_18-1-the-different-goals-of-variable-selection" tabindex="-1"><a class="header-anchor" href="#_18-1-the-different-goals-of-variable-selection"><span>18.1 The different goals of variable selection</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Core Theory</strong>: Causal inference requires adjustment for confounders (L) to interpret associations as causal effects. Without adjustment, association measures cannot be causal.</li><li><strong>Important Concept</strong>: Confounding is a causal concept; it does not apply when the estimand is purely associational (e.g., predictive models).</li><li><strong>Application Scenarios</strong>: <ul><li><em>Causal inference</em>: Adjust for confounders to estimate treatment effects.</li><li><em>Predictive modeling</em>: Include covariates predictive of the outcome, regardless of confounding status.</li></ul></li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Case Analysis</strong>: Smoking cessation ((A)) and weight gain ((Y)) – association estimation requires no confounder adjustment; causal effect estimation does.</li><li><strong>Background</strong>: Predictive models (e.g., heart failure risk classification) use covariates without causal interpretation. Hospitalization predicts heart failure but is not a target for intervention.</li><li><strong>Key Distinction</strong>: Identifying high-risk patients (prediction) vs. identifying effective treatments (causal inference).</li></ul><h4 id="_18-2-variables-that-induce-or-amplify-bias" tabindex="-1"><a class="header-anchor" href="#_18-2-variables-that-induce-or-amplify-bias"><span>18.2 Variables that induce or amplify bias</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Core Theory</strong>: Adjusting for certain variables induces bias even with infinite data: <ul><li><em>Colliders</em> (e.g., Figure 18.1): Adjustment creates spurious associations (selection bias under the null).</li><li><em>Mediators</em> (e.g., Figure 18.4): Adjustment blocks causal paths (overadjustment), biasing total effect estimates.</li><li><em>Instruments</em> (e.g., Figure 18.7): Adjustment amplifies unmeasured confounding (Z-bias).</li></ul></li><li><strong>Key Formulas</strong>: <ul><li>G-formula contrast: (\\sum_l E[Y|A=1,L=l] \\Pr(L=l) - \\sum_l E[Y|A=0,L=l] \\Pr(L=l)) becomes biased if (L) is a collider.</li><li>Bias amplification: Occurs when adjusting for instruments (e.g., (Z) in Figure 18.7).</li></ul></li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Case Analysis</strong>: <ul><li>Figure 18.1: Adjusting for collider (L) biases effect estimates even when true effect is null.</li><li>Figure 18.3: Selection bias occurs only under the alternative (non-null effect).</li><li>Figure 18.5: Post-treatment variables can be adjusted for if they block backdoor paths.</li></ul></li><li><strong>Background</strong>: Temporal order alone cannot determine if (A) affects (L); causal structure requires subject-matter knowledge.</li><li><strong>Fine Point 18.1 (Variable selection for regression models)</strong>: <ul><li><em>Subset selection</em>: Computationally intensive; includes forward/backward/stepwise methods.</li><li><em>Shrinkage methods</em>: Ridge regression and lasso reduce variance by shrinking coefficients.</li></ul></li><li><strong>Fine Point 18.2 (Overfitting and cross-validation)</strong>: <ul><li>Overfitting: Models fit noise in training data, harming generalizability.</li><li>Cross-validation: Splits data into training/validation sets; variants include (k)-fold CV. Deep learning resists overfitting with massive data.</li></ul></li></ul><h4 id="_18-3-causal-inference-and-machine-learning" tabindex="-1"><a class="header-anchor" href="#_18-3-causal-inference-and-machine-learning"><span>18.3 Causal inference and machine learning</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Core Theory</strong>: Assuming (X) contains all confounders and no bias-inducing variables, the goal is to estimate (E[Y^{a=1}] - E[Y^{a=0}]) in high-dimensional settings.</li><li><strong>Key Concepts</strong>: <ul><li>(b(X) = E[Y|A=1, X]) (for g-formula).</li><li>(\\pi(X) = \\Pr[A=1|X]) (for IP weighting).</li></ul></li><li><strong>Challenge</strong>: Traditional parametric models misspecify (b(X)) and (\\pi(X)) when (X) is high-dimensional.</li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Background</strong>: Machine learning (e.g., lasso, random forests, neural nets) improves prediction of (b(X)) and (\\pi(X)) but requires integration with doubly robust estimators.</li></ul><h4 id="_18-4-doubly-robust-machine-learning-estimators" tabindex="-1"><a class="header-anchor" href="#_18-4-doubly-robust-machine-learning-estimators"><span>18.4 Doubly robust machine learning estimators</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Core Theory</strong>: Doubly robust estimators (e.g., AIPW) have bias proportional to the product of errors in (\\hat{\\pi}(x)) and (\\hat{b}(x)), enabling valid inference if errors are (o(n^{-1/4})).</li><li><strong>Key Methods</strong>: <ul><li><em>Sample splitting</em>: Randomly divide data into training (estimates (\\hat{b}, \\hat{\\pi})) and estimation (computes effect) samples.</li><li><em>Cross-fitting</em>: Swaps samples and averages estimates to recover efficiency.</li></ul></li><li><strong>Key Formula</strong>: <ul><li>AIPW estimator: (\\hat{\\psi} = \\frac{1}{q} \\sum_{i=1}^q \\left[ \\hat{b}(X_i) + \\frac{A_i}{\\hat{\\pi}(X_i)} (Y_i - \\hat{b}(X_i)) \\right]).</li></ul></li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Case Analysis</strong>: Full-sample AIPW estimators correlate (\\hat{b}, \\hat{\\pi}) with the outcome, invalidating inference; split-sample avoids this.</li><li><strong>Technical Point 18.1 (AIPW estimator)</strong>: Steps for split-sample and cross-fit estimation.</li><li><strong>Technical Point 18.2 (Statistical properties)</strong>: Asymptotic normality requires (\\hat{b}(x)) and (\\hat{\\pi}(x)) consistent, with bias (o(n^{-1/2})).</li></ul><h4 id="_18-5-variable-selection-is-a-difficult-problem" tabindex="-1"><a class="header-anchor" href="#_18-5-variable-selection-is-a-difficult-problem"><span>18.5 Variable selection is a difficult problem</span></a></h4><p><strong>Technical Points</strong>:</p><ul><li><strong>Core Theory</strong>: Doubly robust ML provides valid inference but has limitations: <ul><li>Subject-matter knowledge gaps may leave residual bias.</li><li>High variance arises if (\\pi(X)) is near 0/1 for some (X).</li></ul></li><li><strong>Key Concept</strong>: Variance-bias trade-off: Including all confounders reduces bias but may inflate variance.</li></ul><p><strong>Details Points</strong>:</p><ul><li><strong>Background</strong>: <ul><li>Discarding variables to reduce variance invalidates confidence intervals.</li><li>Sensitivity analyses (e.g., multiple methods) are recommended when estimates conflict.</li></ul></li><li><strong>Philosophical Note</strong>: Observational datasets inherently omit some confounders, complicating interval interpretation.</li></ul><h3 id="chapter-summary" tabindex="-1"><a class="header-anchor" href="#chapter-summary"><span>Chapter Summary</span></a></h3><ul><li><strong>Logical Flow</strong>: Contrasts causal vs. predictive goals → details bias from colliders/mediators/instruments → integrates ML with doubly robust estimation → highlights unresolved challenges.</li><li><strong>Key Theme</strong>: Variable selection for causal inference requires subject-matter knowledge; automated procedures risk bias. Machine learning must be paired with sample splitting/cross-fitting for valid inference.</li><li><strong>Link to Broader Context</strong>: Sets stage for Part III (time-varying treatments) by emphasizing unresolved complexities in confounder selection.</li></ul><hr><p><strong>Figures</strong>: Described in text but not displayed (e.g., Figure 18.1: collider; Figure 18.4: mediator; Figure 18.7: instrument).<br><strong>Caveats</strong>:</p><ul><li>Image content inferred from textual descriptions only.</li><li>No subjective extrapolation; all conclusions based on provided text.</li></ul>',31))])}const f=n(d,[["render",g]]),b=JSON.parse(`{"path":"/posts/Causal%20inference/What-if%20book%20reading%20notes/response_md/DeepSeek-R1-0528-2nd/Chapter_18.html","title":"Chapter_18","lang":"en-US","frontmatter":{"title":"Chapter_18","category":"Causal inference","tags":["DeepSeek-R1-0528-2nd","response_md"],"description":"### Chapter 18: VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA **Introduction** This chapter addresses variable selection for causal inference, contrasting it with predictive mode...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Chapter_18\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-17T03:19:05.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/Causal%20inference/What-if%20book%20reading%20notes/response_md/DeepSeek-R1-0528-2nd/Chapter_18.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"Chapter_18"}],["meta",{"property":"og:description","content":"### Chapter 18: VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA **Introduction** This chapter addresses variable selection for causal inference, contrasting it with predictive mode..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-17T03:19:05.000Z"}],["meta",{"property":"article:tag","content":"response_md"}],["meta",{"property":"article:tag","content":"DeepSeek-R1-0528-2nd"}],["meta",{"property":"article:modified_time","content":"2025-09-17T03:19:05.000Z"}]]},"git":{"createdTime":1758079145000,"updatedTime":1758079145000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":1,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":3.8,"words":1139},"filePathRelative":"posts/Causal inference/What-if book reading notes/response_md/DeepSeek-R1-0528-2nd/Chapter_18.md","excerpt":"\\n### Chapter 18: VARIABLE SELECTION AND HIGH-DIMENSIONAL DATA  \\n**Introduction**  \\nThis chapter addresses variable selection for causal inference, contrasting it with predictive modeling. All causal adjustment methods (stratification, outcome regression, standardization, IP weighting, g-estimation) require a set of adjustment variables \\\\(L\\\\) that ensure conditional exchangeability. The chapter outlines guidelines for selecting \\\\(L\\\\) in causal analyses, emphasizing that criteria differ from predictive modeling due to the risk of introducing bias.  \\n","autoDesc":true}`);export{f as comp,b as data};
