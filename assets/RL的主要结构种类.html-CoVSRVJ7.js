import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,d as n,b as i}from"./app-BN8VANwn.js";const r={};function o(s,t){return i(),e("div",null,t[0]||(t[0]=[n('<p>好的，这是一个非常核心的问题。强化学习（RL）的算法世界看似纷繁复杂，但它们大多可以根据几个关键的维度进行划分和理解。</p><p>我们可以将强化学习算法的结构主要分为三大类，这三类回答了“<strong>算法学习的核心是什么？</strong>”这个问题。此外，还有两个重要的正交维度，它们描述了“<strong>算法如何学习？</strong>”。</p><hr><h3 id="核心三大算法结构-学习什么" tabindex="-1"><a class="header-anchor" href="#核心三大算法结构-学习什么"><span>核心三大算法结构 (学习什么？)</span></a></h3><p>这三大结构是理解所有RL算法的基石。</p><h4 id="_1-基于价值的算法-value-based" tabindex="-1"><a class="header-anchor" href="#_1-基于价值的算法-value-based"><span>1. 基于价值的算法 (Value-Based)</span></a></h4><p>这类算法不直接学习要执行哪个动作，而是学习一个“价值函数”或“打分函数”，来评估在某个状态下执行某个动作有多好。最终，智能体（Agent）会选择那个预估得分最高的动作。</p><ul><li><strong>核心思想</strong>：学习一个“得分表”（价值函数Q(s, a)），这个表告诉你，在状态s下执行动作a，长期来看能获得多少奖励。</li><li><strong>输出</strong>：每个“状态-动作”对的价值（分数）。</li><li><strong>决策方式</strong>：在当前状态下，查看得分表，选择得分最高的动作执行（贪心策略）。</li><li><strong>优点</strong>： <ul><li>在学习的早期阶段通常更稳定。</li><li>样本利用效率相对较高（特别是Off-policy类型，如Q-Learning）。</li></ul></li><li><strong>缺点</strong>： <ul><li>通常难以处理连续的动作空间（想象一下，你无法为无限个动作都创建一个得分）。</li><li>学习的是确定性策略，无法学习随机策略。</li></ul></li><li><strong>代表算法</strong>： <ul><li><strong>Q-Learning</strong>：经典的表格型RL算法。</li><li><strong>DQN (Deep Q-Network)</strong>：将Q-Learning与深度神经网络结合，开启了深度强化学习的时代，用于处理高维状态空间（例如，从像素玩游戏）。</li></ul></li></ul><h4 id="_2-基于策略的算法-policy-based" tabindex="-1"><a class="header-anchor" href="#_2-基于策略的算法-policy-based"><span>2. 基于策略的算法 (Policy-Based)</span></a></h4><p>这类算法不学习每个动作的分数，而是直接学习一个“策略函数”，这个函数直接告诉智能体在某个状态下应该采取什么动作。</p><ul><li><strong>核心思想</strong>：直接学习一个“行为指南”（策略π(a|s)），这个指南明确指出在状态s下，执行动作a的概率是多少。</li><li><strong>输出</strong>：一个概率分布，表示在当前状态下选择每个动作的可能性。</li><li><strong>决策方式</strong>：根据策略函数输出的概率分布进行抽样，选择一个动作。</li><li><strong>优点</strong>： <ul><li>能很好地处理<strong>连续动作空间</strong>。</li><li>能够学习随机策略，这在某些环境中（如石头剪刀布）至关重要。</li><li>策略更新通常更平滑。</li></ul></li><li><strong>缺点</strong>： <ul><li>训练过程容易不稳定，方差较大（因为奖励信号可能很稀疏或有噪声）。</li><li>样本利用效率通常较低（特别是On-policy类型）。</li></ul></li><li><strong>代表算法</strong>： <ul><li><strong>REINFORCE</strong>：最基础的策略梯度算法。</li><li><strong>PPO (Proximal Policy Optimization)</strong> / <strong>TRPO (Trust Region Policy Optimization)</strong>：REINFORCE的改进版，通过限制每次策略更新的幅度来提高稳定性。</li></ul></li></ul><h4 id="_3-演员-评论家算法-actor-critic" tabindex="-1"><a class="header-anchor" href="#_3-演员-评论家算法-actor-critic"><span>3. 演员-评论家算法 (Actor-Critic)</span></a></h4><p>这是前两类算法的结合体，旨在取长补短。它包含两个部分：</p><ul><li><p><strong>演员 (Actor)</strong>：一个基于<strong>策略</strong>的网络，负责根据当前状态选择动作。</p></li><li><p><strong>评论家 (Critic)</strong>：一个基于<strong>价值</strong>的网络，负责评估演员选择的动作有多好，并向演员提供反馈。</p></li><li><p><strong>核心思想</strong>：演员负责行动，评论家负责打分和指导。评论家告诉演员“你这个动作是好是坏，好多少，坏多少”，演员根据这个更明确的指导来优化自己的行为策略。</p></li><li><p><strong>工作流程</strong>：演员做出动作 -&gt; 评论家评估该动作并给出反馈 -&gt; 演员根据反馈更新策略。</p></li><li><p><strong>优点</strong>：</p><ul><li>结合了两者的优点：既能处理连续动作空间，又通过评论家的指导降低了训练的方差，比纯策略梯度法更稳定、学习更快。</li><li>是目前解决复杂RL问题最主流和最强大的框架。</li></ul></li><li><p><strong>缺点</strong>：</p><ul><li>结构更复杂，有两个网络需要训练和协调。</li></ul></li><li><p><strong>代表算法</strong>：</p><ul><li><strong>A2C / A3C (Advantage Actor-Critic)</strong>：经典的Actor-Critic算法。</li><li><strong>DDPG / SAC (Soft Actor-Critic)</strong>：专为连续控制任务设计的强大算法。</li><li><strong>PPO</strong>：实际上，PPO也是一种Actor-Critic的实现，它有一个策略网络（Actor）和一个价值网络（Critic）。</li></ul></li></ul><hr><h3 id="另外两个重要的分类维度-如何学" tabindex="-1"><a class="header-anchor" href="#另外两个重要的分类维度-如何学"><span>另外两个重要的分类维度 (如何学？)</span></a></h3><p>这两个维度可以与上述三大结构任意组合。</p><h4 id="a-无模型-model-free-vs-有模型-model-based" tabindex="-1"><a class="header-anchor" href="#a-无模型-model-free-vs-有模型-model-based"><span>A. 无模型 (Model-Free) vs. 有模型 (Model-Based)</span></a></h4><ul><li><p><strong>无模型 (Model-Free)</strong>：算法不试图去理解环境的运作规则（物理模型）。它只通过大量的反复试验来学习价值或策略。就像一个不懂棋理但下了无数盘棋的棋手。</p><ul><li><strong>优点</strong>：简单直接，适用于环境模型未知或非常复杂的情况。</li><li><strong>缺点</strong>：通常需要海量的样本才能学好（样本效率低）。</li><li><strong>绝大多数流行算法都是Model-Free的</strong>，例如DQN, PPO, SAC。</li></ul></li><li><p><strong>有模型 (Model-Based)</strong>：算法会先尝试学习一个环境的模型（即预测“我在状态s下执行动作a，会转移到哪个新状态s&#39;并得到多少奖励r”）。然后，它可以在这个内部“虚拟环境”中进行规划和学习。</p><ul><li><strong>优点</strong>：样本效率高得多，因为它可以在虚拟环境中无限次尝试。</li><li><strong>缺点</strong>：如果环境模型学得不准，会导致“差之毫厘，谬以千里”的后果。</li><li><strong>代表算法</strong>：AlphaGo, AlphaZero（其MCTS树搜索部分就扮演了模型的角色）。</li></ul></li></ul><h4 id="b-在线策略-on-policy-vs-离线策略-off-policy" tabindex="-1"><a class="header-anchor" href="#b-在线策略-on-policy-vs-离线策略-off-policy"><span>B. 在线策略 (On-Policy) vs. 离线策略 (Off-Policy)</span></a></h4><ul><li><p><strong>在线策略 (On-Policy)</strong>：学习用的数据必须是由当前正在优化的策略所产生的。简单说就是“边玩边学，用刚玩的数据学”。</p><ul><li><strong>优点</strong>：训练过程通常更稳定。</li><li><strong>缺点</strong>：样本利用效率低，因为每次策略更新后，之前的数据就不能再用了，必须重新采集。</li><li><strong>代表算法</strong>：SARSA, PPO, A2C。</li></ul></li><li><p><strong>离线策略 (Off-Policy)</strong>：学习用的数据可以来自于任何策略，不一定非得是当前策略。可以把过去的所有经验（无论好坏）都存进一个“经验池”，然后反复从中抽取数据来学习。</p><ul><li><strong>优点</strong>：样本利用效率极高，可以重复利用旧数据。</li><li><strong>缺点</strong>：训练过程可能不稳定，容易产生高估等问题。</li><li><strong>代表算法</strong>：Q-Learning, DQN, SAC, DDPG。</li></ul></li></ul><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><table><thead><tr><th style="text-align:left;">算法结构</th><th style="text-align:left;">核心思想</th><th style="text-align:left;">输出</th><th style="text-align:left;">优点</th><th style="text-align:left;">缺点</th><th style="text-align:left;">代表算法</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>基于价值</strong></td><td style="text-align:left;">学习每个动作的“得分”</td><td style="text-align:left;">Q值 (分数)</td><td style="text-align:left;">稳定，样本效率较高</td><td style="text-align:left;">难处理连续动作</td><td style="text-align:left;">DQN, Q-Learning</td></tr><tr><td style="text-align:left;"><strong>基于策略</strong></td><td style="text-align:left;">直接学习“行为指南”</td><td style="text-align:left;">动作的概率</td><td style="text-align:left;">能处理连续动作，能学随机策略</td><td style="text-align:left;">方差大，不稳定</td><td style="text-align:left;">REINFORCE, PPO</td></tr><tr><td style="text-align:left;"><strong>演员-评论家</strong></td><td style="text-align:left;">演员行动，评论家指导</td><td style="text-align:left;">动作概率+状态价值</td><td style="text-align:left;">结合两者优点，稳定且高效</td><td style="text-align:left;">结构复杂</td><td style="text-align:left;">A2C, SAC, PPO</td></tr></tbody></table>',23)]))}const p=l(r,[["render",o]]),d=JSON.parse(`{"path":"/posts/machine_learning/reinforcement%20learning/RL%E7%9A%84%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%84%E7%A7%8D%E7%B1%BB.html","title":"RL的主要结构种类","lang":"en-US","frontmatter":{"title":"RL的主要结构种类","description":"好的，这是一个非常核心的问题。强化学习（RL）的算法世界看似纷繁复杂，但它们大多可以根据几个关键的维度进行划分和理解。 我们可以将强化学习算法的结构主要分为三大类，这三类回答了“算法学习的核心是什么？”这个问题。此外，还有两个重要的正交维度，它们描述了“算法如何学习？”。 核心三大算法结构 (学习什么？) 这三大结构是理解所有RL算法的基石。 1. 基...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"RL的主要结构种类\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-11T07:19:18.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/machine_learning/reinforcement%20learning/RL%E7%9A%84%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%84%E7%A7%8D%E7%B1%BB.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"RL的主要结构种类"}],["meta",{"property":"og:description","content":"好的，这是一个非常核心的问题。强化学习（RL）的算法世界看似纷繁复杂，但它们大多可以根据几个关键的维度进行划分和理解。 我们可以将强化学习算法的结构主要分为三大类，这三类回答了“算法学习的核心是什么？”这个问题。此外，还有两个重要的正交维度，它们描述了“算法如何学习？”。 核心三大算法结构 (学习什么？) 这三大结构是理解所有RL算法的基石。 1. 基..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-11T07:19:18.000Z"}],["meta",{"property":"article:modified_time","content":"2025-09-11T07:19:18.000Z"}]]},"git":{"createdTime":1757508567000,"updatedTime":1757575158000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":2,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":5.97,"words":1791},"filePathRelative":"posts/machine_learning/reinforcement learning/RL的主要结构种类.md","excerpt":"<p>好的，这是一个非常核心的问题。强化学习（RL）的算法世界看似纷繁复杂，但它们大多可以根据几个关键的维度进行划分和理解。</p>\\n<p>我们可以将强化学习算法的结构主要分为三大类，这三类回答了“<strong>算法学习的核心是什么？</strong>”这个问题。此外，还有两个重要的正交维度，它们描述了“<strong>算法如何学习？</strong>”。</p>\\n<hr>\\n<h3>核心三大算法结构 (学习什么？)</h3>\\n<p>这三大结构是理解所有RL算法的基石。</p>\\n<h4>1. 基于价值的算法 (Value-Based)</h4>\\n<p>这类算法不直接学习要执行哪个动作，而是学习一个“价值函数”或“打分函数”，来评估在某个状态下执行某个动作有多好。最终，智能体（Agent）会选择那个预估得分最高的动作。</p>","autoDesc":true}`);export{p as comp,d as data};
