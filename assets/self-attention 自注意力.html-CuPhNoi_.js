import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as n,f as o,b as r}from"./app-CyRTpUnn.js";const i={};function l(a,t){return r(),n("div",null,[...t[0]||(t[0]=[o('<p>太好了！这是一个核心问题。<strong>Self-Attention（自注意力）</strong> 是注意力机制的一种特殊形式，也是Transformer模型成功的基石。</p><p>简单来说：</p><ul><li><strong>普通Attention</strong>：用于连接<strong>两个不同</strong>的序列（例如，源语言句子和目标语言句子）。Query来自一个序列，Key和Value来自另一个序列。</li><li><strong>Self-Attention</strong>：只在<strong>同一个序列</strong>内部进行操作。Query, Key, Value<strong>全部来自于同一个输入序列</strong>。</li></ul><hr><h3 id="核心思想-序列内部的-自我审视" tabindex="-1"><a class="header-anchor" href="#核心思想-序列内部的-自我审视"><span>核心思想：序列内部的“自我审视”</span></a></h3><p>Self-Attention 让序列中的<strong>每一个元素</strong>（例如句子中的每一个词）都去“审视”或“关注”序列中的<strong>所有其他元素</strong>（包括它自己），从而计算出一种新的、融入了全局上下文信息的元素表示。</p><p><strong>它的核心目的是：捕捉一个序列内部的内部结构、语义关联和语法依赖关系。</strong></p><hr><h3 id="一个生动的例子-解决指代问题" tabindex="-1"><a class="header-anchor" href="#一个生动的例子-解决指代问题"><span>一个生动的例子：解决指代问题</span></a></h3><p>考虑这个句子：<br> “<strong>The animal didn&#39;t cross the street because it was too tired.</strong>”</p><p>（“那个动物没有过马路，因为它太累了。”）</p><p>这里的“it”（它）指的是什么？是“street”（马路）还是“animal”（动物）？这对人类来说很简单，但对模型来说是个挑战。</p><p><strong>Self-Attention 如何工作：</strong></p><ol><li>当模型处理到“it”这个词时，<strong>Self-Attention 机制允许“it”这个词的表示（Query）去与句子中的每一个词（Key）计算相关性</strong>。</li><li>通过计算，它会发现“it”与“animal”和“tired”的关联度（注意力分数）非常高。</li><li>因此，在生成“it”的新表示（上下文向量）时，它会<strong>大量地融入“animal”和“tired”的信息</strong>，而几乎忽略“street”的信息。</li><li>这样，模型就能清晰地知道“it”指的是“animal”，而不是“street”。</li></ol><p>这个过程就像是序列中的每个词都在和其他所有词“开会交流”，最终每个词都带着对整句话的理解形成一个新的、更丰富的表示。</p><hr><h3 id="工作流程-与普通attention相同但来源不同" tabindex="-1"><a class="header-anchor" href="#工作流程-与普通attention相同但来源不同"><span>工作流程（与普通Attention相同但来源不同）</span></a></h3><p>它的计算公式与普通注意力完全一样，但Q, K, V的来源变了：</p><p><strong><code>SelfAttention(X) = Attention(Q, K, V) = softmax((QK^T) / √d_k) V</code></strong></p><p><strong>关键区别在于</strong>：</p><ul><li><strong>普通Attention</strong>: <ul><li><code>Q</code> = 目标序列的表示</li><li><code>K, V</code> = 源序列的表示</li></ul></li><li><strong>Self-Attention</strong>: <ul><li><code>Q = K = V =</code> <strong>同一个输入序列X的线性变换</strong><ul><li><code>Q = X * W_Q</code></li><li><code>K = X * W_K</code></li><li><code>V = X * W_V</code></li></ul></li><li><code>W_Q</code>, <code>W_K</code>, <code>W_V</code> 是可学习的参数矩阵，它们将相同的输入X投影到不同的空间，以便扮演不同的角色。</li></ul></li></ul><h3 id="为什么self-attention如此强大" tabindex="-1"><a class="header-anchor" href="#为什么self-attention如此强大"><span>为什么Self-Attention如此强大？</span></a></h3><ol><li><p><strong>强大的长距离依赖建模能力</strong>：</p><ul><li>传统RNN需要一步步顺序处理，距离较远的词之间的依赖关系容易被弱化（梯度消失/爆炸）。</li><li><strong>Self-Attention一步到位</strong>：无论两个词在序列中的距离有多远，它们之间的关联计算都是一步完成的。这使得它非常擅长捕捉长距离依赖关系。</li></ul></li><li><p><strong>极高的并行化程度</strong>：</p><ul><li>RNN的计算是顺序的，无法并行。</li><li><strong>Self-Attention的计算可以完全并行化</strong>。因为序列中所有元素对的相似度计算（<code>QK^T</code>）是相互独立的，可以同时进行，这使得它在大规模硬件（如GPU）上效率极高。</li></ul></li><li><p><strong>可解释性</strong>：</p><ul><li>通过可视化注意力权重，我们可以看到模型在处理一个词时，具体关注了序列中的哪些其他词。这为了解模型的工作机制提供了宝贵的视角。</li></ul></li></ol><h3 id="self-attention在transformer中的角色" tabindex="-1"><a class="header-anchor" href="#self-attention在transformer中的角色"><span>Self-Attention在Transformer中的角色</span></a></h3><p>在Transformer中，Self-Attention有两种主要应用：</p><ol><li><p><strong>编码器中的Self-Attention</strong>：</p><ul><li>让输入序列的每个词都能充分理解其所在句子的全局上下文信息，生成一个“上下文感知”的词表示。</li></ul></li><li><p><strong>解码器中的Masked Self-Attention</strong>：</p><ul><li>为了防止在训练时“作弊”（看到未来的词），解码器的Self-Attention会被屏蔽（Masked）。这意味着在计算位置<code>i</code>的词的注意力时，它只能关注位置<code>1</code>到<code>i</code>的词，而不能关注到<code>i+1</code>及之后的词。</li></ul></li></ol><h3 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h3><table><thead><tr><th style="text-align:left;">特性</th><th style="text-align:left;">普通Attention (Encoder-Decoder Attention)</th><th style="text-align:left;">Self-Attention</th></tr></thead><tbody><tr><td style="text-align:left;"><strong>应用场景</strong></td><td style="text-align:left;">连接两个<strong>不同</strong>的序列（如翻译）</td><td style="text-align:left;">分析<strong>同一个</strong>序列的内部结构</td></tr><tr><td style="text-align:left;"><strong>Q, K, V 来源</strong></td><td style="text-align:left;"><code>Q</code> 来自序列A，<code>K, V</code> 来自序列B</td><td style="text-align:left;"><code>Q, K, V</code> 全部来自同一个序列X</td></tr><tr><td style="text-align:left;"><strong>主要目的</strong></td><td style="text-align:left;"><strong>对齐</strong>（Alignment）</td><td style="text-align:left;"><strong>表征学习</strong>（Representation Learning）</td></tr></tbody></table><p>总而言之，<strong>Self-Attention是一种让序列模型能够高效、并行地捕捉序列内部长距离依赖关系的机制，它是Transformer架构的核心，也是现代大语言模型理解语言上下文的基础。</strong>�言模型理解语言上下文的基础。**</p>',29)])])}const d=e(i,[["render",l]]),c=JSON.parse(`{"path":"/posts/machine_learning/trick_layers/Attention/self-attention%20%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B.html","title":"self-attention 自注意力","lang":"en-US","frontmatter":{"title":"self-attention 自注意力","category":"Machine Learning","tags":["Machine Learning","Attention"],"description":"太好了！这是一个核心问题。Self-Attention（自注意力） 是注意力机制的一种特殊形式，也是Transformer模型成功的基石。 简单来说： 普通Attention：用于连接两个不同的序列（例如，源语言句子和目标语言句子）。Query来自一个序列，Key和Value来自另一个序列。 Self-Attention：只在同一个序列内部进行操作。Q...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"self-attention 自注意力\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-16T12:31:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/machine_learning/trick_layers/Attention/self-attention%20%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"self-attention 自注意力"}],["meta",{"property":"og:description","content":"太好了！这是一个核心问题。Self-Attention（自注意力） 是注意力机制的一种特殊形式，也是Transformer模型成功的基石。 简单来说： 普通Attention：用于连接两个不同的序列（例如，源语言句子和目标语言句子）。Query来自一个序列，Key和Value来自另一个序列。 Self-Attention：只在同一个序列内部进行操作。Q..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-16T12:31:49.000Z"}],["meta",{"property":"article:tag","content":"Attention"}],["meta",{"property":"article:tag","content":"Machine Learning"}],["meta",{"property":"article:modified_time","content":"2025-09-16T12:31:49.000Z"}]]},"git":{"createdTime":1757508567000,"updatedTime":1758025909000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":3,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":4.07,"words":1222},"filePathRelative":"posts/machine_learning/trick&layers/Attention/self-attention 自注意力.md","excerpt":"<p>太好了！这是一个核心问题。<strong>Self-Attention（自注意力）</strong> 是注意力机制的一种特殊形式，也是Transformer模型成功的基石。</p>\\n<p>简单来说：</p>\\n<ul>\\n<li><strong>普通Attention</strong>：用于连接<strong>两个不同</strong>的序列（例如，源语言句子和目标语言句子）。Query来自一个序列，Key和Value来自另一个序列。</li>\\n<li><strong>Self-Attention</strong>：只在<strong>同一个序列</strong>内部进行操作。Query, Key, Value<strong>全部来自于同一个输入序列</strong>。</li>\\n</ul>","autoDesc":true}`);export{d as comp,c as data};
