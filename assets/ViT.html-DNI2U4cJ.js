import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as r,f as n,b as t}from"./app-F5yqrHBi.js";const i={};function s(a,o){return t(),r("div",null,[...o[0]||(o[0]=[n('<h3 id="vision-transformer-vit-从文本到图像的革命性跨越" tabindex="-1"><a class="header-anchor" href="#vision-transformer-vit-从文本到图像的革命性跨越"><span>Vision Transformer (ViT)：从文本到图像的革命性跨越</span></a></h3><p>Vision Transformer (ViT) 的出现，标志着深度学习领域的一个重要转折点。它成功地将原本在自然语言处理（NLP）领域大放异彩的 Transformer 模型，应用于计算机视觉领域，并取得了与顶级卷积神经网络（CNN）相媲美的性能。要理解 ViT 的精髓，我们需要先深入了解它在传统 Encoder-Decoder Transformer 架构上所做的关键改变，并剖析其处理图像数据的每一步。</p><h4 id="vit-对传统-encoder-decoder-transformer-的三大核心改变" tabindex="-1"><a class="header-anchor" href="#vit-对传统-encoder-decoder-transformer-的三大核心改变"><span>ViT 对传统 Encoder-Decoder Transformer 的三大核心改变</span></a></h4><p>传统的 Transformer 模型主要用于处理序列到序列的任务，如机器翻译。它包含一个编码器（Encoder）来理解输入序列（源语言句子），以及一个解码器（Decoder）来生成输出序列（目标语言句子）。然而，ViT 在设计上进行了大胆的简化和调整，使其更适应图像分类等视觉任务：</p><ol><li><p><strong>抛弃解码器（Decoder）</strong>：ViT 的核心任务是图像分类，这是一个“多对一”的问题，即输入一张图像（多个像素），输出一个类别标签（一个结果）。因此，它不需要像机器翻译那样生成一个序列。ViT 只使用了 Transformer 的 <strong>编码器（Encoder）部分</strong>。编码器的作用是深入理解输入图像各个部分之间的关系，并最终输出一个包含整张图像丰富信息的综合表示，用于后续的分类。</p></li><li><p><strong>全新的输入处理方式：将图像转化为序列</strong>：这是 ViT 最具开创性的改变。Transformer 的原生设计是处理一维的序列数据（如单词序列），而图像是二维的像素网格。为了跨越这个“维度鸿沟”，ViT 引入了 <strong>“图像块化”（Image Patching）</strong> 的策略。它不直接处理单个像素，而是将图像分割成一系列固定大小的、不重叠的方块（Patches），然后将这些二维的方块展平（flatten）成一维的向量。这样一来，一张图像就巧妙地转化为了一个“图像块”序列，完美地适配了 Transformer 的输入要求。</p></li><li><p><strong>引入 <code>[class]</code> 令牌（Token）</strong>：在 NLP 的 Transformer 模型（如 BERT）中，常常会有一个特殊的 <code>[CLS]</code> 令牌被添加到序列的开头。这个令牌经过 Transformer 编码器后，其对应的输出向量被视作整个序列的聚合表示，用于下游的分类任务。ViT 借鉴了这一思想，同样在图像块序列的最前面添加了一个可学习的 <code>[class]</code> 嵌入向量。这个 <code>[class]</code> 令牌不代表任何具体的图像块，但它会与其他图像块的表示进行交互，通过自注意力机制（Self-Attention）聚合整个图像的全局信息。在经过所有 Transformer Encoder 层之后，最终只需要取出这个 <code>[class]</code> 令牌对应的输出向量，送入一个简单的分类头（通常是一个全连接层），即可完成图像分类。</p></li></ol><h4 id="vit-算法的详细步骤解析" tabindex="-1"><a class="header-anchor" href="#vit-算法的详细步骤解析"><span>ViT 算法的详细步骤解析</span></a></h4><p>下面我们以图像分类任务为例，一步步拆解 ViT 的工作流程：</p><p><strong>第1步：图像分块与展平 (Patching and Flattening)</strong></p><ul><li><strong>输入</strong>：一张二维图像，例如尺寸为 <code>H × W × C</code>（高 × 宽 × 通道数）。</li><li><strong>操作</strong>：首先，将这张图像分割成 <code>N</code> 个大小均为 <code>P × P</code> 的小方块（Patches）。例如，一张 224x224 的图像，如果 Patch 大小为 16x16，那么将会得到 <code>(224/16) * (224/16) = 14 * 14 = 196</code> 个图像块。</li><li><strong>展平</strong>：将每个二维的图像块（<code>P × P × C</code>）展平为一个一维的向量。在上面的例子中，每个 16x16x3 的图像块会被展平成一个长度为 <code>16 * 16 * 3 = 768</code> 的向量。</li><li><strong>输出</strong>：一个由 <code>N</code> 个一维向量组成的序列，即一个形状为 <code>N × (P² * C)</code> 的矩阵。</li></ul><p><strong>第2步：线性投射与嵌入 (Linear Projection and Embedding)</strong></p><ul><li><strong>操作</strong>：Transformer 内部处理的向量维度是固定的，我们称之为 <code>D</code>（模型维度）。上一步得到的展平向量维度（<code>P² * C</code>）不一定等于 <code>D</code>。因此，需要通过一个可训练的线性投射层（本质上是一个全连接层）将每个展平后的图像块向量从 <code>P² * C</code> 维映射到 <code>D</code> 维。这些经过投射后的向量被称为“块嵌入”（Patch Embeddings）。</li></ul><p><strong>第3步：添加 <code>[class]</code> 令牌与位置编码 (Prepending <code>[class]</code> Token &amp; Adding Positional Encoding)</strong></p><ul><li><strong>添加 <code>[class]</code> 令牌</strong>：在 <code>N</code> 个块嵌入序列的最前面，拼接上一个可学习的 <code>[class]</code> 嵌入向量（维度也是 <code>D</code>）。现在，序列的长度变成了 <code>N+1</code>。</li><li><strong>添加位置编码</strong>：Transformer 的自注意力机制本身是无法感知序列顺序的。为了让模型知道每个图像块的原始空间位置（例如，哪个块在左上角，哪个在中间），必须为序列中的每一个嵌入向量（包括 <code>[class]</code> 令牌）添加一个“位置编码”（Positional Encoding）。这个位置编码通常也是一个可学习的 <code>D</code> 维向量，它携带着位置信息，与块嵌入相加后，共同作为 Transformer Encoder 的输入。</li></ul><p><strong>第4步：输入 Transformer 编码器 (Passing to Transformer Encoder)</strong></p><ul><li><strong>核心结构</strong>：ViT 的 Transformer Encoder 由多个相同的层堆叠而成。每一层主要包含两个子模块： <ol><li><strong>多头自注意力 (Multi-Head Self-Attention)</strong>：这是 Transformer 的灵魂。它允许序列中的每个元素（每个图像块的表示）关注并加权融合序列中所有其他元素的信息。这使得模型能够捕捉图像块之间的长距离依赖关系，例如，识别出图像中一只猫的耳朵和尾巴虽然相距很远，但它们都属于“猫”这个整体。</li><li><strong>前馈网络 (Feed-Forward Network)</strong>：在自注意力层之后，每个位置的输出都会经过一个简单的全连接前馈网络，进行非线性变换，增加模型的表达能力。</li></ol></li><li><strong>残差连接与层归一化</strong>：在每个子模块（自注意力和前馈网络）的周围，都使用了残差连接（Residual Connection）和层归一化（Layer Normalization），这对于训练深度 Transformer 模型至关重要，可以有效防止梯度消失和爆炸问题。</li><li><strong>操作</strong>：包含 <code>[class]</code> 令牌和位置信息的嵌入序列会依次通过所有 Encoder 层。在每一层中，信息都会在所有图像块之间进行流动和整合。</li></ul><p><strong>第5步：分类 (Classification)</strong></p><ul><li><strong>提取聚合信息</strong>：当序列通过所有 Encoder 层后，我们只关心序列第一个位置，也就是 <code>[class]</code> 令牌对应的输出向量。此时，这个向量已经通过自注意力机制充分聚合了整张图像的全局信息。</li><li><strong>送入分类头</strong>：将这个 <code>D</code> 维的向量送入一个分类头（通常是一个简单的多层感知机，MLP Head），最终输出对各个类别的预测概率。</li></ul><h4 id="为什么-vit-能够有效处理图像数据" tabindex="-1"><a class="header-anchor" href="#为什么-vit-能够有效处理图像数据"><span>为什么 ViT 能够有效处理图像数据？</span></a></h4><p>ViT 的成功并非偶然，其核心在于它巧妙地解决了 Transformer 处理图像的几个关键挑战：</p><ul><li><p><strong>全局感受野</strong>：传统的 CNN 通过堆叠卷积层来逐步扩大感受野，需要很多层才能捕捉到全局信息。而 ViT 中的自注意力机制在第一层就能够建立图像中任意两个块之间的关系，从而拥有 <strong>天然的全局感受野</strong>。这对于理解需要长距离上下文信息的复杂场景至关重要。</p></li><li><p><strong>灵活性与可扩展性</strong>：与卷积核大小固定的 CNN 不同，ViT 的结构更具灵活性。通过调整模型深度、宽度以及注意力头的数量，可以轻松地扩展模型规模。研究表明，当拥有足够大的数据集（如 JFT-300M）进行预训练时，更大规模的 ViT 模型性能会持续提升，展现出比 CNN 更强的可扩展性。</p></li><li><p><strong>参数效率</strong>：在处理高分辨率图像时，ViT 将图像分割成块，使得输入序列的长度与图像分辨率的平方成正比，而不是像素总数。这种设计避免了在每个像素上计算注意力，从而在计算上是可行的。</p></li></ul><p>总而言之，ViT 并非对传统 Transformer 的简单照搬，而是通过 <strong>将图像块化为序列</strong>、<strong>引入 <code>[class]</code> 令牌进行信息聚合</strong> 以及 <strong>仅使用编码器</strong> 等一系列精巧的改造，成功地将 Transformer 强大的序列建模能力释放到了计算机视觉领域，开启了一个全新的研究范式。新的研究范式。</p>',21)])])}const l=e(i,[["render",s]]),g=JSON.parse(`{"path":"/posts/Machine_learning/CV/ViT.html","title":"ViT","lang":"en-US","frontmatter":{"date":"2025-09-10T20:49:27.000Z","title":"ViT","category":"Machine Learning","tags":["Machine Learning","CV"],"description":"Vision Transformer (ViT)：从文本到图像的革命性跨越 Vision Transformer (ViT) 的出现，标志着深度学习领域的一个重要转折点。它成功地将原本在自然语言处理（NLP）领域大放异彩的 Transformer 模型，应用于计算机视觉领域，并取得了与顶级卷积神经网络（CNN）相媲美的性能。要理解 ViT 的精髓，我们...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"ViT\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-09-10T20:49:27.000Z\\",\\"dateModified\\":\\"2025-11-26T03:53:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/Machine_learning/CV/ViT.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"ViT"}],["meta",{"property":"og:description","content":"Vision Transformer (ViT)：从文本到图像的革命性跨越 Vision Transformer (ViT) 的出现，标志着深度学习领域的一个重要转折点。它成功地将原本在自然语言处理（NLP）领域大放异彩的 Transformer 模型，应用于计算机视觉领域，并取得了与顶级卷积神经网络（CNN）相媲美的性能。要理解 ViT 的精髓，我们..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-11-26T03:53:06.000Z"}],["meta",{"property":"article:tag","content":"CV"}],["meta",{"property":"article:tag","content":"Machine Learning"}],["meta",{"property":"article:published_time","content":"2025-09-10T20:49:27.000Z"}],["meta",{"property":"article:modified_time","content":"2025-11-26T03:53:06.000Z"}]]},"git":{"createdTime":1757508567000,"updatedTime":1764129186000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":5,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":7.44,"words":2232},"filePathRelative":"posts/Machine_learning/CV/ViT.md","excerpt":"<h3>Vision Transformer (ViT)：从文本到图像的革命性跨越</h3>\\n<p>Vision Transformer (ViT) 的出现，标志着深度学习领域的一个重要转折点。它成功地将原本在自然语言处理（NLP）领域大放异彩的 Transformer 模型，应用于计算机视觉领域，并取得了与顶级卷积神经网络（CNN）相媲美的性能。要理解 ViT 的精髓，我们需要先深入了解它在传统 Encoder-Decoder Transformer 架构上所做的关键改变，并剖析其处理图像数据的每一步。</p>","autoDesc":true}`);export{l as comp,g as data};
