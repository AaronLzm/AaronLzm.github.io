import{_ as t}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as e,f as r,b as n}from"./app-DdlDaJJl.js";const i={};function a(c,o){return n(),e("div",null,[...o[0]||(o[0]=[r('<p>非常好，这个问题直击了PPO算法设计的精髓。PPO通过其核心机制<strong>间接地、但非常有效地</strong>应对了策略上的“模式坍塌”（即过早收敛到局部最优）。</p><p>它并非像熵正则化那样<strong>直接</strong>奖励多样性，而是通过一种更根本的方式——<strong>保证优化过程的稳定性</strong>——来避免策略过早地变得狭隘和确定。</p><p>PPO主要通过以下两个相互关联的机制来做到这一点：</p><hr><h3 id="_1-核心机制-截断的替代目标-clipped-surrogate-objective" tabindex="-1"><a class="header-anchor" href="#_1-核心机制-截断的替代目标-clipped-surrogate-objective"><span>1. 核心机制：截断的替代目标 (Clipped Surrogate Objective)</span></a></h3><p>这是PPO的灵魂。它的目标是：<strong>在不让新策略偏离旧策略太远的前提下，尽可能地提升策略。</strong></p><p>让我们把它拆解开来看：</p><ul><li><p><strong>策略比例 (Probability Ratio) <code>r_t(θ)</code></strong>：<br> 这个值的计算方法是：<code>r_t = π_θ(a|s) / π_θ_old(a|s)</code>。</p><ul><li><code>π_θ(a|s)</code> 是<strong>新策略</strong>（正在优化的策略）在状态 <code>s</code> 下选择动作 <code>a</code> 的概率。</li><li><code>π_θ_old(a|s)</code> 是<strong>旧策略</strong>（本次更新开始前的策略）的概率。</li><li>如果 <code>r_t &gt; 1</code>，说明这个动作在新策略下变得更可能被选中了。</li><li>如果 <code>r_t &lt; 1</code>，说明这个动作在新策略下变得更不可能被选中了。</li></ul></li><li><p><strong>截断 (Clipping)</strong>：<br> PPO设置了一个超参数 <code>ε</code> (epsilon，通常是0.1或0.2)。然后，它将策略比例 <code>r_t</code> 限制在一个<strong>安全区间</strong>内： <code>[1 - ε, 1 + ε]</code>。</p><ul><li>这意味着，即使某个动作的“优势”（Advantage）非常高，PPO也不允许新策略将选中该动作的概率无限拔高。最多只能提升到旧策略的 <code>1 + ε</code> 倍。</li><li>反之，即使一个动作的优势非常差，PPO也不允许新策略过度惩罚它，最多将概率降低到旧策略的 <code>1 - ε</code> 倍。</li></ul></li></ul><p><strong>这个“截断”机制如何防止模式坍塌？</strong></p><p>想象一下，Actor（演员）偶然发现了一个“还不错”的动作，这个动作带来了比预期要好得多的奖励（即高优势）。</p><ul><li><p><strong>没有PPO的普通策略梯度</strong>：会像打了兴奋剂一样，疯狂地增加这个动作被选中的概率，可能在一次更新后，策略就从“50%概率选A”变成了“99%概率选A”。策略迅速收敛，<strong>探索性急剧下降</strong>，这就是模式坍塌。它再也没有机会去发现可能比A更好的动作B了。</p></li><li><p><strong>有PPO</strong>：当新策略试图将动作A的概率大幅提升时，<code>r_t</code> 会迅速增长并超过 <code>1 + ε</code>。此时，<strong>截断机制会介入</strong>，将<code>r_t</code>强行“按回”到 <code>1 + ε</code>。这意味着，本次更新对这个动作的奖励是<strong>有上限的</strong>。策略只能从“50%概率选A”温和地变成，比如说，“60%概率选A”。</p></li></ul><p><strong>结论：PPO通过限制单次更新的步长，强制策略进行“小步慢跑”式的优化，而不是“百米冲刺”。这种温和的更新方式，使得策略在更长的时间内保持其随机性和探索性，大大降低了过早陷入局部最优（模式坍塌）的风险。</strong></p><hr><h3 id="_2-优势函数-advantage-function-的使用" tabindex="-1"><a class="header-anchor" href="#_2-优势函数-advantage-function-的使用"><span>2. 优势函数 (Advantage Function) 的使用</span></a></h3><p>PPO作为一种Actor-Critic算法，它不只是看奖励的绝对值，而是使用<strong>优势函数 A(s, a)</strong> 来指导策略更新。</p><ul><li><code>A(s, a) = Q(s, a) - V(s)</code></li><li>它衡量的不是“动作a好不好”，而是“<strong>在状态s下，动作a比平均水平好多少</strong>”。</li></ul><p>这提供了一个更稳定、方差更低的基线。它避免了智能体仅仅因为某个状态本身价值很高，就错误地加强在该状态下采取的所有动作。通过关注“超出预期的好坏”，优势函数为策略更新提供了更精确的信号，这有助于引导Actor进行更有效的探索，而不是简单地固化任何能带来正奖励的行为。</p><h3 id="总结-ppo与模式坍塌的关系" tabindex="-1"><a class="header-anchor" href="#总结-ppo与模式坍塌的关系"><span>总结：PPO与模式坍塌的关系</span></a></h3><p>我们可以用一个比喻来理解：</p><ul><li><p>一个<strong>激进的登山者（普通策略梯度）</strong>，一旦发现一条看起来向上的路，就会不顾一切地冲上去，结果很可能被困在一个小山峰上（局部最优），错过了通往主峰（全局最优）的真正路径。</p></li><li><p>一个<strong>谨慎的登山者（PPO）</strong>，他每次只向上走一小段固定的距离（由<code>ε</code>限制）。即使他发现了一条看似很好的路，他也会小步前进，并不断地重新评估周围的环境。这种谨慎的策略让他不容易被困住，有更多的机会发现并转向那条通往真正顶峰的道路。</p></li></ul><p>因此，PPO通过其核心的<strong>截断机制</strong>来维持训练的<strong>稳定性</strong>，而这种稳定性又<strong>间接地保留了策略的探索性</strong>，从而成为了对抗策略“模式坍塌”的一道坚固防线。在实际应用中，PPO的目标函数里通常还会<strong>显式地加入熵正则化项</strong>，双管齐下，进一步鼓励探索，让策略更加稳健。��，进一步鼓励探索，让策略更加稳健。</p>',21)])])}const l=t(i,[["render",a]]),g=JSON.parse(`{"path":"/posts/machine_learning/reinforcement%20learning/actor-critic/How%20PPO%20improved%20on%20mode%20collapsing.html","title":"How PPO improved on mode collapsing","lang":"en-US","frontmatter":{"title":"How PPO improved on mode collapsing","category":"Machine Learning","tags":["Machine Learning","actor-critic"],"description":"非常好，这个问题直击了PPO算法设计的精髓。PPO通过其核心机制间接地、但非常有效地应对了策略上的“模式坍塌”（即过早收敛到局部最优）。 它并非像熵正则化那样直接奖励多样性，而是通过一种更根本的方式——保证优化过程的稳定性——来避免策略过早地变得狭隘和确定。 PPO主要通过以下两个相互关联的机制来做到这一点： 1. 核心机制：截断的替代目标 (Clip...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"How PPO improved on mode collapsing\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-09-16T12:31:49.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Aaron L\\",\\"url\\":\\"https://aaronlzm.github.io\\",\\"email\\":\\"lzm_aaron@outlook.com\\"}]}"],["meta",{"property":"og:url","content":"https://aaronlzm.github.io/posts/machine_learning/reinforcement%20learning/actor-critic/How%20PPO%20improved%20on%20mode%20collapsing.html"}],["meta",{"property":"og:site_name","content":"Aaron's Blog"}],["meta",{"property":"og:title","content":"How PPO improved on mode collapsing"}],["meta",{"property":"og:description","content":"非常好，这个问题直击了PPO算法设计的精髓。PPO通过其核心机制间接地、但非常有效地应对了策略上的“模式坍塌”（即过早收敛到局部最优）。 它并非像熵正则化那样直接奖励多样性，而是通过一种更根本的方式——保证优化过程的稳定性——来避免策略过早地变得狭隘和确定。 PPO主要通过以下两个相互关联的机制来做到这一点： 1. 核心机制：截断的替代目标 (Clip..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2025-09-16T12:31:49.000Z"}],["meta",{"property":"article:tag","content":"actor-critic"}],["meta",{"property":"article:tag","content":"Machine Learning"}],["meta",{"property":"article:modified_time","content":"2025-09-16T12:31:49.000Z"}]]},"git":{"createdTime":1757508567000,"updatedTime":1758025909000,"contributors":[{"name":"lizhimou","username":"lizhimou","email":"lizhimou@bytedance.com","commits":3,"url":"https://github.com/lizhimou"}]},"readingTime":{"minutes":4.36,"words":1309},"filePathRelative":"posts/machine_learning/reinforcement learning/actor-critic/How PPO improved on mode collapsing.md","excerpt":"<p>非常好，这个问题直击了PPO算法设计的精髓。PPO通过其核心机制<strong>间接地、但非常有效地</strong>应对了策略上的“模式坍塌”（即过早收敛到局部最优）。</p>\\n<p>它并非像熵正则化那样<strong>直接</strong>奖励多样性，而是通过一种更根本的方式——<strong>保证优化过程的稳定性</strong>——来避免策略过早地变得狭隘和确定。</p>\\n<p>PPO主要通过以下两个相互关联的机制来做到这一点：</p>","autoDesc":true}`);export{l as comp,g as data};
