/**
* @vue/shared v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**//*! #__NO_SIDE_EFFECTS__ */function xt(e){const t=Object.create(null);for(const n of e.split(","))t[n]=1;return n=>n in t}const Ot={},St=()=>{},ze=Object.assign,It=Object.prototype.hasOwnProperty,se=(e,t)=>It.call(e,t),z=Array.isArray,Y=e=>Ve(e)==="[object Map]",oe=e=>typeof e=="function",Ce=e=>typeof e=="string",L=e=>typeof e=="symbol",B=e=>e!==null&&typeof e=="object",Nt=Object.prototype.toString,Ve=e=>Nt.call(e),ke=e=>Ve(e).slice(8,-1),ie=e=>Ce(e)&&e!=="NaN"&&e[0]!=="-"&&""+parseInt(e,10)===e,Mt=e=>{const t=Object.create(null);return n=>t[n]||(t[n]=e(n))},Ln=Mt(e=>e.charAt(0).toUpperCase()+e.slice(1)),W=(e,t)=>!Object.is(e,t);let $e;const ce=()=>$e||($e=typeof globalThis<"u"?globalThis:typeof self<"u"?self:typeof window<"u"?window:typeof global<"u"?global:{});/**
* @vue/reactivity v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/function Rt(e,...t){console.warn(`[Vue warn] ${e}`,...t)}let Tt,je=0,ae;function ue(){je++}function le(){if(--je>0)return;let e;for(;ae;){let t=ae;for(ae=void 0;t;){const n=t.next;if(t.next=void 0,t.flags&=-9,t.flags&1)try{t.trigger()}catch(s){e||(e=s)}t=n}}if(e)throw e}let G=!0;const De=[];function fe(){De.push(G),G=!1}function de(){const e=De.pop();G=e===void 0?!0:e}class Fe{constructor(t){this.computed=t,this.version=0,this.activeLink=void 0,this.subs=void 0,this.map=void 0,this.key=void 0,this.sc=0,this.__v_skip=!0}track(t){}trigger(t){this.version++,this.notify(t)}notify(t){ue();try{for(let n=this.subs;n;n=n.prevSub)n.sub.notify()&&n.sub.dep.notify()}finally{le()}}}const he=new WeakMap,C=Symbol(""),pe=Symbol(""),H=Symbol("");function E(e,t,n){if(G&&Tt){let s=he.get(e);s||he.set(e,s=new Map);let r=s.get(n);r||(s.set(n,r=new Fe),r.map=s,r.key=n),r.track()}}function I(e,t,n,s,r,o){const i=he.get(e);if(!i)return;const c=a=>{a&&a.trigger()};if(ue(),t==="clear")i.forEach(c);else{const a=z(e),u=a&&ie(n);if(a&&n==="length"){const l=Number(s);i.forEach((f,d)=>{(d==="length"||d===H||!L(d)&&d>=l)&&c(f)})}else switch((n!==void 0||i.has(void 0))&&c(i.get(n)),u&&c(i.get(H)),t){case"add":a?u&&c(i.get("length")):(c(i.get(C)),Y(e)&&c(i.get(pe)));break;case"delete":a||(c(i.get(C)),Y(e)&&c(i.get(pe)));break;case"set":Y(e)&&c(i.get(C));break}}le()}function $(e){const t=p(e);return t===e?t:(E(t,"iterate",H),N(e)?t:t.map(v))}function ge(e){return E(e=p(e),"iterate",H),e}const zt={__proto__:null,[Symbol.iterator](){return _e(this,Symbol.iterator,v)},concat(...e){return $(this).concat(...e.map(t=>z(t)?$(t):t))},entries(){return _e(this,"entries",e=>(e[1]=v(e[1]),e))},every(e,t){return O(this,"every",e,t,void 0,arguments)},filter(e,t){return O(this,"filter",e,t,n=>n.map(v),arguments)},find(e,t){return O(this,"find",e,t,v,arguments)},findIndex(e,t){return O(this,"findIndex",e,t,void 0,arguments)},findLast(e,t){return O(this,"findLast",e,t,v,arguments)},findLastIndex(e,t){return O(this,"findLastIndex",e,t,void 0,arguments)},forEach(e,t){return O(this,"forEach",e,t,void 0,arguments)},includes(...e){return me(this,"includes",e)},indexOf(...e){return me(this,"indexOf",e)},join(e){return $(this).join(e)},lastIndexOf(...e){return me(this,"lastIndexOf",e)},map(e,t){return O(this,"map",e,t,void 0,arguments)},pop(){return K(this,"pop")},push(...e){return K(this,"push",e)},reduce(e,...t){return Pe(this,"reduce",e,t)},reduceRight(e,...t){return Pe(this,"reduceRight",e,t)},shift(){return K(this,"shift")},some(e,t){return O(this,"some",e,t,void 0,arguments)},splice(...e){return K(this,"splice",e)},toReversed(){return $(this).toReversed()},toSorted(e){return $(this).toSorted(e)},toSpliced(...e){return $(this).toSpliced(...e)},unshift(...e){return K(this,"unshift",e)},values(){return _e(this,"values",v)}};function _e(e,t,n){const s=ge(e),r=s[t]();return s!==e&&!N(e)&&(r._next=r.next,r.next=()=>{const o=r._next();return o.value&&(o.value=n(o.value)),o}),r}const Ct=Array.prototype;function O(e,t,n,s,r,o){const i=ge(e),c=i!==e&&!N(e),a=i[t];if(a!==Ct[t]){const f=a.apply(e,o);return c?v(f):f}let u=n;i!==e&&(c?u=function(f,d){return n.call(this,v(f),d,e)}:n.length>2&&(u=function(f,d){return n.call(this,f,d,e)}));const l=a.call(i,u,s);return c&&r?r(l):l}function Pe(e,t,n,s){const r=ge(e);let o=n;return r!==e&&(N(e)?n.length>3&&(o=function(i,c,a){return n.call(this,i,c,a,e)}):o=function(i,c,a){return n.call(this,i,v(c),a,e)}),r[t](o,...s)}function me(e,t,n){const s=p(e);E(s,"iterate",H);const r=s[t](...n);return(r===-1||r===!1)&&Jt(n[0])?(n[0]=p(n[0]),s[t](...n)):r}function K(e,t,n=[]){fe(),ue();const s=p(e)[t].apply(e,n);return le(),de(),s}const Vt=xt("__proto__,__v_isRef,__isVue"),Ae=new Set(Object.getOwnPropertyNames(Symbol).filter(e=>e!=="arguments"&&e!=="caller").map(e=>Symbol[e]).filter(L));function kt(e){L(e)||(e=String(e));const t=p(this);return E(t,"has",e),t.hasOwnProperty(e)}class Le{constructor(t=!1,n=!1){this._isReadonly=t,this._isShallow=n}get(t,n,s){if(n==="__v_skip")return t.__v_skip;const r=this._isReadonly,o=this._isShallow;if(n==="__v_isReactive")return!r;if(n==="__v_isReadonly")return r;if(n==="__v_isShallow")return o;if(n==="__v_raw")return s===(r?o?Kt:Ke:o?Ht:He).get(t)||Object.getPrototypeOf(t)===Object.getPrototypeOf(s)?t:void 0;const i=z(t);if(!r){let a;if(i&&(a=zt[n]))return a;if(n==="hasOwnProperty")return kt}const c=Reflect.get(t,n,V(t)?t:s);return(L(n)?Ae.has(n):Vt(n))||(r||E(t,"get",n),o)?c:V(c)?i&&ie(n)?c:c.value:B(c)?r?Ue(c):qe(c):c}}class $t extends Le{constructor(t=!1){super(!1,t)}set(t,n,s,r){let o=t[n];if(!this._isShallow){const a=j(o);if(!N(s)&&!j(s)&&(o=p(o),s=p(s)),!z(t)&&V(o)&&!V(s))return a?!1:(o.value=s,!0)}const i=z(t)&&ie(n)?Number(n)<t.length:se(t,n),c=Reflect.set(t,n,s,V(t)?t:r);return t===p(r)&&(i?W(s,o)&&I(t,"set",n,s,o):I(t,"add",n,s)),c}deleteProperty(t,n){const s=se(t,n),r=t[n],o=Reflect.deleteProperty(t,n);return o&&s&&I(t,"delete",n,void 0,r),o}has(t,n){const s=Reflect.has(t,n);return(!L(n)||!Ae.has(n))&&E(t,"has",n),s}ownKeys(t){return E(t,"iterate",z(t)?"length":C),Reflect.ownKeys(t)}}class jt extends Le{constructor(t=!1){super(!0,t)}set(t,n){return!0}deleteProperty(t,n){return!0}}const Dt=new $t,Ft=new jt,ye=e=>e,Q=e=>Reflect.getPrototypeOf(e);function Pt(e,t,n){return function(...s){const r=this.__v_raw,o=p(r),i=Y(o),c=e==="entries"||e===Symbol.iterator&&i,a=e==="keys"&&i,u=r[e](...s),l=n?ye:t?we:v;return!t&&E(o,"iterate",a?pe:C),{next(){const{value:f,done:d}=u.next();return d?{value:f,done:d}:{value:c?[l(f[0]),l(f[1])]:l(f),done:d}},[Symbol.iterator](){return this}}}}function X(e){return function(...t){return e==="delete"?!1:e==="clear"?void 0:this}}function At(e,t){const n={get(r){const o=this.__v_raw,i=p(o),c=p(r);e||(W(r,c)&&E(i,"get",r),E(i,"get",c));const{has:a}=Q(i),u=t?ye:e?we:v;if(a.call(i,r))return u(o.get(r));if(a.call(i,c))return u(o.get(c));o!==i&&o.get(r)},get size(){const r=this.__v_raw;return!e&&E(p(r),"iterate",C),Reflect.get(r,"size",r)},has(r){const o=this.__v_raw,i=p(o),c=p(r);return e||(W(r,c)&&E(i,"has",r),E(i,"has",c)),r===c?o.has(r):o.has(r)||o.has(c)},forEach(r,o){const i=this,c=i.__v_raw,a=p(c),u=t?ye:e?we:v;return!e&&E(a,"iterate",C),c.forEach((l,f)=>r.call(o,u(l),u(f),i))}};return ze(n,e?{add:X("add"),set:X("set"),delete:X("delete"),clear:X("clear")}:{add(r){!t&&!N(r)&&!j(r)&&(r=p(r));const o=p(this);return Q(o).has.call(o,r)||(o.add(r),I(o,"add",r,r)),this},set(r,o){!t&&!N(o)&&!j(o)&&(o=p(o));const i=p(this),{has:c,get:a}=Q(i);let u=c.call(i,r);u||(r=p(r),u=c.call(i,r));const l=a.call(i,r);return i.set(r,o),u?W(o,l)&&I(i,"set",r,o,l):I(i,"add",r,o),this},delete(r){const o=p(this),{has:i,get:c}=Q(o);let a=i.call(o,r);a||(r=p(r),a=i.call(o,r));const u=c?c.call(o,r):void 0,l=o.delete(r);return a&&I(o,"delete",r,void 0,u),l},clear(){const r=p(this),o=r.size!==0,i=void 0,c=r.clear();return o&&I(r,"clear",void 0,void 0,i),c}}),["keys","values","entries",Symbol.iterator].forEach(r=>{n[r]=Pt(r,e,t)}),n}function We(e,t){const n=At(e,t);return(s,r,o)=>r==="__v_isReactive"?!e:r==="__v_isReadonly"?e:r==="__v_raw"?s:Reflect.get(se(n,r)&&r in s?n:s,r,o)}const Lt={get:We(!1,!1)},Wt={get:We(!0,!1)};function Wn(e,t,n){const s=p(n);if(s!==n&&t.call(e,s)){const r=ke(e);Rt(`Reactive ${r} contains both the raw and reactive versions of the same object${r==="Map"?" as keys":""}, which can lead to inconsistencies. Avoid differentiating between the raw and reactive versions of an object and only use the reactive version if possible.`)}}const He=new WeakMap,Ht=new WeakMap,Ke=new WeakMap,Kt=new WeakMap;function qt(e){switch(e){case"Object":case"Array":return 1;case"Map":case"Set":case"WeakMap":case"WeakSet":return 2;default:return 0}}function Ut(e){return e.__v_skip||!Object.isExtensible(e)?0:qt(ke(e))}function qe(e){return j(e)?e:Je(e,!1,Dt,Lt,He)}function Ue(e){return Je(e,!0,Ft,Wt,Ke)}function Je(e,t,n,s,r){if(!B(e)||e.__v_raw&&!(t&&e.__v_isReactive))return e;const o=Ut(e);if(o===0)return e;const i=r.get(e);if(i)return i;const c=new Proxy(e,o===2?s:n);return r.set(e,c),c}function j(e){return!!(e&&e.__v_isReadonly)}function N(e){return!!(e&&e.__v_isShallow)}function Jt(e){return e?!!e.__v_raw:!1}function p(e){const t=e&&e.__v_raw;return t?p(t):e}const v=e=>B(e)?qe(e):e,we=e=>B(e)?Ue(e):e;function V(e){return e?e.__v_isRef===!0:!1}function Yt(e){return Bt(e,!1)}function Bt(e,t){return V(e)?e:new Gt(e,t)}class Gt{constructor(t,n){this.dep=new Fe,this.__v_isRef=!0,this.__v_isShallow=!1,this._rawValue=n?t:p(t),this._value=n?t:v(t),this.__v_isShallow=n}get value(){return this.dep.track(),this._value}set value(t){const n=this._rawValue,s=this.__v_isShallow||N(t)||j(t);t=s?t:p(t),W(t,n)&&(this._rawValue=t,this._value=s?t:v(t),this.dep.trigger())}}/**
* @vue/runtime-core v3.5.17
* (c) 2018-present Yuxi (Evan) You and Vue contributors
* @license MIT
**/const k=[];function Hn(e){k.push(e)}function Kn(){k.pop()}let be=!1;function qn(e,...t){if(be)return;be=!0,fe();const n=k.length?k[k.length-1].component:null,s=n&&n.appContext.config.warnHandler,r=Qt();if(s)ve(s,n,11,[e+t.map(o=>{var i,c;return(c=(i=o.toString)==null?void 0:i.call(o))!=null?c:JSON.stringify(o)}).join(""),n&&n.proxy,r.map(({vnode:o})=>`at <${et(n,o.type)}>`).join(`
`),r]);else{const o=[`[Vue warn]: ${e}`,...t];r.length&&o.push(`
`,...Xt(r)),console.warn(...o)}de(),be=!1}function Qt(){let e=k[k.length-1];if(!e)return[];const t=[];for(;e;){const n=t[0];n&&n.vnode===e?n.recurseCount++:t.push({vnode:e,recurseCount:0});const s=e.component&&e.component.parent;e=s&&s.vnode}return t}function Xt(e){const t=[];return e.forEach((n,s)=>{t.push(...s===0?[]:[`
`],...Zt(n))}),t}function Zt({vnode:e,recurseCount:t}){const n=t>0?`... (${t} recursive calls)`:"",s=e.component?e.component.parent==null:!1,r=` at <${et(e.component,e.type,s)}`,o=">"+n;return e.props?[r,...en(e.props),o]:[r+o]}function en(e){const t=[],n=Object.keys(e);return n.slice(0,3).forEach(s=>{t.push(...Ye(s,e[s]))}),n.length>3&&t.push(" ..."),t}function Ye(e,t,n){return Ce(t)?(t=JSON.stringify(t),n?t:[`${e}=${t}`]):typeof t=="number"||typeof t=="boolean"||t==null?n?t:[`${e}=${t}`]:V(t)?(t=Ye(e,p(t.value),!0),n?t:[`${e}=Ref<`,t,">"]):oe(t)?[`${e}=fn${t.name?`<${t.name}>`:""}`]:(t=p(t),n?t:[`${e}=`,t])}const Un={sp:"serverPrefetch hook",bc:"beforeCreate hook",c:"created hook",bm:"beforeMount hook",m:"mounted hook",bu:"beforeUpdate hook",u:"updated",bum:"beforeUnmount hook",um:"unmounted hook",a:"activated hook",da:"deactivated hook",ec:"errorCaptured hook",rtc:"renderTracked hook",rtg:"renderTriggered hook",0:"setup function",1:"render function",2:"watcher getter",3:"watcher callback",4:"watcher cleanup function",5:"native event handler",6:"component event handler",7:"vnode hook",8:"directive hook",9:"transition hook",10:"app errorHandler",11:"app warnHandler",12:"ref function",13:"async component loader",14:"scheduler flush",15:"component update",16:"app unmount cleanup function"};function ve(e,t,n,s){try{return s?e(...s):e()}catch(r){Be(r,t,n)}}function Be(e,t,n,s=!0){const r=t?t.vnode:null,{errorHandler:o,throwUnhandledErrorInProduction:i}=t&&t.appContext.config||Ot;if(t){let c=t.parent;const a=t.proxy,u=`https://vuejs.org/error-reference/#runtime-${n}`;for(;c;){const l=c.ec;if(l){for(let f=0;f<l.length;f++)if(l[f](e,a,u)===!1)return}c=c.parent}if(o){fe(),ve(o,null,10,[e,a,u]),de();return}}tn(e,n,r,s,i)}function tn(e,t,n,s=!0,r=!1){if(r)throw e;console.error(e)}const x=[];let S=-1;const D=[];let M=null,F=0;const nn=Promise.resolve();let Ee=null;const rn=100;function sn(e){let t=S+1,n=x.length;for(;t<n;){const s=t+n>>>1,r=x[s],o=q(r);o<e||o===e&&r.flags&2?t=s+1:n=s}return t}function on(e){if(!(e.flags&1)){const t=q(e),n=x[x.length-1];!n||!(e.flags&2)&&t>=q(n)?x.push(e):x.splice(sn(t),0,e),e.flags|=1,Ge()}}function Ge(){Ee||(Ee=nn.then(Qe))}function cn(e){z(e)?D.push(...e):M&&e.id===-1?M.splice(F+1,0,e):e.flags&1||(D.push(e),e.flags|=1),Ge()}function an(e){if(D.length){const t=[...new Set(D)].sort((n,s)=>q(n)-q(s));if(D.length=0,M){M.push(...t);return}for(M=t,F=0;F<M.length;F++){const n=M[F];n.flags&4&&(n.flags&=-2),n.flags&8||n(),n.flags&=-2}M=null,F=0}}const q=e=>e.id==null?e.flags&2?-1:1/0:e.id;function Qe(e){const t=St;try{for(S=0;S<x.length;S++){const n=x[S];n&&!(n.flags&8)&&(n.flags&4&&(n.flags&=-2),ve(n,n.i,n.i?15:14),n.flags&4||(n.flags&=-2))}}finally{for(;S<x.length;S++){const n=x[S];n&&(n.flags&=-2)}S=-1,x.length=0,an(e),Ee=null,(x.length||D.length)&&Qe(e)}}function Jn(e,t){const n=e.get(t)||0;if(n>rn){const s=t.i,r=s&&Ze(s.type);return Be(`Maximum recursive updates exceeded${r?` in component <${r}>`:""}. This means you have a reactive effect that is mutating its own dependencies and thus recursively triggering itself. Possible sources include component template, render function, updated hook or watcher source function.`,null,10),!0}return e.set(t,n+1),!1}const xe=new Map,Z=new Map;function Yn(e,t){return Z.has(e)?!1:(Z.set(e,{initialDef:ee(t),instances:new Set}),!0)}function ee(e){return fn(e)?e.__vccOpts:e}function Bn(e,t){const n=Z.get(e);n&&(n.initialDef.render=t,[...n.instances].forEach(s=>{t&&(s.render=t,ee(s.type).render=t),s.renderCache=[],s.update()}))}function Gn(e,t){const n=Z.get(e);if(!n)return;t=ee(t),Xe(n.initialDef,t);const s=[...n.instances];for(let r=0;r<s.length;r++){const o=s[r],i=ee(o.type);let c=xe.get(i);c||(i!==n.initialDef&&Xe(i,t),xe.set(i,c=new Set)),c.add(o),o.appContext.propsCache.delete(o.type),o.appContext.emitsCache.delete(o.type),o.appContext.optionsCache.delete(o.type),o.ceReload?(c.add(o),o.ceReload(t.styles),c.delete(o)):o.parent?on(()=>{o.parent.update(),c.delete(o)}):o.appContext.reload?o.appContext.reload():typeof window<"u"?window.location.reload():console.warn("[HMR] Root or manually mounted instance modified. Full reload required."),o.root.ce&&o!==o.root&&o.root.ce._removeChildStyle(i)}cn(()=>{xe.clear()})}function Xe(e,t){ze(e,t);for(const n in e)n!=="__file"&&!(n in t)&&delete e[n]}function Qn(e){return(t,n)=>{try{return e(t,n)}catch(s){console.error(s),console.warn("[HMR] Something went wrong during Vue component hot-reload. Full reload required.")}}}ce().requestIdleCallback,ce().cancelIdleCallback;const Xn={};{const e=ce(),t=(n,s)=>{let r;return(r=e[n])||(r=e[n]=[]),r.push(s),o=>{r.length>1?r.forEach(i=>i(o)):r[0](o)}};t("__VUE_INSTANCE_SETTERS__",n=>n),t("__VUE_SSR_SETTERS__",n=>n)}const un=/(?:^|[-_])(\w)/g,ln=e=>e.replace(un,t=>t.toUpperCase()).replace(/[-_]/g,"");function Ze(e,t=!0){return oe(e)?e.displayName||e.name:e.name||t&&e.__name}function et(e,t,n=!1){let s=Ze(t);if(!s&&t.__file){const r=t.__file.match(/([^/\\]+)\.\w+$/);r&&(s=r[1])}if(!s&&e&&e.parent){const r=o=>{for(const i in o)if(o[i]===t)return i};s=r(e.components||e.parent.type.components)||r(e.appContext.components)}return s?ln(s):n?"App":"Anonymous"}function fn(e){return oe(e)&&"__vccOpts"in e}const tt=()=>document.documentElement.getAttribute("data-theme")==="dark";[...new Array(6)].map((e,t)=>`[vp-content] h${t+1}`).join(",");const{entries:dn}=Object,{fromEntries:hn}=Object,nt=Yt(!1);typeof document<"u"&&(nt.value=tt(),new MutationObserver(()=>{nt.value=tt()}).observe(document.documentElement,{attributeFilter:["data-theme"],attributes:!0}));const pn="ENTRIES",rt="KEYS",st="VALUES",w="";class Oe{set;_type;_path;constructor(t,n){const s=t._tree,r=Array.from(s.keys());this.set=t,this._type=n,this._path=r.length>0?[{node:s,keys:r}]:[]}next(){const t=this.dive();return this.backtrack(),t}dive(){if(this._path.length===0)return{done:!0,value:void 0};const{node:t,keys:n}=P(this._path);if(P(n)===w)return{done:!1,value:this.result()};const s=t.get(P(n));return this._path.push({node:s,keys:Array.from(s.keys())}),this.dive()}backtrack(){if(this._path.length===0)return;const t=P(this._path).keys;t.pop(),!(t.length>0)&&(this._path.pop(),this.backtrack())}key(){return this.set._prefix+this._path.map(({keys:t})=>P(t)).filter(t=>t!==w).join("")}value(){return P(this._path).node.get(w)}result(){switch(this._type){case st:return this.value();case rt:return this.key();default:return[this.key(),this.value()]}}[Symbol.iterator](){return this}}const P=e=>e[e.length-1],gn=(e,t,n)=>{const s=new Map;if(typeof t!="string")return s;const r=t.length+1,o=r+n,i=new Uint8Array(o*r).fill(n+1);for(let c=0;c<r;++c)i[c]=c;for(let c=1;c<o;++c)i[c*r]=c;return ot(e,t,n,s,i,1,r,""),s},ot=(e,t,n,s,r,o,i,c)=>{const a=o*i;e:for(const u of e.keys())if(u===w){const l=r[a-1];l<=n&&s.set(c,[e.get(u),l])}else{let l=o;for(let f=0;f<u.length;++f,++l){const d=u[f],m=i*l,b=m-i;let g=r[m];const h=Math.max(0,l-n-1),_=Math.min(i-1,l+n);for(let y=h;y<_;++y){const R=d!==t[y],re=r[b+y]+ +R,J=r[b+y+1]+1,T=r[m+y]+1,A=r[m+y+1]=Math.min(re,J,T);A<g&&(g=A)}if(g>n)continue e}ot(e.get(u),t,n,s,r,l,i,c+u)}};let it=class U{_tree;_prefix;_size=void 0;constructor(t=new Map,n=""){this._tree=t,this._prefix=n}atPrefix(t){if(!t.startsWith(this._prefix))throw new Error("Mismatched prefix");const[n,s]=te(this._tree,t.slice(this._prefix.length));if(n===void 0){const[r,o]=Ne(s);for(const i of r.keys())if(i!==w&&i.startsWith(o)){const c=new Map;return c.set(i.slice(o.length),r.get(i)),new U(c,t)}}return new U(n,t)}clear(){this._size=void 0,this._tree.clear()}delete(t){return this._size=void 0,_n(this._tree,t)}entries(){return new Oe(this,pn)}forEach(t){for(const[n,s]of this)t(n,s,this)}fuzzyGet(t,n){return gn(this._tree,t,n)}get(t){const n=Se(this._tree,t);return n!==void 0?n.get(w):void 0}has(t){return Se(this._tree,t)?.has(w)??!1}keys(){return new Oe(this,rt)}set(t,n){if(typeof t!="string")throw new Error("key must be a string");return this._size=void 0,Ie(this._tree,t).set(w,n),this}get size(){if(this._size)return this._size;this._size=0;const t=this.entries();for(;!t.next().done;)this._size+=1;return this._size}update(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const s=Ie(this._tree,t);return s.set(w,n(s.get(w))),this}fetch(t,n){if(typeof t!="string")throw new Error("key must be a string");this._size=void 0;const s=Ie(this._tree,t);let r=s.get(w);return r===void 0&&s.set(w,r=n()),r}values(){return new Oe(this,st)}[Symbol.iterator](){return this.entries()}static from(t){const n=new U;for(const[s,r]of t)n.set(s,r);return n}static fromObject(t){return U.from(Object.entries(t))}};const te=(e,t,n=[])=>{if(t.length===0||e==null)return[e,n];for(const s of e.keys())if(s!==w&&t.startsWith(s))return n.push([e,s]),te(e.get(s),t.slice(s.length),n);return n.push([e,t]),te(void 0,"",n)},Se=(e,t)=>{if(t.length===0||!e)return e;for(const n of e.keys())if(n!==w&&t.startsWith(n))return Se(e.get(n),t.slice(n.length))},Ie=(e,t)=>{const n=t.length;e:for(let s=0;e&&s<n;){for(const o of e.keys())if(o!==w&&t[s]===o[0]){const i=Math.min(n-s,o.length);let c=1;for(;c<i&&t[s+c]===o[c];)++c;const a=e.get(o);if(c===o.length)e=a;else{const u=new Map;u.set(o.slice(c),a),e.set(t.slice(s,s+c),u),e.delete(o),e=u}s+=c;continue e}const r=new Map;return e.set(t.slice(s),r),r}return e},_n=(e,t)=>{const[n,s]=te(e,t);if(n!==void 0){if(n.delete(w),n.size===0)ct(s);else if(n.size===1){const[r,o]=n.entries().next().value;at(s,r,o)}}},ct=e=>{if(e.length===0)return;const[t,n]=Ne(e);if(t.delete(n),t.size===0)ct(e.slice(0,-1));else if(t.size===1){const[s,r]=t.entries().next().value;s!==w&&at(e.slice(0,-1),s,r)}},at=(e,t,n)=>{if(e.length===0)return;const[s,r]=Ne(e);s.set(r+t,n),s.delete(r)},Ne=e=>e[e.length-1],mn=(e,t)=>{const n=e._idToShortId.get(t);if(n!=null)return e._storedFields.get(n)},yn=/[\n\r\p{Z}\p{P}]+/u,Me="or",ut="and",wn="and_not",bn=(e,t)=>{e.includes(t)||e.push(t)},lt=(e,t)=>{for(const n of t)e.includes(n)||e.push(n)},ft=({score:e},{score:t})=>t-e,vn=()=>new Map,ne=e=>{const t=new Map;for(const n of Object.keys(e))t.set(parseInt(n,10),e[n]);return t},dt=(e,t)=>Object.prototype.hasOwnProperty.call(e,t)?e[t]:void 0,ht={[Me]:(e,t)=>{for(const n of t.keys()){const s=e.get(n);if(s==null)e.set(n,t.get(n));else{const{score:r,terms:o,match:i}=t.get(n);s.score=s.score+r,s.match=Object.assign(s.match,i),lt(s.terms,o)}}return e},[ut]:(e,t)=>{const n=new Map;for(const s of t.keys()){const r=e.get(s);if(r==null)continue;const{score:o,terms:i,match:c}=t.get(s);lt(r.terms,i),n.set(s,{score:r.score+o,terms:r.terms,match:Object.assign(r.match,c)})}return n},[wn]:(e,t)=>{for(const n of t.keys())e.delete(n);return e}},En=(e,t,n,s,r,o)=>{const{k:i,b:c,d:a}=o;return Math.log(1+(n-t+.5)/(t+.5))*(a+e*(i+1)/(e+i*(1-c+c*s/r)))},xn=e=>(t,n,s)=>({term:t,fuzzy:typeof e.fuzzy=="function"?e.fuzzy(t,n,s):e.fuzzy??!1,prefix:typeof e.prefix=="function"?e.prefix(t,n,s):e.prefix===!0,termBoost:typeof e.boostTerm=="function"?e.boostTerm(t,n,s):1}),pt=(e,t,n,s)=>{for(const r of Object.keys(e._fieldIds))if(e._fieldIds[r]===n){e._options.logger("warn",`SlimSearch: document with ID ${e._documentIds.get(t)} has changed before removal: term "${s}" was not present in field "${r}". Removing a document after it has changed can corrupt the index!`,"version_conflict");return}},On=(e,t,n,s)=>{if(!e._index.has(s)){pt(e,n,t,s);return}const r=e._index.fetch(s,vn),o=r.get(t),i=o?.get(n);!o||typeof i>"u"?pt(e,n,t,s):i<=1?o.size<=1?r.delete(t):o.delete(n):o.set(n,i-1),e._index.get(s).size===0&&e._index.delete(s)},Sn={k:1.2,b:.7,d:.5},In={idField:"id",extractField:(e,t)=>e[t],tokenize:e=>e.split(yn),processTerm:e=>e.toLowerCase(),fields:void 0,searchOptions:void 0,storeFields:[],logger:(e,t)=>{console?.[e]?.(t)},autoVacuum:!0},gt={combineWith:Me,prefix:!1,fuzzy:!1,maxFuzzy:6,boost:{},weights:{fuzzy:.45,prefix:.375},bm25:Sn},Nn={combineWith:ut,prefix:(e,t,n)=>t===n.length-1},Mn={batchSize:1e3,batchWait:10},_t={minDirtFactor:.1,minDirtCount:20},Rn={...Mn,..._t},mt=Symbol("*"),Tn=(e,t)=>{const n=new Map,s={...e._options.searchOptions,...t};for(const[r,o]of e._documentIds){const i=s.boostDocument?s.boostDocument(o,"",e._storedFields.get(r)):1;n.set(r,{score:i,terms:[],match:{}})}return n},yt=(e,t=Me)=>{if(e.length===0)return new Map;const n=t.toLowerCase();if(!(n in ht))throw new Error(`Invalid combination operator: ${t}`);return e.reduce(ht[n])},Re=(e,t,n,s,r,o,i,c,a,u=new Map)=>{if(o==null)return u;for(const l of Object.keys(i)){const f=i[l],d=e._fieldIds[l],m=o.get(d);if(m==null)continue;let b=m.size;const g=e._avgFieldLength[d];for(const h of m.keys()){if(!e._documentIds.has(h)){On(e,d,h,n),b-=1;continue}const _=c?c(e._documentIds.get(h),n,e._storedFields.get(h)):1;if(!_)continue;const y=m.get(h),R=e._fieldLength.get(h)[d],re=En(y,b,e._documentCount,R,g,a),J=s*r*f*_*re,T=u.get(h);if(T){T.score+=J,bn(T.terms,t);const A=dt(T.match,n);A?A.push(l):T.match[n]=[l]}else u.set(h,{score:J,terms:[t],match:{[n]:[l]}})}}return u},zn=(e,t,n)=>{const s={...e._options.searchOptions,...n},r=(s.fields??e._options.fields).reduce((g,h)=>({...g,[h]:dt(s.boost,h)||1}),{}),{boostDocument:o,weights:i,maxFuzzy:c,bm25:a}=s,{fuzzy:u,prefix:l}={...gt.weights,...i},f=e._index.get(t.term),d=Re(e,t.term,t.term,1,t.termBoost,f,r,o,a);let m,b;if(t.prefix&&(m=e._index.atPrefix(t.term)),t.fuzzy){const g=t.fuzzy===!0?.2:t.fuzzy,h=g<1?Math.min(c,Math.round(t.term.length*g)):g;h&&(b=e._index.fuzzyGet(t.term,h))}if(m)for(const[g,h]of m){const _=g.length-t.term.length;if(!_)continue;b?.delete(g);const y=l*g.length/(g.length+.3*_);Re(e,t.term,g,y,t.termBoost,h,r,o,a,d)}if(b)for(const g of b.keys()){const[h,_]=b.get(g);if(!_)continue;const y=u*g.length/(g.length+_);Re(e,t.term,g,y,t.termBoost,h,r,o,a,d)}return d},wt=(e,t,n={})=>{if(t===mt)return Tn(e,n);if(typeof t!="string"){const l={...n,...t,queries:void 0},f=t.queries.map(d=>wt(e,d,l));return yt(f,l.combineWith)}const{tokenize:s,processTerm:r,searchOptions:o}=e._options,i={tokenize:s,processTerm:r,...o,...n},{tokenize:c,processTerm:a}=i,u=c(t).flatMap(l=>a(l)).filter(l=>!!l).map(xn(i)).map(l=>zn(e,l,i));return yt(u,i.combineWith)},bt=(e,t,n={})=>{const{searchOptions:s}=e._options,r={...s,...n},o=wt(e,t,n),i=[];for(const[c,{score:a,terms:u,match:l}]of o){const f=u.length||1,d={id:e._documentIds.get(c),score:a*f,terms:Object.keys(l),queryTerms:u,match:l};Object.assign(d,e._storedFields.get(c)),(r.filter==null||r.filter(d))&&i.push(d)}return t===mt&&r.boostDocument==null||i.sort(ft),i},Cn=(e,t,n={})=>{n={...e._options.autoSuggestOptions,...n};const s=new Map;for(const{score:o,terms:i}of bt(e,t,n)){const c=i.join(" "),a=s.get(c);a!=null?(a.score+=o,a.count+=1):s.set(c,{score:o,terms:i,count:1})}const r=[];for(const[o,{score:i,terms:c,count:a}]of s)r.push({suggestion:o,terms:c,score:i/a});return r.sort(ft),r};class Vn{_options;_index;_documentCount;_documentIds;_idToShortId;_fieldIds;_fieldLength;_avgFieldLength;_nextId;_storedFields;_dirtCount;_currentVacuum;_enqueuedVacuum;_enqueuedVacuumConditions;constructor(t){if(!t?.fields)throw new Error('SlimSearch: option "fields" must be provided');const n=t.autoVacuum==null||t.autoVacuum===!0?Rn:t.autoVacuum;this._options={...In,...t,autoVacuum:n,searchOptions:{...gt,...t.searchOptions},autoSuggestOptions:{...Nn,...t.autoSuggestOptions}},this._index=new it,this._documentCount=0,this._documentIds=new Map,this._idToShortId=new Map,this._fieldIds={},this._fieldLength=new Map,this._avgFieldLength=[],this._nextId=0,this._storedFields=new Map,this._dirtCount=0,this._currentVacuum=null,this._enqueuedVacuum=null,this._enqueuedVacuumConditions=_t,this.addFields(this._options.fields)}get isVacuuming(){return this._currentVacuum!=null}get dirtCount(){return this._dirtCount}get dirtFactor(){return this._dirtCount/(1+this._documentCount+this._dirtCount)}get documentCount(){return this._documentCount}get termCount(){return this._index.size}toJSON(){const t=[];for(const[n,s]of this._index){const r={};for(const[o,i]of s)r[o]=Object.fromEntries(i);t.push([n,r])}return{documentCount:this._documentCount,nextId:this._nextId,documentIds:Object.fromEntries(this._documentIds),fieldIds:this._fieldIds,fieldLength:Object.fromEntries(this._fieldLength),averageFieldLength:this._avgFieldLength,storedFields:Object.fromEntries(this._storedFields),dirtCount:this._dirtCount,index:t,version:2}}addFields(t){for(let n=0;n<t.length;n++)this._fieldIds[t[n]]=n}}const kn=e=>new Vn(e),$n=({documentCount:e,nextId:t,fieldIds:n,averageFieldLength:s,dirtCount:r,version:o},i)=>{if(o!==2)throw new Error("SlimSearch: cannot deserialize an index created with an incompatible version");const c=kn(i);return c._documentCount=e,c._nextId=t,c._idToShortId=new Map,c._fieldIds=n,c._avgFieldLength=s,c._dirtCount=r??0,c._index=new it,c},jn=(e,t)=>{const{index:n,documentIds:s,fieldLength:r,storedFields:o}=e,i=$n(e,t);i._documentIds=ne(s),i._fieldLength=ne(r),i._storedFields=ne(o);for(const[c,a]of i._documentIds)i._idToShortId.set(a,c);for(const[c,a]of n){const u=new Map;for(const l of Object.keys(a))u.set(parseInt(l,10),ne(a[l]));i._index.set(c,u)}return i},Te=(e,t)=>{const n=e.toLowerCase(),s=t.toLowerCase(),r=[];let o=0,i=0;const c=(u,l=!1)=>{let f;i===0?f=u.length>20?`… ${u.slice(-20)}`:u:l?f=u.length+i>100?`${u.slice(0,100-i)}… `:u:f=u.length>20?`${u.slice(0,20)} … ${u.slice(-20)}`:u,f&&r.push(f),i+=f.length,l||(r.push(["mark",t]),i+=t.length,i>=100&&r.push(" …"))};let a=n.indexOf(s,o);if(a===-1)return null;for(;a>=0;){const u=a+s.length;if(c(e.slice(o,a)),o=u,i>100)break;a=n.indexOf(s,o)}return i<100&&c(e.slice(o),!0),r},{entries:Dn}=Object,Fn=(e,t)=>t.contents.reduce((n,[,s])=>n+s,0)-e.contents.reduce((n,[,s])=>n+s,0),Pn=(e,t)=>Math.max(...t.contents.map(([,n])=>n))-Math.max(...e.contents.map(([,n])=>n)),vt=(e,t,n={},s="max")=>{const r={};return bt(t,e,{boost:{h:2,t:1,c:4},prefix:!0,...n}).forEach(o=>{const{id:i,terms:c,score:a}=o,u=i.includes("@"),l=i.includes("#"),[f,d]=i.split(/[#@]/),m=Number(f),b=c.sort((h,_)=>h.length-_.length).filter((h,_)=>c.slice(_+1).every(y=>!y.includes(h))),{contents:g}=r[m]??={title:"",contents:[]};if(u)g.push([{type:"customField",id:m,index:d,display:b.map(h=>o.c.map(_=>Te(_,h))).flat().filter(h=>h!==null)},a]);else{const h=b.map(_=>Te(o.h,_)).filter(_=>_!==null);if(h.length&&g.push([{type:l?"heading":"title",id:m,...l&&{anchor:d},display:h},a]),"t"in o&&o.t)for(const _ of o.t){const y=b.map(R=>Te(_,R)).filter(R=>R!==null);y.length&&g.push([{type:"text",id:m,...l&&{anchor:d},display:y},a])}}}),Dn(r).sort(([,o],[,i])=>(s?Fn:Pn)(o,i)).map(([o,{title:i,contents:c}])=>{if(!i){const a=mn(t,o);a&&(i=a.h)}return{title:i,contents:c.map(([a])=>a)}})},Et=(e,t,n={})=>{const s=Cn(t,e,{fuzzy:.2,maxFuzzy:3,...n}).map(({suggestion:r})=>r);return e.includes(" ")?s:s.filter(r=>!r.includes(" "))},An=hn(dn(JSON.parse("{\"/zh/\":{\"documentCount\":10,\"nextId\":10,\"documentIds\":{\"0\":\"2\",\"1\":\"3\",\"2\":\"3#🎯-专业概述\",\"3\":\"3#💼-工作经历\",\"4\":\"3#云解决方案工程师-微软\",\"5\":\"3#🎓-教育背景\",\"6\":\"3#高级计算机科学硕士\",\"7\":\"3#物联网工程学士\",\"8\":\"3#🔧-技术栈\",\"9\":\"5\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1,10],\"1\":[3,12],\"2\":[2,15],\"3\":[2],\"4\":[2,23],\"5\":[2],\"6\":[1,9],\"7\":[1,3],\"8\":[2,22],\"9\":[2]},\"averageFieldLength\":[1.8,13.422222222222224],\"storedFields\":{\"0\":{\"h\":\"博客主页\",\"t\":[\"Game is never over, we are not checkmated yet.\"]},\"1\":{\"h\":\"李直谋(Aaron)\",\"t\":[\"✉️ lzm_aaron@outlook.com | 🌐 aaronlzm.github.io | 💼 领英主页\"]},\"2\":{\"h\":\"🎯 专业概述\",\"t\":[\"云计算与机器学习工程师 | 5年+经验 核心能力：\",\"构建可扩展云服务（Azure PaaS/Kubernetes）\",\"实现工业级机器学习流水线\",\"设计DevOps自动化部署方案\",\"全栈应用开发（Vue.js/Python）\"]},\"3\":{\"h\":\"💼 工作经历\"},\"4\":{\"h\":\"| 微软\",\"t\":[\"(2020年至今)\",\"云服务优化\",\"成功解决900+例PaaS环境技术问题（涉及Azure函数/应用服务/容器应用/静态Web应用）\",\".NET内存转储分析与开源项目调试\",\"DevOps部署故障排查\",\"全栈开发\",\"基于Vue.js前端与Python RESTful API构建内部工作流系统\",\"实现容器化应用的自动扩缩容方案\",\"大语言模型工具开发\",\"基于Azure AI搜索与Autogen搭建RAG系统，优化内部技术文档检索准确率\",\"开发支持网页浏览/数据库查询的GPT功能模块\",\"实施精准提示词工程方案\"]},\"5\":{\"h\":\"🎓 教育背景\"},\"6\":{\"h\":\"\",\"t\":[\"纽卡斯尔大学 | 2019年毕业\",\"毕业论文：基于贝叶斯CNN的半监督生成对抗网络图像生成\",\"核心课程：分布式系统/机器学习/云架构\"]},\"7\":{\"h\":\"\",\"t\":[\"海南大学 | 2016年毕业\"]},\"8\":{\"h\":\"🔧 技术栈\",\"t\":[\"云计算\",\"机器学习\",\"编程语言\",\"开发工具\",\"Azure云服务\",\"生成式模型\",\"Python\",\"Kubernetes\",\"PaaS解决方案\",\"深度学习\",\"JavaScript\",\"Spark\",\"微服务架构\",\"自然语言处理\",\"SQL\",\"Hadoop\",\"CI/CD流水线\",\"贝叶斯方法\",\"Jenkins\",\"期待与您邮件沟通 📬\"]},\"9\":{\"h\":\"中文博文暂时没有更新。\"}},\"dirtCount\":0,\"index\":[[\"中文博文暂时没有更新\",{\"0\":{\"9\":1}}],[\"期待与您邮件沟通\",{\"1\":{\"8\":1}}],[\"贝叶斯方法\",{\"1\":{\"8\":1}}],[\"hadoop\",{\"1\":{\"8\":1}}],[\"sql\",{\"1\":{\"8\":1}}],[\"spark\",{\"1\":{\"8\":1}}],[\"自然语言处理\",{\"1\":{\"8\":1}}],[\"微服务架构\",{\"1\":{\"8\":1}}],[\"微软\",{\"0\":{\"4\":1}}],[\"jenkins\",{\"1\":{\"8\":1}}],[\"javascript\",{\"1\":{\"8\":1}}],[\"js前端与python\",{\"1\":{\"4\":1}}],[\"js\",{\"1\":{\"2\":1}}],[\"深度学习\",{\"1\":{\"8\":1}}],[\"生成式模型\",{\"1\":{\"8\":1}}],[\"开发工具\",{\"1\":{\"8\":1}}],[\"开发支持网页浏览\",{\"1\":{\"4\":1}}],[\"编程语言\",{\"1\":{\"8\":1}}],[\"技术栈\",{\"0\":{\"8\":1}}],[\"📬\",{\"1\":{\"8\":1}}],[\"🔧\",{\"0\":{\"8\":1}}],[\"💼\",{\"0\":{\"3\":1},\"1\":{\"1\":1}}],[\"海南大学\",{\"1\":{\"7\":1}}],[\"机器学习\",{\"1\":{\"6\":1,\"8\":1}}],[\"分布式系统\",{\"1\":{\"6\":1}}],[\"核心课程\",{\"1\":{\"6\":1}}],[\"核心能力\",{\"1\":{\"2\":1}}],[\"毕业论文\",{\"1\":{\"6\":1}}],[\"2016年毕业\",{\"1\":{\"7\":1}}],[\"2019年毕业\",{\"1\":{\"6\":1}}],[\"2020年至今\",{\"1\":{\"4\":1}}],[\"纽卡斯尔大学\",{\"1\":{\"6\":1}}],[\"教育背景\",{\"0\":{\"5\":1}}],[\"实施精准提示词工程方案\",{\"1\":{\"4\":1}}],[\"实现容器化应用的自动扩缩容方案\",{\"1\":{\"4\":1}}],[\"实现工业级机器学习流水线\",{\"1\":{\"2\":1}}],[\"数据库查询的gpt功能模块\",{\"1\":{\"4\":1}}],[\"优化内部技术文档检索准确率\",{\"1\":{\"4\":1}}],[\"基于贝叶斯cnn的半监督生成对抗网络图像生成\",{\"1\":{\"6\":1}}],[\"基于azure\",{\"1\":{\"4\":1}}],[\"基于vue\",{\"1\":{\"4\":1}}],[\"大语言模型工具开发\",{\"1\":{\"4\":1}}],[\"restful\",{\"1\":{\"4\":1}}],[\"全栈开发\",{\"1\":{\"4\":1}}],[\"全栈应用开发\",{\"1\":{\"2\":1}}],[\"devops部署故障排查\",{\"1\":{\"4\":1}}],[\"静态web应用\",{\"1\":{\"4\":1}}],[\"容器应用\",{\"1\":{\"4\":1}}],[\"应用服务\",{\"1\":{\"4\":1}}],[\"涉及azure函数\",{\"1\":{\"4\":1}}],[\"成功解决900+例paas环境技术问题\",{\"1\":{\"4\":1}}],[\"云计算\",{\"1\":{\"8\":1}}],[\"云计算与机器学习工程师\",{\"1\":{\"2\":1}}],[\"云架构\",{\"1\":{\"6\":1}}],[\"云服务优化\",{\"1\":{\"4\":1}}],[\"工作经历\",{\"0\":{\"3\":1}}],[\"python\",{\"1\":{\"2\":1,\"8\":1}}],[\"paas解决方案\",{\"1\":{\"8\":1}}],[\"paas\",{\"1\":{\"2\":1}}],[\"vue\",{\"1\":{\"2\":1}}],[\"设计devops自动化部署方案\",{\"1\":{\"2\":1}}],[\"kubernetes\",{\"1\":{\"2\":1,\"8\":1}}],[\"构建可扩展云服务\",{\"1\":{\"2\":1}}],[\"5年+经验\",{\"1\":{\"2\":1}}],[\"专业概述\",{\"0\":{\"2\":1}}],[\"🎓\",{\"0\":{\"5\":1}}],[\"🎯\",{\"0\":{\"2\":1}}],[\"🌐\",{\"1\":{\"1\":1}}],[\"领英主页\",{\"1\":{\"1\":1}}],[\"io\",{\"1\":{\"1\":1}}],[\"is\",{\"1\":{\"0\":1}}],[\"github\",{\"1\":{\"1\":1}}],[\"game\",{\"1\":{\"0\":1}}],[\"|\",{\"0\":{\"4\":1},\"1\":{\"1\":2,\"2\":1,\"6\":1,\"7\":1}}],[\"cd流水线\",{\"1\":{\"8\":1}}],[\"ci\",{\"1\":{\"8\":1}}],[\"com\",{\"1\":{\"1\":1}}],[\"checkmated\",{\"1\":{\"0\":1}}],[\"outlook\",{\"1\":{\"1\":1}}],[\"over\",{\"1\":{\"0\":1}}],[\"lzm\",{\"1\":{\"1\":1}}],[\"✉️\",{\"1\":{\"1\":1}}],[\"ai搜索与autogen搭建rag系统\",{\"1\":{\"4\":1}}],[\"api构建内部工作流系统\",{\"1\":{\"4\":1}}],[\"azure云服务\",{\"1\":{\"8\":1}}],[\"azure\",{\"1\":{\"2\":1}}],[\"aaronlzm\",{\"1\":{\"1\":1}}],[\"aaron\",{\"0\":{\"1\":1},\"1\":{\"1\":1}}],[\"are\",{\"1\":{\"0\":1}}],[\"李直谋\",{\"0\":{\"1\":1}}],[\"yet\",{\"1\":{\"0\":1}}],[\"net内存转储分析与开源项目调试\",{\"1\":{\"4\":1}}],[\"never\",{\"1\":{\"0\":1}}],[\"not\",{\"1\":{\"0\":1}}],[\"we\",{\"1\":{\"0\":1}}],[\"博客主页\",{\"0\":{\"0\":1}}]],\"version\":2},\"/\":{\"documentCount\":211,\"nextId\":211,\"documentIds\":{\"0\":\"0\",\"1\":\"1\",\"2\":\"1#🎯-brief-summary\",\"3\":\"1#💼-professional-experience\",\"4\":\"1#cloud-solutions-engineer-microsoft\",\"5\":\"1#🎓-education\",\"6\":\"1#msc-advanced-computer-science\",\"7\":\"1#bsc-internet-of-things\",\"8\":\"1#🔧-technical-expertise\",\"9\":\"4\",\"10\":\"6\",\"11\":\"7\",\"12\":\"7#gpt-content\",\"13\":\"8\",\"14\":\"9\",\"15\":\"10\",\"16\":\"11\",\"17\":\"11#prior-uncertainty-a-key-hurdle-in-advancing-bayesian-machine-learning\",\"18\":\"12\",\"19\":\"13\",\"20\":\"14\",\"21\":\"15\",\"22\":\"15#核心要点-重参数技巧是什么\",\"23\":\"15#详细解释-为什么重参数技巧能起作用\",\"24\":\"15#_1-分离随机性与参数-打通梯度路径\",\"25\":\"15#_2-显著降低梯度估计的方差\",\"26\":\"15#总结\",\"27\":\"16\",\"28\":\"16#vision-transformer-vit-从文本到图像的革命性跨越\",\"29\":\"16#vit-对传统-encoder-decoder-transformer-的三大核心改变\",\"30\":\"16#vit-算法的详细步骤解析\",\"31\":\"16#为什么-vit-能够有效处理图像数据\",\"32\":\"17\",\"33\":\"17#step-1-understanding-the-core-concept-of-energy-based-models\",\"34\":\"17#step-2-the-theorem-the-gibbs-boltzmann-distribution\",\"35\":\"17#step-3-how-to-build-a-model-based-on-this-principle\",\"36\":\"17#component-1-designing-the-energy-function-e-x\",\"37\":\"17#component-2-training-the-model\",\"38\":\"18\",\"39\":\"18#the-forward-process-systematically-destroying-an-image\",\"40\":\"18#the-reverse-process-learning-to-recreate-the-image\",\"41\":\"18#generating-a-new-image\",\"42\":\"19\",\"43\":\"19#key-technical-aspects-of-the-u-net-in-diffusion-models\",\"44\":\"20\",\"45\":\"20#_1-变分推断-vi-的核心问题是什么\",\"46\":\"20#_2-kl散度如何自然地出现\",\"47\":\"20#_3-为什么是-而不是\",\"48\":\"20#总结\",\"49\":\"21\",\"50\":\"21#什么是jensen-shannon-divergence-js散度\",\"51\":\"21#_1-数学定义\",\"52\":\"21#_2-为什么js散度可以作为距离\",\"53\":\"21#js散度与kl散度的对比总结\",\"54\":\"21#一个简单的例子\",\"55\":\"22\",\"56\":\"22#_1-从直观理解开始-从点到点的路径\",\"57\":\"22#_2-数学定义\",\"58\":\"22#l2-norm-欧几里得范数\",\"59\":\"22#l1-norm-曼哈顿范数\",\"60\":\"22#_3-几何意义-单位圆\",\"61\":\"22#_4-关键特性与对比\",\"62\":\"22#_5-在机器学习和数据科学中的应用\",\"63\":\"22#总结\",\"64\":\"23\",\"65\":\"23#_1-核心思想的传承-对抗过拟合\",\"66\":\"23#_2-在深度学习中的具体应用形式\",\"67\":\"23#l2正则化-权重衰减-weight-decay\",\"68\":\"23#l1正则化\",\"69\":\"23#_3-深度学习中的其他正则化-利器\",\"70\":\"23#总结与对比\",\"71\":\"24\",\"72\":\"24#第一步-svd-是什么-——-拆解数据的终极工具\",\"73\":\"24#第二步-svd-为什么在机器学习中如此强大-——-它的核心能力\",\"74\":\"24#第三步-svd-在机器学习中的具体应用场景\",\"75\":\"24#第四步-一个简单的例子——推荐系统\",\"76\":\"24#总结\",\"77\":\"25\",\"78\":\"26\",\"79\":\"27\",\"80\":\"28\",\"81\":\"28#核心三大算法结构-学习什么\",\"82\":\"28#_1-基于价值的算法-value-based\",\"83\":\"28#_2-基于策略的算法-policy-based\",\"84\":\"28#_3-演员-评论家算法-actor-critic\",\"85\":\"28#另外两个重要的分类维度-如何学\",\"86\":\"28#a-无模型-model-free-vs-有模型-model-based\",\"87\":\"28#b-在线策略-on-policy-vs-离线策略-off-policy\",\"88\":\"28#总结\",\"89\":\"29\",\"90\":\"29#detr算法-基于transformer的端到端目标检测新范式\",\"91\":\"29#核心思想\",\"92\":\"29#模型架构\",\"93\":\"29#优势与挑战\",\"94\":\"30\",\"95\":\"30#i-jepa-深入解析-它不仅是vit-更是一种创新的自监督学习框架\",\"96\":\"30#i-jepa的核心思想-在抽象空间中进行预测\",\"97\":\"30#i-jepa的架构-由vit构成的三大组件\",\"98\":\"30#多块掩码策略-鼓励学习语义信息\",\"99\":\"30#i-jepa与vit的关系-框架与组件\",\"100\":\"30#i-jepa的优势与意义\",\"101\":\"31\",\"102\":\"31#_1-gan中的模式坍塌-the-classic-case\",\"103\":\"31#_2-actor-critic中的-模式坍塌-the-analogue\",\"104\":\"31#核心共性与解决方案\",\"105\":\"32\",\"106\":\"32#_1-核心机制-截断的替代目标-clipped-surrogate-objective\",\"107\":\"32#_2-优势函数-advantage-function-的使用\",\"108\":\"32#总结-ppo与模式坍塌的关系\",\"109\":\"33\",\"110\":\"33#_1-基础-状态空间模型-ssm\",\"111\":\"33#_2-离散化-从连续时间到离散序列\",\"112\":\"33#_3-卷积模式-并行训练\",\"113\":\"33#_4-mamba-的核心创新-选择性-ssm\",\"114\":\"33#_5-计算挑战与高效算法\",\"115\":\"33#总结与意义\",\"116\":\"34\",\"117\":\"34#多头注意力-multi-head-attention-数学详解\",\"118\":\"34#_1-输入表示\",\"119\":\"34#_2-多头投影\",\"120\":\"34#_3-单头注意力计算\",\"121\":\"34#_4-多头输出拼接\",\"122\":\"34#_5-输出线性变换\",\"123\":\"34#完整数学公式汇总\",\"124\":\"34#维度变化可视化\",\"125\":\"34#数学优势分析\",\"126\":\"34#实例演算\",\"127\":\"34#与传统单头对比\",\"128\":\"34#为什么有效\",\"129\":\"35\",\"130\":\"35#核心思想-序列内部的-自我审视\",\"131\":\"35#一个生动的例子-解决指代问题\",\"132\":\"35#工作流程-与普通attention相同但来源不同\",\"133\":\"35#为什么self-attention如此强大\",\"134\":\"35#self-attention在transformer中的角色\",\"135\":\"35#总结\",\"136\":\"36\",\"137\":\"36#第1步-从输入向量创建query-key-value\",\"138\":\"36#第2步-计算注意力分数\",\"139\":\"36#第3步-应用softmax获得注意力权重\",\"140\":\"36#第4步-计算输出向量-上下文向量\",\"141\":\"36#总结与关键点\",\"142\":\"37\",\"143\":\"37#_1-输入表示-input-representation\",\"144\":\"37#_2-线性投影-linear-projections\",\"145\":\"37#_3-注意力分数计算-attention-scores\",\"146\":\"37#_4-缩放与归一化-scaling-softmax\",\"147\":\"37#a-缩放-防止点积过大\",\"148\":\"37#b-softmax-归一化\",\"149\":\"37#_5-加权求和-output-calculation\",\"150\":\"37#完整公式汇总\",\"151\":\"37#几何解释\",\"152\":\"37#实例演算-简化版\",\"153\":\"37#数学意义总结\",\"154\":\"38\",\"155\":\"38#第1步-计算关于-的梯度\",\"156\":\"38#第2步-计算关于-的梯度\",\"157\":\"38#第3步-计算关于-的梯度-这是最核心的一步\",\"158\":\"38#第4步-计算关于-和-的梯度\",\"159\":\"38#总结与直观理解\",\"160\":\"39\",\"161\":\"39#_1-1-1-encoder\",\"162\":\"39#_1-1-2-decoder\",\"163\":\"39#_1-1-3-linear-and-softmax-layer\",\"164\":\"40\",\"165\":\"40#一、rmsnorm-是什么\",\"166\":\"40#_1-核心思想\",\"167\":\"40#_2-数学公式\",\"168\":\"40#二、为什么有效-layernorm的痛点与rmsnorm的解决方案\",\"169\":\"40#_1-重新审视layernorm的目标\",\"170\":\"40#_2-rmsnorm的洞察-中心化可能不是必须的\",\"171\":\"40#_3-rmsnorm的有效性原理\",\"172\":\"40#三、总结与对比\",\"173\":\"40#为什么现在这么流行\",\"174\":\"42\",\"175\":\"42#reference\",\"176\":\"43\",\"177\":\"43#_1-rotary-position-embedding-rope-总结\",\"178\":\"43#latex-算式\",\"179\":\"43#_2-算法优点分析\",\"180\":\"43#_2-1-相对位置编码的天然集成\",\"181\":\"43#_2-2-旋转不变性和数学优雅\",\"182\":\"43#_2-3-长序列处理能力\",\"183\":\"43#_2-4-计算效率和简洁性\",\"184\":\"43#_3-总结\",\"185\":\"44\",\"186\":\"44#核心思想\",\"187\":\"44#关键步骤\",\"188\":\"44#为何有效\",\"189\":\"44#典型应用场景\",\"190\":\"44#经典变体\",\"191\":\"44#示例流程-图像分类\",\"192\":\"44#意义总结\",\"193\":\"45\",\"194\":\"46\",\"195\":\"47\",\"196\":\"48\",\"197\":\"49\",\"198\":\"50\",\"199\":\"51\",\"200\":\"52\",\"201\":\"53\",\"202\":\"54\",\"203\":\"55\",\"204\":\"56\",\"205\":\"57\",\"206\":\"58\",\"207\":\"59\",\"208\":\"60\",\"209\":\"61\",\"210\":\"62\"},\"fieldIds\":{\"h\":0,\"t\":1,\"c\":2},\"fieldLength\":{\"0\":[1],\"1\":[2,12],\"2\":[3,33],\"3\":[3],\"4\":[2,75],\"5\":[2],\"6\":[1,22],\"7\":[1,5],\"8\":[3,28],\"9\":[1,25],\"10\":[4,14],\"11\":[6,2],\"12\":[3,79],\"13\":[7,125],\"14\":[2,77],\"15\":[6,118],\"16\":[10],\"17\":[10,250],\"18\":[3,156],\"19\":[6,62],\"20\":[8,54],\"21\":[2,14],\"22\":[3,78],\"23\":[3,3],\"24\":[3,60],\"25\":[2,51],\"26\":[1,27],\"27\":[1],\"28\":[4,21],\"29\":[6,96],\"30\":[2,180],\"31\":[4,54],\"32\":[4,13],\"33\":[10,120],\"34\":[7,113],\"35\":[11,16],\"36\":[6,91],\"37\":[5,165],\"38\":[5,61],\"39\":[7,134],\"40\":[8,108],\"41\":[4,119],\"42\":[10,102],\"43\":[11,288],\"44\":[6,21],\"45\":[5,48],\"46\":[3,65],\"47\":[4,80],\"48\":[1,39],\"49\":[4,8],\"50\":[5,15],\"51\":[2,40],\"52\":[3,63],\"53\":[1,60],\"54\":[1,50],\"55\":[2,16],\"56\":[3,23],\"57\":[2,12],\"58\":[4,31],\"59\":[4,30],\"60\":[3,24],\"61\":[2,75],\"62\":[2,43],\"63\":[1,32],\"64\":[4,22],\"65\":[3,32],\"66\":[2,7],\"67\":[6,50],\"68\":[1,62],\"69\":[4,42],\"70\":[1,41],\"71\":[1,6],\"72\":[4,80],\"73\":[4,79],\"74\":[3,55],\"75\":[3,46],\"76\":[1,17],\"77\":[2,10],\"78\":[4,6],\"79\":[5,85],\"80\":[1,15],\"81\":[3,2],\"82\":[5,56],\"83\":[5,44],\"84\":[6,52],\"85\":[3,2],\"86\":[8,41],\"87\":[8,37],\"88\":[1,37],\"89\":[2],\"90\":[2,16],\"91\":[1,24],\"92\":[1,31],\"93\":[1,58],\"94\":[4],\"95\":[5,20],\"96\":[3,31],\"97\":[3,42],\"98\":[2,14],\"99\":[3,26],\"100\":[2,23],\"101\":[2,17],\"102\":[6,38],\"103\":[7,67],\"104\":[1,74],\"105\":[6,12],\"106\":[7,105],\"107\":[5,26],\"108\":[2,31],\"109\":[1,22],\"110\":[5,59],\"111\":[3,41],\"112\":[3,55],\"113\":[5,64],\"114\":[2,49],\"115\":[1,53],\"116\":[3],\"117\":[5,12],\"118\":[1,10],\"119\":[1,21],\"120\":[1,14],\"121\":[1,15],\"122\":[1,10],\"123\":[1,13],\"124\":[1,31],\"125\":[1,32],\"126\":[1,62],\"127\":[1,18],\"128\":[1,23],\"129\":[3,19],\"130\":[4,16],\"131\":[2,59],\"132\":[3,25],\"133\":[3,27],\"134\":[2,20],\"135\":[1,29],\"136\":[2,19],\"137\":[4,47],\"138\":[2,29],\"139\":[2,31],\"140\":[4,38],\"141\":[1,42],\"142\":[4,11],\"143\":[1,15],\"144\":[1,34],\"145\":[1,17],\"146\":[1],\"147\":[2,1],\"148\":[2,20],\"149\":[1,13],\"150\":[1,3],\"151\":[1,28],\"152\":[1,74],\"153\":[1,21],\"154\":[3,33],\"155\":[3,19],\"156\":[3,17],\"157\":[5,78],\"158\":[4,24],\"159\":[1,62],\"160\":[3,25],\"161\":[1,28],\"162\":[1,29],\"163\":[1,18],\"164\":[1,18],\"165\":[4],\"166\":[2,11],\"167\":[2,31],\"168\":[4,4],\"169\":[2,22],\"170\":[4,15],\"171\":[2,36],\"172\":[2,34],\"173\":[2,24],\"174\":[3,163],\"175\":[1,35],\"176\":[1,16],\"177\":[6,16],\"178\":[2,61],\"179\":[2,6],\"180\":[3,18],\"181\":[2,14],\"182\":[3,15],\"183\":[3,14],\"184\":[2,14],\"185\":[2,14],\"186\":[1,47],\"187\":[1,45],\"188\":[1,12],\"189\":[1,18],\"190\":[1,16],\"191\":[1],\"192\":[1,14],\"193\":[1,3],\"194\":[2],\"195\":[1],\"196\":[1],\"197\":[1],\"198\":[1],\"199\":[1],\"200\":[1],\"201\":[1],\"202\":[2],\"203\":[2],\"204\":[1],\"205\":[1],\"206\":[2],\"207\":[1],\"208\":[2],\"209\":[1],\"210\":[1]},\"averageFieldLength\":[2.90521327014218,42.00417137109521],\"storedFields\":{\"0\":{\"h\":\"Home\"},\"1\":{\"h\":\"Curriculum Vitae\",\"t\":[\"✉️ lzm_aaron@outlook.com | 🌐 aaronlzm.github.io | 💼 Linkedin\"]},\"2\":{\"h\":\"🎯 Brief Summary\",\"t\":[\"Cloud & ML engineer with 5+ years experience. Proven ability to:\",\"Build scalable cloud services (Azure PaaS products/Kubernetes)\",\"Implement ML pipelines for real-world applications\",\"Build devops pipeline\",\"Deliver full-stack applications (Vue.js/Python)\"]},\"3\":{\"h\":\"💼 Professional Experience\"},\"4\":{\"h\":\"| Microsoft\",\"t\":[\"(2020 - Present)\",\"Cloud Service Optimization\",\"900+ successful technical troubleshooting cases for PaaS environments (Azure Functions/App Service/Azure Container app/Azure static web app)\",\".Net dump analyzation/open source software sample code/project and troubleshooting\",\"Devops deployment troubleshooting\",\"Full-stack Development\",\"Built internal workflow system with Vue.js frontend and Python Restful API\",\"Implemented auto-scaling solutions for containerized applications\",\"LLM based tooling\",\"Built RAG systems retrieving internal troubleshooting guide using Azure AI search and autogen to boost accuracy\",\"GPT funcs for web browsing, quering databases\",\"Necessary prompt engineering\"]},\"5\":{\"h\":\"🎓 Education\"},\"6\":{\"h\":\"\",\"t\":[\"Newcastle University | Graduated: 2019\",\"Dissertation: Semi-supervised Bayesian CNN GANs for Image Generation\",\"Key Courses: Distributed Systems, Machine Learning, Cloud Architecture\"]},\"7\":{\"h\":\"\",\"t\":[\"Hainan University | Graduated: 2016\"]},\"8\":{\"h\":\"🔧 Technical Expertise\",\"t\":[\"Cloud\",\"ML/AI\",\"Languages\",\"Tools\",\"Azure Services\",\"Generative Models\",\"Python\",\"Kubernetes\",\"PaaS Solutions\",\"Deep Learning\",\"JavaScript\",\"Spark\",\"Microservices\",\"NLP Applications\",\"SQL\",\"Hadoop\",\"CI/CD Pipelines\",\"Bayesian Methods\",\"Jenkins\"]},\"9\":{\"h\":\"Resources\",\"t\":[\"Websites:https://www.deeplearning.ai/https://www.kaggle.com/https://github.com/huggingface\",\"Books MLAPP Foundation of ML\",\"IIya's sharing:https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE\"]},\"10\":{\"h\":\"All bayesian inference methods\",\"t\":[\"#Bayes\",\"Variational Inference\",\"Markov Chain Monte Carlo\",\"Laplace Approximation\",\"Stochastic Variational Inference\",\"Expectation-Maximization Algorithm\"]},\"11\":{\"h\":\"Bayes by backprop - naive variational inference\",\"t\":[\"#Bayes\"]},\"12\":{\"h\":\"GPT content:\",\"t\":[\"\\\"Bayes by Backprop\\\"的核心是使用变分推理来近似权重的后验分布。这一过程涉及到一些复杂的数学，但我会尽力将其分解为详细的步骤。\",\"假设我们的神经网络的权重是 w，我们的数据是 D，那么在贝叶斯推理中，我们希望计算权重的后验分布 p(w|D)。然而，这通常是非常困难的，因为这需要计算一个涉及到所有可能权重值的积分，这在实际中是不可行的。因此，我们使用变分推理来近似这个后验分布。\",\"在变分推理中，我们引入一个变分分布 q(w|θ)，这个分布是由参数θ描述的，通常我们假设这个分布是高斯分布。我们的目标是找到最优的参数θ*，使得q(w|θ*)最接近真实的后验分布p(w|D)。\",\"为了找到这个最优的参数θ*，我们需要优化一个叫做变分下界（Variational Lower Bound）或者证据下界（Evidence Lower Bound，ELBO）的量。ELBO定义为：\",\"ELBO(θ) = E_q[log p(D|w)] - KL(q(w|θ)||p(w))\",\"其中，E_q[log p(D|w)]是在变分分布q(w|θ)下，模型对数据的对数似然的期望，KL(q(w|θ)||p(w))是变分分布q(w|θ)和先验分布p(w)之间的KL散度（Kullback-Leibler Divergence）。\",\"我们的目标是最大化这个ELBO。为了实现这个，我们需要计算ELBO关于θ的梯度，然后使用一个优化算法（比如随机梯度下降）来更新θ。计算梯度的过程就是所谓的反向传播。\",\"在计算了梯度并更新了θ后，我们就得到了新的权重分布q(w|θ)。然后，我们可以在下一次前向传播时，从这个新的分布中采样权重。\",\"这个过程会反复进行，每次迭代都会使得我们的权重分布更接近真实的后验分布，也就是使得ELBO更大。最终，我们得到的不仅是一组能使得预测性能最优的权重，还有一个描述权重不确定性的分布，这对于理解模型的不确定性，防止过拟合，以及在面对新的、未见过的数据时提供有用的不确定性信息是非常有价值的。\"]},\"13\":{\"h\":\"Difference between MCMC and VI(SVI)\",\"t\":[\"#Bayes\",\"MCMC（Markov Chain Monte Carlo）和变分推断（Variational Inference）都是用于估计后验分布的技术，但它们在处理复杂度、精度和计算速度等方面存在一些差异。以下是这两种方法的优点和缺点的对比：\",\"MCMC：\",\"优点：\",\"精确性：MCMC方法，如Gibbs采样，Metropolis-Hastings，Hamiltonian Monte Carlo等，可以生成精确的后验样本，从而得到精确的后验分布（在足够多的迭代次数下）。\",\"通用性：MCMC可以处理各种复杂的模型和分布，包括那些无法解析计算的分布。\",\"不需要选择逼近分布：与变分推断不同，MCMC不需要选择一个逼近分布。\",\"缺点：\",\"计算成本：MCMC通常需要大量的计算时间和资源，特别是在大数据集和高维参数空间中。\",\"收敛速度：MCMC的收敛速度可能会很慢，特别是在存在高度相关参数或复杂后验分布的情况下。\",\"变分推断：\",\"优点：\",\"计算效率：变分推断通常比MCMC更快，因为它是通过优化问题来逼近后验分布的，而不是通过采样。\",\"可扩展性：变分推断更容易扩展到大数据集，因为它可以利用随机优化算法（如随机梯度下降）和小批量（mini-batch）计算。\",\"确定性：变分推断提供了一个确定性的逼近分布，可以直接用于预测或决策。\",\"缺点：\",\"逼近误差：变分推断需要选择一个逼近分布，这可能会导致一些逼近误差。特别是，变分推断往往会低估后验分布的方差。\",\"选择逼近分布：选择一个合适的逼近分布可能会很困难，特别是在处理复杂模型或分布时。\",\"优化难度：变分推断涉及到一个非凸优化问题，可能会陷入局部最优。\",\"总的来说，选择哪种方法取决于具体的任务和需求。如果需要更精确的结果，或者处理的是复杂的模型或分布，可能会选择MCMC。如果需要处理大数据集，或者需要更快的计算速度，可能会选择变分推断。\",\"在这里，我将使用LaTeX数学公式来解释MCMC和变分推断的一些关键概念和公式：\",\"MCMC（Markov Chain Monte Carlo）：在MCMC中，我们构建一个马尔可夫链，使其的平稳分布就是我们要求的后验分布p(θ∣x)，其中θ是参数，x是观测到的数据。然后，我们通过对马尔可夫链进行随机游走来生成后验样本。例如，在Metropolis-Hastings算法中，我们在每一步中都会根据一个提议分布q(θ′∣θ)来提出一个新的状态θ′，然后以接受率α(θ,θ′)=min(1,p(θ∣x)q(θ′∣θ)p(θ′∣x)q(θ∣θ′)​)来决定是否接受这个提议。\",\"变分推断（Variational Inference）：在变分推断中，我们选择一个参数化的逼近分布q(θ∣ϕ)，其中ϕ是变分参数，然后通过优化KL散度KL(q(θ∣ϕ)∣∣p(θ∣x))来找到最佳的ϕ。根据贝叶斯规则，我们有p(θ∣x)∝p(x∣θ)p(θ)，所以这个优化问题可以等价为最大化证据下界（ELBO）：L(ϕ)=Eq(θ∣ϕ)​[logp(x∣θ)+logp(θ)−logq(θ∣ϕ)]。\",\"每种方法都有其优点和缺点。MCMC可以提供精确的后验样本，但计算成本高且收敛慢。变分推断则计算效率高且易于扩展，但可能会引入一些逼近误差，并需要选择一个合适的逼近分布。具体选择哪种方法取决于任务的需求和可用的计算资源。\"]},\"14\":{\"h\":\"Laplace Approximation\",\"t\":[\"#Bayes\",\"拉普拉斯近似是一种简单且计算效率高的方法，它假设后验分布在最大后验概率（MAP）点附近是高斯的。步骤如下：\",\"a. 找到最大后验概率（MAP）点：这通常通过梯度上升或牛顿法等优化方法来实现。\",\"b. 在MAP点附近做二阶泰勒展开：通过计算对数后验概率的Hessian矩阵（二阶导数矩阵）来近似后验分布的形状。\",\"c. 得到高斯近似：Hessian矩阵的逆就是高斯近似的协方差矩阵，而MAP点就是高斯近似的均值。\",\"对于神经网络权重更新使用Laplace Approximation的具体实现，实际上有很多相关的研究论文。例如，MacKay在1992年的论文\\\"Bayesian Interpolation\\\"中就介绍了一种基于Laplace Approximation的方法用于神经网络的训练。\",\"然而，需要注意的是，虽然Laplace Approximation的计算效率高，但其仅适用于后验分布在最大后验（Maximum A Posteriori，MAP）点附近可以被高斯分布很好地近似的情况。对于复杂的、多峰的后验分布，Laplace Approximation可能无法提供一个好的近似。因此，在实际应用中，仍然需要根据具体的问题和数据来选择最合适的方法。\",\"Q: Laplace Approximation可以用于long-tail 分布么\",\"A: Laplace Approximation是一种在统计学中常用的近似方法，通常用于估计复杂模型的后验分布。这种方法基于二次泰勒展开，主要适用于对于分布形态相对简单，比如正态分布或者其他对称分布的情况。\",\"对于长尾分布（Long-tail Distribution），这种分布的特点是尾部的概率密度下降的速度相对较慢，因此在尾部有相对较高的概率密度。由于Laplace Approximation的基本思想是在分布的最大值（或者期望）处进行二阶泰勒展开，因此对于长尾分布，这种方法可能无法很好地捕捉到尾部的信息。\",\"这是因为在长尾分布中，尾部的信息对于整个分布的影响是非常重要的。然而，Laplace Approximation主要关注的是分布的峰值附近的信息，可能会忽略尾部的信息，因此对于长尾分布，Laplace Approximation可能无法提供一个准确的近似。\",\"总的来说，虽然Laplace Approximation是一种实用的近似方法，但是对于长尾分布，可能需要采取其他的近似方法，比如Monte Carlo方法等，来得到更准确的结果。\"]},\"15\":{\"h\":\"Markov Chain Monte Carlo(MCMC)\",\"t\":[\"#Bayes\",\"马尔科夫链蒙特卡洛（MCMC）是一种统计方法，主要用于从复杂的高维度分布中抽样。在深度神经网络中，MCMC可以用来估计网络权重的后验分布并进行更新。以下是使用MCMC进行深度神经网络权重更新的详细步骤：\",\"初始化：首先，需要初始化神经网络的权重。这通常可以通过随机初始化或使用预训练模型来进行。\",\"设定目标分布：在MCMC中，我们的目标是从后验分布中抽样，这里的后验分布是给定数据后，权重的概率分布。这通常是通过贝叶斯定理来计算的，其中先验分布可以是权重的高斯分布，似然函数可以通过神经网络的损失函数来定义。\",\"抽样：在MCMC中，马尔科夫链的定义是下一个状态（这里的状态是权重的值）只依赖于当前状态，不依赖于过去的状态。常见的MCMC抽样方法包括Metropolis-Hastings算法和Hamiltonian Monte Carlo等。\",\"例如，在Metropolis-Hastings算法中，首先从一个建议分布中抽样一个新的权重值，然后根据接受率来决定是否接受这个新的权重值。接受率是新的权重值和当前权重值在目标分布下的概率比，如果这个比值大于1，那么就接受新的权重值，否则以这个比值作为接受新的权重值的概率。\",\"更新权重：如果接受了新的权重值，那么就用这个新的权重值更新神经网络的权重。然后重复步骤3，直到马尔科夫链达到平稳分布，或者达到预设的迭代次数。\",\"后处理：由于MCMC的前几次迭代可能还没有达到平稳分布，因此通常需要抛弃前几次迭代的结果，这个过程叫做burn-in。然后可以通过对剩下的抽样结果求平均来得到权重的估计值。\",\"请注意，MCMC是一种处理不确定性的方法，因此在使用MCMC时，神经网络的权重不再是固定的值，而是一个分布。这意味着网络的输出也将是不确定的，这可以用来衡量模型的不确定性。\",\"初始化权重：我们假设神经网络的权重矩阵为W，可以使用随机初始化或预训练模型将初始权重设置为W0​。\",\"设定目标分布：我们的目标是从权重W的后验分布p(W∣D)中进行抽样，其中D是训练数据。根据贝叶斯定理，我们有\",\"p(W∣D)=p(D)p(D∣W)⋅p(W)​\",\"其中p(D∣W)是似然函数，可以通过神经网络的损失函数来定义。p(W)是权重的先验分布，可以假设为高斯分布。p(D)是数据的边缘概率，通常用于归一化，但在MCMC中我们不需要计算它。\",\"抽样：我们使用MCMC抽样方法来抽样新的权重值。假设当前权重值为Wt​，我们从一个建议分布q(W′∣Wt​)中抽样一个新的权重值W′。然后计算接受率α，如果使用Metropolis-Hastings算法，我们有\",\"α=min(1,p(Wt​∣D)⋅q(W′∣Wt​)p(W′∣D)⋅q(Wt​∣W′)​)\",\"其中p(W′∣D)和p(Wt​∣D)是新的权重值和当前权重值在目标分布下的概率，q(Wt​∣W′)和q(W′∣Wt​)是新的权重值和当前权重值在建议分布下的概率。\",\"更新权重：我们生成一个[0,1]之间的随机数u，如果u<α，那么我们接受新的权重值，即Wt+1​=W′；否则，我们保持当前的权重值，即Wt+1​=Wt​。然后重复步骤3，直到马尔科夫链达到平稳分布，或者达到预设的迭代次数。\",\"后处理：我们丢弃前N个抽样结果，然后对剩下的抽样结果求平均来得到权重的估计值，即\",\"W^=T−N1​t=N+1∑T​Wt​\",\"其中T是总的迭代次数，N是burn-in期的迭代次数，Wt​是第t次迭代的权重值。\"]},\"16\":{\"h\":\"Prior Uncertainty - A Key Hurdle in Advancing Bayesian Machine Learning\"},\"17\":{\"h\":\"Prior Uncertainty: A Key Hurdle in Advancing Bayesian Machine Learning\",\"t\":[\"Bayesian inference in machine learning is currently significantly restrained by the challenge of undetermined prior settings. While not the sole obstacle, the selection of appropriate prior distributions is a critical and often unresolved issue that fundamentally impacts the performance and reliability of Bayesian models, particularly in the complex domain of deep learning.\",\"The issue of prior selection is a foundational challenge in Bayesian methodology. Priors represent the initial beliefs about the model parameters before observing data, and a poorly chosen prior can lead to inaccurate conclusions. In the context of Bayesian deep learning, this problem is magnified due to the high dimensionality of the parameter space in neural networks. The common practice of using so-called \\\"uninformative\\\" or vague priors, such as standard Gaussian distributions, is often a pragmatic choice due to the difficulty of specifying meaningful priors for millions of parameters. However, research has shown that such seemingly innocuous choices can introduce unintended biases and negatively affect the model's performance.\",\"A misspecified prior can undermine the very advantages that make Bayesian methods attractive, such as reliable uncertainty quantification and model selection. Studies have demonstrated that the choice of prior has a substantial impact on a model's true and false positive rates. Furthermore, there is no universally optimal prior; the best choice is often dependent on the specific dataset being analyzed.\",\"This challenge is not merely a theoretical concern. The difficulty in defining appropriate priors has practical consequences, hindering the widespread adoption of Bayesian methods in complex, high-stakes applications. The process of mapping subjective prior beliefs into a tractable probability distribution is non-trivial and can be a significant hurdle for practitioners.\",\"While the challenge of prior selection is a major restraint, it is important to acknowledge that it is one of two primary obstacles, the other being computational complexity. Bayesian methods, especially for deep learning models, are notoriously computationally expensive. Techniques like Markov Chain Monte Carlo (MCMC) for sampling from the posterior distribution can be prohibitively slow for large models. While more scalable methods like variational inference have been developed, they introduce approximations that can affect the quality of the results.\",\"These two challenges—prior specification and computational cost—are often intertwined. More sophisticated and potentially more appropriate priors can further complicate the already difficult task of posterior inference, increasing the computational burden. Therefore, it is more accurate to view the advancement of Bayesian machine learning as being constrained by the dual challenges of both defining meaningful priors and performing inference in a computationally tractable manner.\",\"In conclusion, while computational cost remains a significant barrier, the problem of undetermined prior settings is a fundamental and pressing issue that restrains the full potential of Bayesian machine learning. The active area of research into developing more robust methods for prior specification, including learning priors from data and creating more flexible prior distributions, underscores the criticality of this challenge. Overcoming this hurdle is essential for the continued progress and broader application of Bayesian inference in the field of machine learning.\"]},\"18\":{\"h\":\"Stochastic Variational Inference\",\"t\":[\"#Bayes\",\"使用随机变分推断（Stochastic Variational Inference, SVI）来更新深度学习中神经网络权重的过程涉及到几个步骤。以下是详细的步骤，包括相关的数学公式：\",\"初始化：首先，我们需要初始化网络的权重。对于一个具有D个权重的网络，我们可以初始化权重向量w和变分分布的参数。这个变分分布通常是一个高斯分布q(w;μ,σ)，其均值向量μ和协方差矩阵σ也需要初始化。\",\"定义优化目标：在变分推断中，我们的目标是最大化证据下界（ELBO）。对于一个有N个样本的数据集D，ELBO的定义为：\",\"[L(μ,σ)=Eq(w;μ,σ)​[logp(D∣w)]−KL(q(w;μ,σ)∣∣p(w))]]\",\"其中，p(D∣w)是数据的似然函数，p(w)是权重的先验分布，KL(⋅∣∣⋅)是KL散度，用来衡量变分分布和先验分布的相似程度。\",\"采样和前向传播：在每个训练步骤中，我们从变分分布q(w;μ,σ)中采样权重w，并使用这些权重来执行前向传播。具体地，我们使用重参数化技巧，即引入一个无参数的噪声向量ϵ，使得w=μ+σ⊙ϵ，其中⊙表示元素级的乘法。\",\"计算梯度：然后我们计算损失函数（即负ELBO）关于变分分布参数的梯度。具体地，我们计算∂μ∂(−L)​和∂σ∂(−L)​。由于我们使用了重参数化技巧，所以可以直接通过反向传播来计算这些梯度。\",\"更新权重：最后，我们用计算出的梯度来更新变分分布的参数，通常使用随机梯度下降（SGD）或其变体。具体地，我们有：\",\"[μ←μ−α∂μ∂(−L)​][σ←σ−α∂σ∂(−L)​]\",\"其中，α是学习率。\",\"重复步骤：重复步骤3-5直到满足停止条件，例如达到预定的迭代次数，或者ELBO收敛到一个稳定值。\",\"以上就是在深度学习中使用SVI来更新神经网络权重的详细步骤和相关的数学公式。\",\"在随机变分推断中，变分分布和权重的先验分布都是关于神经网络权重的概率分布，但它们的含义和作用是不同的。\",\"权重的先验分布：这是我们对权重在看到任何数据之前的知识或假设。它通常是由问题的背景知识和经验选择的。例如，我们可能会假设权重来自一个以零为中心的正态分布，这反映了我们的假设，即权重应该接近零但可以在两个方向上变化。在贝叶斯推断中，先验分布是必需的，因为它与似然函数相结合，形成了后验分布。\",\"变分分布：这是我们在变分推断中引入的一个新的分布，用来近似权重的真实后验分布。它是一个参数化的分布，其参数（例如均值和方差）由神经网络（推断网络）输出，这些参数在训练过程中会被优化。最优的变分分布将在KL散度意义下尽可能接近真实的后验分布。\",\"在训练神经网络时，我们的目标是找到最优的变分分布，使得其最大化了ELBO（证据下界）。这意味着我们需要找到一组权重，使得它们能够在拟合数据（最大化数据的似然）和遵循先验（最小化与先验分布的KL散度）之间找到一个好的平衡。\",\"似然函数是一个在统计学和机器学习中非常重要的概念。它描述了在给定模型参数下，观测到当前数据的概率。\",\"假设我们有一个统计模型，模型参数为θ，观测数据为X。似然函数L(θ|X)是一个关于θ的函数，表示在给定参数θ的情况下，观测到数据X的概率。在许多情况下，我们可以将似然函数视为参数θ的函数，数据X是已知的。\",\"例如，假设我们有一个从正态分布抽取的数据集X，正态分布的参数是均值μ和标准差σ。在这种情况下，似然函数是：\",\"L(μ,σ∣X)=i=1∏n​σ2π​1​e−2σ2(xi​−μ)2​\",\"这个公式表示了在给定均值μ和标准差σ的情况下，观测到数据集X的概率。\",\"似然函数在参数估计中起着重要的作用。我们通常想要找到一组参数，使得似然函数最大，这就是最大似然估计（Maximum Likelihood Estimation, MLE）。在贝叶斯推断中，似然函数与先验分布相结合，形成了后验分布，这个后验分布描述了在观测到数据之后，参数的不确定性。\",\"需要注意的是，虽然似然函数和概率密度函数看起来非常相似，但它们的角色和含义是不同的。概率密度函数描述了在给定参数的情况下，某个具体的观测值发生的概率，是对观测值的函数；而似然函数则是描述在给定观测值的情况下，参数取某个值的可能性，是对参数的函数。\"]},\"19\":{\"h\":\"What is uncertainty in neural networks\",\"t\":[\"#Bayes\",\"在深度神经网络中，不确定性通常分为两种类型：模型不确定性（或称为认知不确定性）和数据不确定性（或称为偶发不确定性）。\",\"模型不确定性（Epistemic Uncertainty）: 这是由于模型参数的不确定性导致的。例如，当我们的模型在训练数据上过拟合时，模型对于新的、未见过的数据可能会有很大的不确定性。这种不确定性可以通过收集更多的数据来减小。\",\"一个常用的方法来量化模型不确定性是贝叶斯神经网络（Bayesian Neural Networks），它将每个权重视为随机变量，从而引入了权重的分布。对于一个具有权重w的神经网络，后验分布可以表示为P(w|D)，其中D是训练数据。这个后验分布可以通过贝叶斯定理计算：\",\"P(w|D) = P(D|w)P(w) / P(D)\",\"在这个公式中，P(w|D)是权重w的后验分布，P(D|w)是在给定权重w的情况下数据D的似然性，P(w)是权重w的先验分布，P(D)是数据的证据（也称为边缘似然性）。我们可以使用随机变分推断或MCMC等方法来近似这个后验分布。\",\"数据不确定性（Aleatoric Uncertainty）: 这是由于数据本身的噪声或测量误差导致的。这种不确定性是固有的，不能通过收集更多的数据来减小。\",\"数据不确定性可以通过模型的预测分布来表示。例如，对于回归问题，我们可以假设预测值y遵循一个以f(x)为均值、σ2为方差的高斯分布。这里的σ2可以看作是数据不确定性的度量，通过最小化对数似然性来估计：\",\"log P(D|w) = Σ_{i} log P(y_i|f(x_i), w)\",\"在这个公式中，P(y_i|f(x_i), w)是在给定输入x_i和权重w的情况下，输出y_i的概率分布。我们可以通过最小化这个对数似然性来估计模型的参数和数据的不确定性。\"]},\"20\":{\"h\":\"Why we are assuming all weight distributions are Gaussian\",\"t\":[\"#Bayes\",\"Q: 对于深度神经网络来说他们的权重符合正态分布么？\",\"A: 深度神经网络的权重是否符合正态分布这个问题并没有一个定论，它取决于多种因素，包括网络的初始化方式、网络的结构、训练数据的分布以及训练过程中的优化算法等。\",\"初始化方式：在神经网络的训练开始时，我们通常会对权重进行初始化。通常的初始化方法有很多种，其中一种常见的方法就是使用正态分布或者均匀分布来进行初始化。这样一来，至少在训练开始时，权重会近似地服从这种分布。\",\"网络结构和训练数据：随着训练的进行，权重的分布会受到训练数据和网络结构的影响。例如，如果网络的某一层主要学习的特征在数据中的分布是非对称的，那么这一层的权重的分布可能就会偏离正态分布。\",\"优化算法：训练过程中使用的优化算法也会影响权重的分布。例如，使用带有动量项的优化算法（如Adam）可能会使权重的分布在训练过程中变得更加复杂。\",\"因此，虽然我们可以通过初始化的方式使权重的分布在训练开始时近似正态分布，但在训练过程中，权重的分布可能会发生变化，甚至可能偏离正态分布。另外，我们也无法简单地假设所有的深度神经网络的权重都符合正态分布，因为这会忽略了网络结构、训练数据和优化算法等因素的影响。\",\"对于权重的分布，一种常见的研究方法是在训练过程中对权重进行可视化，观察其分布的变化。这可以帮助我们更好地理解网络的训练过程，以及权重的分布对网络性能的影响。\",\"Q: Are they?\",\"A: They are not, see \\\"Bayesian attention module\\\" 2020 ICLR\"]},\"21\":{\"h\":\"reparameter trick\",\"t\":[\"在贝叶斯变分推断（Bayesian Variational Inference）中，重参数技巧（Reparameterization Trick）是一项核心技术，它巧妙地解决了在随机计算图中进行梯度反向传播的难题。简单来说，该技巧通过将随机变量的生成过程重构为一个确定性函数和一个独立的噪声源，从而使得整个模型可以进行端到端的梯度优化。\"]},\"22\":{\"h\":\"核心要点：重参数技巧是什么？\",\"t\":[\"在变分推断中，我们的目标是找到一个简单的参数化分布族 qϕ​(z∣x) 来逼近真实的后验分布 p(z∣x)，其中 z 是隐变量，x 是观测数据，ϕ 是变分参数。为此，我们需要最大化证据下界（Evidence Lower Bound, ELBO）。这个优化过程通常涉及到对一个期望的梯度计算，例如 ∇ϕ​Eqϕ​(z∣x)​[f(z)]。\",\"问题的关键在于，期望的计算涉及到从 qϕ​(z∣x) 中采样，而采样操作本身是随机的、不可导的，这导致梯度无法直接通过采样步骤进行反向传播。\",\"重参数技巧的核心思想是： 将从 qϕ​(z∣x) 中采样 z 的过程，转化为从一个固定的、与参数 ϕ 无关的简单分布 p(ϵ) 中采样一个噪声变量 ϵ，然后通过一个确定性的、可导的函数 gϕ​(x,ϵ) 来生成 z。\",\"也就是说，我们将 z∼qϕ​(z∣x) 重写为：z=gϕ​(x,ϵ), 其中 ϵ∼p(ϵ)\",\"一个最经典的例子是高斯分布。假设我们的变分后验 qϕ​(z∣x) 是一个均值为 μϕ​(x)、标准差为 σϕ​(x) 的高斯分布 N(μϕ​(x),σϕ​(x)2)。直接从这个分布采样是不可导的。但我们可以利用重参数技巧：\",\"从标准正态分布 N(0,1) 中采样一个噪声 ϵ。\",\"通过以下确定性变换来得到 z：z=μϕ​(x)+σϕ​(x)⋅ϵ\",\"通过这种方式，随机性完全来自于与参数 ϕ 无关的 ϵ，而 z 变成了关于 ϕ（通过 μ 和 σ）和 ϵ 的确定性函数。\"]},\"23\":{\"h\":\"详细解释：为什么重参数技巧能起作用？\",\"t\":[\"重参数技巧之所以有效，主要基于以下两个关键原因：\"]},\"24\":{\"h\":\"1. 分离随机性与参数，打通梯度路径\",\"t\":[\"在未使用重参数技巧时，计算图的结构是：参数 ϕ 定义了分布 qϕ​，然后从 qϕ​ 中进行随机采样得到 z，最后 z 被用于计算损失函数。在这个流程中，“随机采样”是一个黑箱操作，它阻断了损失函数到参数 ϕ 的梯度流。 这就像试图通过询问一个随机数生成器的输出来调整它的内部参数一样，梯度信息丢失了。\",\"重参数技巧通过重构计算图解决了这个问题：\",\"改造前：ϕ→qϕ​(z∣x)采样​z→Loss (梯度在采样处中断)\",\"改造后：\",\"ϕ→μϕ​,σϕ​ (确定性计算)\",\"ϵ∼p(ϵ) (固定的外部噪声源)\",\"(μϕ​,σϕ​,ϵ)z=μϕ​+σϕ​⋅ϵ​z (确定性计算)\",\"z→Loss\",\"在这个新的计算图中，从参数 ϕ 到最终的损失函数，每一步都是确定性的、可微分的计算。 随机性被作为一个独立的输入变量 ϵ 注入到系统中。这样一来，我们就可以利用标准的自动微分工具（如TensorFlow或PyTorch中的反向传播算法）来计算损失函数关于参数 ϕ 的梯度。\",\"因此，原来关于期望的梯度 ∇ϕ​Eqϕ​(z∣x)​[f(z)] 就被转化为了对期望内部函数的梯度 Ep(ϵ)​[∇ϕ​f(gϕ​(x,ϵ))]。由于期望是线性的，梯度可以直接推入期望内部，从而可以通过蒙特卡洛采样来近似这个梯度。\"]},\"25\":{\"h\":\"2. 显著降低梯度估计的方差\",\"t\":[\"在变分推断中，除了重参数技巧，还有另一种处理随机节点梯度的技术，叫做得分函数估计器（Score Function Estimator），也常被称为REINFORCE。它不需要函数 f(z) 可导，但其梯度估计的方差通常非常高。\",\"高方差的梯度估计会导致模型训练过程非常不稳定，收敛速度慢，甚至可能无法收敛。 这就像在一个非常嘈杂的环境中试图找到正确的方向，每一步的指引都充满了随机干扰。\",\"重参数技巧得到的梯度估计器通常具有低得多的方差。 直观地理解，这是因为：\",\"重参数技巧直接利用了函数 f(z) 内部关于参数 ϕ 的梯度信息。只要 f(z) 本身是相对平滑的，那么无论采样的 ϵ 如何变化，梯度 ∇ϕ​f(gϕ​(x,ϵ)) 的波动会相对较小。\",\"得分函数估计器的梯度形式为 Eqϕ​​[f(z)∇ϕ​logqϕ​(z∣x)]。它将函数值 f(z) 作为梯度的权重。如果 f(z) 的值本身波动很大，就会导致梯度估计的方差随之增大。\",\"低方差的梯度使得随机梯度下降（SGD）等优化算法能够更稳定、更高效地进行，从而能够训练更复杂、更深层的生成模型。\"]},\"26\":{\"h\":\"总结\",\"t\":[\"总而言之，用于贝叶斯变分推断的重参数技巧是一种优雅而强大的技术，它解决了在随机模型中进行梯度优化的核心障碍。\",\"它是什么？ 一种将随机变量的生成过程分解为“确定性变换 + 独立噪声”的方法，从而将对参数的依赖从随机采样操作中解耦出来。\",\"为什么能起作用？\",\"打通梯度流： 它通过将随机采样操作移出梯度计算路径，构建了一个从模型参数到最终损失的全微分计算图，使得标准的反向传播算法得以应用。\",\"降低方差： 相比于得分函数估计器等替代方法，它能提供方差更低的梯度估计，这使得模型的训练过程更稳定、更高效，是成功训练变分自编码器（VAE）等模型的关键。\",\"可以说，没有重参数技巧，许多现代深度生成模型和概率编程框架的成功将是难以想象的。\"]},\"27\":{\"h\":\"ViT\"},\"28\":{\"h\":\"Vision Transformer (ViT)：从文本到图像的革命性跨越\",\"t\":[\"Vision Transformer (ViT) 的出现，标志着深度学习领域的一个重要转折点。它成功地将原本在自然语言处理（NLP）领域大放异彩的 Transformer 模型，应用于计算机视觉领域，并取得了与顶级卷积神经网络（CNN）相媲美的性能。要理解 ViT 的精髓，我们需要先深入了解它在传统 Encoder-Decoder Transformer 架构上所做的关键改变，并剖析其处理图像数据的每一步。\"]},\"29\":{\"h\":\"ViT 对传统 Encoder-Decoder Transformer 的三大核心改变\",\"t\":[\"传统的 Transformer 模型主要用于处理序列到序列的任务，如机器翻译。它包含一个编码器（Encoder）来理解输入序列（源语言句子），以及一个解码器（Decoder）来生成输出序列（目标语言句子）。然而，ViT 在设计上进行了大胆的简化和调整，使其更适应图像分类等视觉任务：\",\"抛弃解码器（Decoder）：ViT 的核心任务是图像分类，这是一个“多对一”的问题，即输入一张图像（多个像素），输出一个类别标签（一个结果）。因此，它不需要像机器翻译那样生成一个序列。ViT 只使用了 Transformer 的 编码器（Encoder）部分。编码器的作用是深入理解输入图像各个部分之间的关系，并最终输出一个包含整张图像丰富信息的综合表示，用于后续的分类。\",\"全新的输入处理方式：将图像转化为序列：这是 ViT 最具开创性的改变。Transformer 的原生设计是处理一维的序列数据（如单词序列），而图像是二维的像素网格。为了跨越这个“维度鸿沟”，ViT 引入了 “图像块化”（Image Patching） 的策略。它不直接处理单个像素，而是将图像分割成一系列固定大小的、不重叠的方块（Patches），然后将这些二维的方块展平（flatten）成一维的向量。这样一来，一张图像就巧妙地转化为了一个“图像块”序列，完美地适配了 Transformer 的输入要求。\",\"引入 [class] 令牌（Token）：在 NLP 的 Transformer 模型（如 BERT）中，常常会有一个特殊的 [CLS] 令牌被添加到序列的开头。这个令牌经过 Transformer 编码器后，其对应的输出向量被视作整个序列的聚合表示，用于下游的分类任务。ViT 借鉴了这一思想，同样在图像块序列的最前面添加了一个可学习的 [class] 嵌入向量。这个 [class] 令牌不代表任何具体的图像块，但它会与其他图像块的表示进行交互，通过自注意力机制（Self-Attention）聚合整个图像的全局信息。在经过所有 Transformer Encoder 层之后，最终只需要取出这个 [class] 令牌对应的输出向量，送入一个简单的分类头（通常是一个全连接层），即可完成图像分类。\"]},\"30\":{\"h\":\"ViT 算法的详细步骤解析\",\"t\":[\"下面我们以图像分类任务为例，一步步拆解 ViT 的工作流程：\",\"第1步：图像分块与展平 (Patching and Flattening)\",\"输入：一张二维图像，例如尺寸为 H × W × C（高 × 宽 × 通道数）。\",\"操作：首先，将这张图像分割成 N 个大小均为 P × P 的小方块（Patches）。例如，一张 224x224 的图像，如果 Patch 大小为 16x16，那么将会得到 (224/16) * (224/16) = 14 * 14 = 196 个图像块。\",\"展平：将每个二维的图像块（P × P × C）展平为一个一维的向量。在上面的例子中，每个 16x16x3 的图像块会被展平成一个长度为 16 * 16 * 3 = 768 的向量。\",\"输出：一个由 N 个一维向量组成的序列，即一个形状为 N × (P² * C) 的矩阵。\",\"第2步：线性投射与嵌入 (Linear Projection and Embedding)\",\"操作：Transformer 内部处理的向量维度是固定的，我们称之为 D（模型维度）。上一步得到的展平向量维度（P² * C）不一定等于 D。因此，需要通过一个可训练的线性投射层（本质上是一个全连接层）将每个展平后的图像块向量从 P² * C 维映射到 D 维。这些经过投射后的向量被称为“块嵌入”（Patch Embeddings）。\",\"第3步：添加 [class] 令牌与位置编码 (Prepending [class] Token & Adding Positional Encoding)\",\"添加 [class] 令牌：在 N 个块嵌入序列的最前面，拼接上一个可学习的 [class] 嵌入向量（维度也是 D）。现在，序列的长度变成了 N+1。\",\"添加位置编码：Transformer 的自注意力机制本身是无法感知序列顺序的。为了让模型知道每个图像块的原始空间位置（例如，哪个块在左上角，哪个在中间），必须为序列中的每一个嵌入向量（包括 [class] 令牌）添加一个“位置编码”（Positional Encoding）。这个位置编码通常也是一个可学习的 D 维向量，它携带着位置信息，与块嵌入相加后，共同作为 Transformer Encoder 的输入。\",\"第4步：输入 Transformer 编码器 (Passing to Transformer Encoder)\",\"核心结构：ViT 的 Transformer Encoder 由多个相同的层堆叠而成。每一层主要包含两个子模块： \",\"多头自注意力 (Multi-Head Self-Attention)：这是 Transformer 的灵魂。它允许序列中的每个元素（每个图像块的表示）关注并加权融合序列中所有其他元素的信息。这使得模型能够捕捉图像块之间的长距离依赖关系，例如，识别出图像中一只猫的耳朵和尾巴虽然相距很远，但它们都属于“猫”这个整体。\",\"前馈网络 (Feed-Forward Network)：在自注意力层之后，每个位置的输出都会经过一个简单的全连接前馈网络，进行非线性变换，增加模型的表达能力。\",\"残差连接与层归一化：在每个子模块（自注意力和前馈网络）的周围，都使用了残差连接（Residual Connection）和层归一化（Layer Normalization），这对于训练深度 Transformer 模型至关重要，可以有效防止梯度消失和爆炸问题。\",\"操作：包含 [class] 令牌和位置信息的嵌入序列会依次通过所有 Encoder 层。在每一层中，信息都会在所有图像块之间进行流动和整合。\",\"第5步：分类 (Classification)\",\"提取聚合信息：当序列通过所有 Encoder 层后，我们只关心序列第一个位置，也就是 [class] 令牌对应的输出向量。此时，这个向量已经通过自注意力机制充分聚合了整张图像的全局信息。\",\"送入分类头：将这个 D 维的向量送入一个分类头（通常是一个简单的多层感知机，MLP Head），最终输出对各个类别的预测概率。\"]},\"31\":{\"h\":\"为什么 ViT 能够有效处理图像数据？\",\"t\":[\"ViT 的成功并非偶然，其核心在于它巧妙地解决了 Transformer 处理图像的几个关键挑战：\",\"全局感受野：传统的 CNN 通过堆叠卷积层来逐步扩大感受野，需要很多层才能捕捉到全局信息。而 ViT 中的自注意力机制在第一层就能够建立图像中任意两个块之间的关系，从而拥有 天然的全局感受野。这对于理解需要长距离上下文信息的复杂场景至关重要。\",\"灵活性与可扩展性：与卷积核大小固定的 CNN 不同，ViT 的结构更具灵活性。通过调整模型深度、宽度以及注意力头的数量，可以轻松地扩展模型规模。研究表明，当拥有足够大的数据集（如 JFT-300M）进行预训练时，更大规模的 ViT 模型性能会持续提升，展现出比 CNN 更强的可扩展性。\",\"参数效率：在处理高分辨率图像时，ViT 将图像分割成块，使得输入序列的长度与图像分辨率的平方成正比，而不是像素总数。这种设计避免了在每个像素上计算注意力，从而在计算上是可行的。\",\"总而言之，ViT 并非对传统 Transformer 的简单照搬，而是通过 将图像块化为序列、引入 [class] 令牌进行信息聚合 以及 仅使用编码器 等一系列精巧的改造，成功地将 Transformer 强大的序列建模能力释放到了计算机视觉领域，开启了一个全新的研究范式。\"]},\"32\":{\"h\":\"How to build EBMs\",\"t\":[\"Of course, let's break down Energy-Based Models (EBMs) step by step.\"]},\"33\":{\"h\":\"Step 1: Understanding the Core Concept of Energy-Based Models\",\"t\":[\"At its heart, an Energy-Based Model is a type of machine learning model that learns to associate a scalar \\\"energy\\\" value with every possible configuration of its input data. The central idea is inspired by physics:\",\"Low Energy = High Probability: Data points that are similar to the data the model was trained on (i.e., they are \\\"correct\\\" or \\\"plausible\\\") are assigned a low energy score. In the analogy of a physical system, this is a stable, likely state.\",\"High Energy = Low Probability: Data points that are unlike the training data (i.e., they are \\\"incorrect\\\" or \\\"implausible\\\") are assigned a high energy score. This represents an unstable, unlikely state.\",\"Imagine you are training a model on images of cats. The EBM would learn an \\\"energy function\\\" that works like this:\",\"Input: An image of a cat -> Output: Low energy\",\"Input: An image of a dog -> Output: High energy\",\"Input: An image of random static -> Output: Very high energy\",\"The model isn't directly outputting a probability, but rather this energy score, which can then be converted into a probability. The key takeaway is that the model's entire job is to learn to distinguish between \\\"good\\\" and \\\"bad\\\" examples by assigning them different energy levels.\"]},\"34\":{\"h\":\"Step 2: The \\\"Theorem\\\" - The Gibbs-Boltzmann Distribution\",\"t\":[\"The \\\"theorem\\\" or mathematical principle that formalizes this relationship between energy and probability is the Gibbs-Boltzmann distribution, which comes from statistical mechanics. It states that the probability of a given data point x, denoted as p(x), is related to its energy E(x) by the following formula:\",\"p(x) = (e ^ -E(x)) / Z\",\"Let's break down this formula:\",\"E(x): This is the energy function. It's the core of your model, which takes a data point x and outputs its energy. As E(x) goes up, e ^ -E(x) goes down, meaning the probability p(x) also goes down.\",\"e ^ -E(x): This is the exponential of the negative energy. This mathematical operation ensures that the resulting value is always positive.\",\"Z: This is the partition function. It is the sum of e ^ -E(x) over all possible data configurations. Its purpose is to normalize the values so that the total probability of all possible inputs adds up to 1.\",\"The big challenge with EBMs is that calculating Z is often computationally impossible, especially for high-dimensional data like images, because you would have to evaluate the energy of every possible image. This is a crucial point that dictates how we have to train these models.\"]},\"35\":{\"h\":\"Step 3: How to Build a Model Based on This Principle\",\"t\":[\"Building an EBM involves two main components: designing the energy function and then training it.\"]},\"36\":{\"h\":\"Component 1: Designing the Energy Function\",\"t\":[\"The beauty of EBMs lies in their flexibility. The energy function E(x) can be almost any function that takes your data x as input and outputs a single scalar value (the energy). In modern deep learning, this function is typically a neural network.\",\"For Images: You might use a Convolutional Neural Network (CNN). The input is an image, it passes through several convolutional layers, and the final output is a single number representing the energy.\",\"For Tabular Data: A simple Multi-Layer Perceptron (MLP) could serve as the energy function.\",\"The key is that you are not constrained by having to output a specific probability distribution (like in other models). The network's architecture can be chosen freely to best capture the features of your data.\"]},\"37\":{\"h\":\"Component 2: Training the Model\",\"t\":[\"Since we can't calculate the partition function Z, we can't directly use maximum likelihood estimation to train the model. Instead, we train it using a contrastive approach. The goal is to shape the \\\"energy landscape\\\":\",\"Push Down the energy of real data points (we want them to be low).\",\"Push Up the energy of \\\"fake\\\" or \\\"negative\\\" data points (we want them to be high).\",\"A very common and effective training algorithm for this is called Contrastive Divergence. Here’s a step-by-step intuition of how it works during one training step:\",\"Pick a \\\"Positive\\\" Sample: Take a real data point x from your training dataset (e.g., a real image of a cat).\",\"Calculate its Energy: Feed this real data point x through your neural network to get its energy, E(x).\",\"Adjust the Weights: Using gradient descent, update the weights of your network to make E(x) lower. This is the \\\"push down\\\" part.\",\"Generate a \\\"Negative\\\" Sample: This is the tricky part. You need to create a \\\"fake\\\" sample that the model currently thinks is good (has low energy). A common way to do this is using a sampling technique like Langevin Dynamics or Markov Chain Monte Carlo (MCMC). You start with a random data point and iteratively adjust it based on the model's own energy function until it settles into a low-energy state. Let's call this generated sample x'.\",\"Calculate its Energy: Feed this generated negative sample x' through your network to get its energy, E(x').\",\"Adjust the Weights Again: Using gradient descent, update the weights of your network to make E(x') higher. This is the \\\"push up\\\" part.\",\"By repeatedly performing these two steps—pulling the energy of real data down and pushing the energy of generated data up—the model learns to create an energy landscape where the valleys correspond to the real data distribution. This process effectively trains the model without ever needing to compute the intractable partition function Z.\"]},\"38\":{\"h\":\"How does diffusion model work\",\"t\":[\"At its heart, a diffusion model is a generative model, which means its primary purpose is to create new data that is similar to the data it was trained on. In the context of images, this means generating new, realistic-looking pictures. The core idea behind diffusion models is both elegant and surprisingly intuitive: to learn how to create data, you should first learn how to destroy it.\",\"The entire process can be broken down into two main stages: the forward process and the reverse process.\"]},\"39\":{\"h\":\"The Forward Process: Systematically Destroying an Image\",\"t\":[\"The forward process, also known as the diffusion process, is the \\\"destruction\\\" phase. It's a fixed procedure, meaning it doesn't involve any learning. Here's what happens step-by-step:\",\"Start with a real image from your training dataset (e.g., a picture of a cat).\",\"Gradually add a small amount of Gaussian noise to the image. Gaussian noise is a type of random statistical noise that follows a bell curve distribution, often resembling static on an old television.\",\"Repeat this process over and over again, adding a little more noise at each step. This is done for a predefined number of timesteps, often denoted as T (which could be, for example, 1000).\",\"The key here is that the noise is added in a controlled manner. We know exactly how much noise is being added at each timestep. As you progress through these timesteps, the original image becomes increasingly corrupted. By the final timestep T, the image is indistinguishable from pure, random noise.\",\"Think of it like adding a drop of black ink to a glass of clear water every second. Initially, you can still see through the water, but after many drops, the water becomes completely opaque. The forward process is this journey from a clear image to pure noise.\"]},\"40\":{\"h\":\"The Reverse Process: Learning to Recreate the Image\",\"t\":[\"This is where the magic and the machine learning happen. The goal of the reverse process is to undo the forward process. It starts with random noise and gradually removes it to generate a clean, coherent image.\",\"To do this, we need a powerful neural network. In most modern diffusion models, this is a U-Net architecture. Here's how it's trained and what it does:\",\"Input: The U-Net is given a noisy image from a specific point in the forward process (say, the image at timestep t).\",\"The Goal: Its task is not to directly guess the final clean image. Instead, it is trained to predict the noise that was added to the image at that particular timestep t.\",\"Training: We can train this network because we know the original image and we know exactly how much noise was added at each step in the forward process. So, we can give the network the noisy image and ask it to predict the noise. We then compare its prediction to the actual noise that was added and adjust the network's parameters to make its prediction more accurate. This is repeated for countless images at all possible timesteps.\"]},\"41\":{\"h\":\"Generating a New Image\",\"t\":[\"Once the U-Net is well-trained, we can use it to generate new images from scratch:\",\"Start with Pure Noise: We generate a random image that is pure Gaussian noise. This is our starting point at timestep T.\",\"Iterative Denoising:\",\"We feed this noisy image into our trained U-Net and also tell it that we are at timestep T.\",\"The U-Net analyzes the image and predicts the noise component within it.\",\"We then subtract a small amount of this predicted noise from the image. This gives us a slightly less noisy image, corresponding to the image at timestep T-1.\",\"Repeat: We take this new, slightly cleaner image, tell the U-Net we are now at timestep T-1, and repeat the process. The U-Net again predicts the noise, and we subtract it.\",\"Final Image: We continue this iterative denoising process all the way down to timestep t=0. By this point, if the model is well-trained, we will have a clean, realistic-looking image that is entirely new but shares the characteristics of the images the model was trained on.\",\"In essence, the diffusion model learns the path from noise back to a clear image by mastering the art of predicting and removing that noise, one small step at a time. This gradual, step-by-step refinement is what allows these models to generate such incredibly detailed and coherent images.\"]},\"42\":{\"h\":\"What does the U-net do in diffusion models?\",\"t\":[\"At its core, the U-Net in a diffusion model acts as the engine of the reverse diffusion process. In simple terms, diffusion models work by first progressively adding noise to an image until it becomes pure noise (the forward process) and then learning to reverse this process to generate a new image from random noise (the reverse process). The U-Net is the neural network trained to perform this reversal, step by step.\",\"The primary and most critical task of the U-Net is to predict the noise that was added to an image at a particular timestep. Given a noisy image at a specific step in the diffusion process, the U-Net analyzes it and outputs a prediction of the noise component. This predicted noise is then subtracted from the noisy image to produce a slightly less noisy version. This iterative denoising, guided by the U-Net, is what allows a diffusion model to generate a clean, coherent image from an initial field of random noise.\",\"Now, let's delve into the finer technical points of why the U-Net is so well-suited for this task and how it operates within the diffusion model framework.\"]},\"43\":{\"h\":\"Key Technical Aspects of the U-Net in Diffusion Models:\",\"t\":[\"Encoder-Decoder Structure: The U-Net is fundamentally an encoder-decoder network.\",\"The encoder part, also known as the contracting path, consists of a series of convolutional and downsampling layers (like max pooling). This path progressively reduces the spatial resolution of the input image while increasing the number of feature channels. The purpose is to capture the contextual, high-level features of the noisy image—what the image contains, but in a compressed representation.\",\"The decoder part, or the expansive path, takes this compressed feature representation and progressively upsamples it, using transposed convolutions, to return to the original image resolution. Its job is to reconstruct the spatial details.\",\"Skip Connections: The U-Net's \\\"Secret Sauce\\\": This is arguably the most critical feature of the U-Net architecture for diffusion models.\",\"Skip connections directly link feature maps from the encoder to their corresponding layers in the decoder.\",\"This is vital because the downsampling process in the encoder can lead to the loss of fine-grained spatial information. By feeding these high-resolution features from the encoder directly to the decoder, the network can combine the high-level context from the bottleneck with the precise, low-level details from earlier in the network.\",\"For the task of noise prediction, these skip connections are essential for preserving the intricate details of the image, ensuring that the final generated output is sharp and not just a blurry approximation.\",\"Input and Output Parity: A crucial requirement for the noise prediction network is that its output (the predicted noise) must have the same spatial dimensions as its input (the noisy image). The U-Net's symmetric encoder-decoder structure with skip connections is perfectly designed to handle this, making it a natural fit for the task.\",\"Timestep Conditioning: The amount of noise in an image is different at every step of the reverse diffusion process. The U-Net needs to know at which step it is operating to make an accurate prediction.\",\"This is achieved by providing the timestep t as an additional input to the U-Net.\",\"Typically, the integer timestep is converted into a high-dimensional vector representation called a time embedding.\",\"This time embedding is then usually added to the feature maps at various points within the U-Net's architecture, often in the residual blocks, allowing the network to modulate its behavior based on the current noise level.\",\"Integration of Advanced Mechanisms: Modern U-Nets used in state-of-the-art diffusion models like Stable Diffusion incorporate several enhancements:\",\"Residual Blocks: Instead of standard convolutional layers, many U-Nets use residual blocks. These blocks, which contain their own skip connections, help in training very deep networks by mitigating the vanishing gradient problem and improving the flow of information.\",\"Attention Mechanisms: Self-attention blocks are often integrated, particularly at lower resolutions in the U-Net. This allows the network to weigh the importance of different spatial locations in the image, enabling it to model long-range dependencies and understand the relationships between different parts of the image more effectively.\",\"Cross-Attention for Conditioning: In text-to-image models, the U-Net is further conditioned on the text prompt. This is often achieved by injecting the text embeddings into the U-Net using cross-attention layers. These layers allow the network to \\\"pay attention\\\" to relevant parts of the text prompt when generating different parts of the image.\",\"In summary, the U-Net is not just an arbitrary choice of architecture; its inherent design is exceptionally well-suited for the iterative denoising process at the heart of diffusion models. Its ability to capture both high-level context and preserve fine-grained detail, all while being conditioned on the noise level (timestep), makes it the powerful and effective backbone for generating the high-quality images we see from modern diffusion models.\"]},\"44\":{\"h\":\"KL Divergence in Variational Inference.\",\"t\":[\"非常好且深刻的问题！你触及了变分推断（Variational Inference, VI）的核心。\",\"是的，KL散度非常适合用来做变分推断，而这正是因为它天然地、数学必然地成为了证据下界（ELBO）的一部分。这两者不是简单的“适合”关系，而是从解决VI核心问题的数学推导中，KL散度自然而然地“浮现”了出来。\",\"下面我们来一步步拆解这其中的逻辑和原因。\"]},\"45\":{\"h\":\"1. 变分推断（VI）的核心问题是什么？\",\"t\":[\"变分推断要解决的是一个贝叶斯推理中的核心计算难题：计算后验分布 (Posterior Distribution)p(z∣x)。\",\"其中：\",\"z：模型中的隐变量（我们想推断的）。\",\"x：我们观测到的数据。\",\"根据贝叶斯定理：\",\"p(z∣x)=p(x)p(x∣z)p(z)​\",\"分母 p(x) 称为证据（Evidence） 或边缘似然（Marginal Likelihood）。计算它需要对所有可能的 z 进行积分（或求和）：\",\"p(x)=∫p(x∣z)p(z)dz\",\"这个积分在高维空间 often 是难以计算（intractable） 的。这就是VI要解决的问题。\",\"VI的思路是：既然无法直接计算真正的后验 p(z∣x)，我们就从一个简单的分布族 Q（例如高斯分布族）中，找到一个与 p(z∣x)最接近的分布 q∗(z) 来近似它。\",\"这就引出了两个问题：\",\"如何衡量“接近”？\",\"如何找到这个 q∗(z)？\"]},\"46\":{\"h\":\"2. KL散度如何自然地出现？\",\"t\":[\"我们需要一个衡量分布 q(z) 和 p(z∣x) 之间差异的度量。最直接的选择之一就是KL散度 DKL​。\",\"我们的目标是最小化这个差异：\",\"q∗(z)=argq∈Qmin​DKL​(q(z)∥p(z∣x))\",\"现在，我们展开这个KL散度：\",\"DKL​(q(z)∥p(z∣x))​=Ez∼q​[logp(z∣x)q(z)​]=Ez∼q​[logq(z)]−Ez∼q​[logp(z∣x)]=Ez∼q​[logq(z)]−Ez∼q​[logp(x,z)]+Ez∼q​[logp(x)](因为p(z∣x)=p(x,z)/p(x))=Ez∼q​[logq(z)]−Ez∼q​[logp(x,z)]+logp(x)​\",\"我们注意到最后一项 logp(x) 是一个与 q 无关的常数。我们将前两项重新排列，并取负号：\",\"DKL​(q(z)∥p(z∣x))=−(Ez∼q​[logp(x,z)]−Ez∼q​[logq(z)])+logp(x)\",\"令：\",\"ELBO(q)=Ez∼q​[logp(x,z)]−Ez∼q​[logq(z)]\",\"上面的等式就变成了：\",\"DKL​(q(z)∥p(z∣x))=−ELBO(q)+logp(x)\",\"移项后得到：\",\"logp(x)=ELBO(q)+DKL​(q(z)∥p(z∣x))\",\"这个等式是理解VI的关键！\",\"logp(x) 是固定的常数（证据）。\",\"DKL​(q(z)∥p(z∣x)) 衡量了我们近似分布与真实后验的差距，总是 ≥0。\",\"因此，ELBO是证据 logp(x) 的一个下界（Lower Bound）。当我们的近似分布 q(z) 越接近真实后验 p(z∣x)，KL散度就越小，ELBO就越大，从而更紧地逼近真实的 logp(x)。\",\"所以，我们的最小化KL散度的原始目标：\",\"qmin​DKL​(q(z)∥p(z∣x))\",\"数学上完全等价于最大化ELBO：\",\"qmax​ELBO(q)\",\"因为 logp(x) 是常数，最小化KL散度就是最大化ELBO。\"]},\"47\":{\"h\":\"3. 为什么是 ，而不是 ？\",\"t\":[\"这是另一个关键点，也是KL散度非对称性在VI中体现出的巨大优势。VI选择 DKL​(q∥p)（称为反向KL）是经过深思熟虑的：\",\"特性\",\"DKL​(q∥p) (Reverse KL, VI所用)\",\"DKL​(p∥q) (Forward KL)\",\"行为模式\",\"模式寻求（Mode-Seeking）\",\"均值寻求（Mean-Seeking）\",\"效果\",\"会让 q(z) 抓住 p(z∣x) 中的一个主要模式（峰值），而忽略其他模式。\",\"会让 q(z) 覆盖 p(z∣x) 的所有模式，但可能导致在任何一个模式上都拟合不佳。\",\"对 q 的要求\",\"避免在 p 概率低的地方放置 q 的概率质量。即，q(z) 只有在 p(z∣x) 也有概率时才有概率。这通常会产生一个更紧的、更安全的近似。\",\"避免在 p 概率高的地方 q 没有概率质量。即，要求 q(z) 必须覆盖所有 p(z∣x) 有概率的区域。\",\"计算可行性\",\"在优化ELBO时，只需要对 q 求期望 (Ez∼q​[⋅])，这可以通过重参数化技巧（Reparameterization Trick） 和蒙特卡洛采样进行高效、可微的估计。这是VI和VAE能够成功的关键。\",\"需要对真实后验 p 求期望 (Ez∼p​[⋅])，而这正是我们无法计算的！\",\"直观图示： 想象真实后验 p 是一个双峰分布（有两个波峰）。\",\"使用 DKL​(q∥p) 会让近似分布 q（如一个高斯分布）去拟合其中一个主要的波峰，得到一个虽然不完整但很精确的局部近似。\",\"使用 DKL​(p∥q) 会迫使单个高斯分布 q 去覆盖两个波峰，结果可能是在两个峰之间形成一个宽而不准的“驼峰”，拟合效果反而更差。\",\"VI通常选择更精确的模式寻求行为，并且其数学形式（对 q 求期望）在计算上是可行的。\"]},\"48\":{\"h\":\"总结\",\"t\":[\"根本原因：KL散度并非“被选为”VI的度量，而是在数学推导最小化后验近似误差的过程中，自然而然地成为了ELBO的一部分。最大化ELBO等价于最小化 DKL​(q∥p)。\",\"非对称性的优势：VI特意选择了 DKL​(q∥p)（反向KL），因为它的模式寻求（Mode-Seeking） 特性往往能产生更实际、更精确的近似，并且其数学形式（对近似分布 q 求期望）使得基于梯度的优化成为可能，这是VAE等工作的重要基础。\",\"ELBO的核心地位：ELBO不仅仅是包含KL散度，它本身就是一个可优化的目标函数。通过最大化ELBO，我们同时实现了两个目标：一是在逼近真实的证据 logp(x)，二是在最小化近似分布与真实后验之间的KL散度。\",\"所以，你的直觉是正确的：KL散度适合做变分推断，正是因为它与ELBO有着不可分割的、数学上的本质联系，并且其非对称性在计算和实践中提供了关键优势。\"]},\"49\":{\"h\":\"KL and JS divergence\",\"t\":[\"好的，这是一个非常自然的延伸问题。Jensen-Shannon Divergence (JS散度) 正是为了克服KL散度的一些主要缺点而设计的。\"]},\"50\":{\"h\":\"什么是Jensen-Shannon Divergence（JS散度）？\",\"t\":[\"JS散度是KL散度的一种变体，它通过对称化和平滑处理，得到了一个对称的、总是有定义的、且值域固定的散度度量。\",\"它的核心思想是：计算两个分布 P 和 Q 与它们的中间分布（平均分布）M 之间的KL散度的平均值。\"]},\"51\":{\"h\":\"1. 数学定义\",\"t\":[\"给定两个概率分布 P 和 Q，我们首先定义一个中间分布 M：\",\"M=21​(P+Q)\",\"即，M 是 P 和 Q 的简单算术平均。\",\"然后，JS散度被定义为：\",\"DJS​(P∥Q)=21​DKL​(P∥M)+21​DKL​(Q∥M)\",\"其中，DKL​ 是之前介绍的KL散度。\",\"解读：\",\"JS散度衡量的是 P 和 Q 分别与它们的“中间人” M 的平均差异。\",\"由于 M 是由 P 和 Q 混合而成，它保证了在任何 P 或 Q 有定义的点上，M 的概率都大于零（只要 P 和 Q 不是完全无处重叠）。这解决了KL散度中分母 Q(i) 可能为零的问题。\"]},\"52\":{\"h\":\"2. 为什么JS散度可以作为距离？\",\"t\":[\"虽然JS散度的名字是“散度”（Divergence），但它的数学性质使其可以作为一个真正的距离度量（在数学上称为度量函数, Metric）。一个真正的距离度量必须满足以下四个条件：\",\"非负性（Non-negativity）: DJS​(P,Q)≥0\",\"同一性（Identity of Indiscernibles）: DJS​(P,Q)=0当且仅当P=Q\",\"对称性（Symmetry）: DJS​(P,Q)=DJS​(Q,P)\",\"三角不等式（Triangle Inequality）: DJS​(P,R)≤DJS​(P,Q)+DJS​(Q,R)\",\"让我们逐一验证JS散度是否满足这些条件：\",\"非负性：KL散度本身是非负的，所以两个非负KL散度的平均值也必然非负。✅\",\"同一性：如果 P=Q，那么 M=P=Q，此时 DKL​(P∥M)=0，所以 DJS​=0。反之，如果 DJS​=0，也必然要求 P=Q。✅\",\"对称性：从定义就可以直接看出，DJS​(P∥Q) 和 DJS​(Q∥P) 的计算方式完全一样，仅仅是 P 和 Q 在公式中的位置交换了，结果必然相同。这完美解决了KL散度的非对称性问题。 ✅\",\"三角不等式：这一点证明起来较为复杂，但已经得到数学上的证明，JS散度满足三角不等式。✅\",\"正因为JS散度同时满足了以上四个条件，所以它可以被用作一个有效的、真正的距离度量。 它的平方根 DJS​​ 有时被用来形成一个更符合直观欧几里得空间性质的距离函数。\"]},\"53\":{\"h\":\"JS散度与KL散度的对比总结\",\"t\":[\"特性\",\"KL散度 (Kullback-Leibler Divergence)\",\"JS散度 (Jensen-Shannon Divergence)\",\"对称性\",\"非对称： DKL​(P∥Q)=DKL​(Q∥P)\",\"对称： DJS​(P∥Q)=DJS​(Q∥P)\",\"定义域问题\",\"可能无定义：当 Q(i)=0 而 P(i)>0 时，DKL​ 趋于无穷。\",\"总有定义：中间分布 M 平滑了 P 和 Q，避免了零概率问题。\",\"值域\",\"从 0 到 +∞\",\"从 0 到 ln(2)（以2为底的对数时是 0 到 1）有界，便于解释和比较。\",\"是否是距离度量\",\"不是，因为它不对称且不满足三角不等式。\",\"是，它满足距离度量的所有条件。\",\"主要用途\",\"源于信息论，衡量信息损失。常用于变分推断（VAE）。\",\"衡量两个分布之间的相似性。曾是生成对抗网络（GAN）的原始损失函数。\"]},\"54\":{\"h\":\"一个简单的例子\",\"t\":[\"假设有两个离散分布：\",\"P=[0.9,0.1,0.0] （90%的概率在事件1，10%在事件2，0%在事件3）\",\"Q=[0.0,0.1,0.9] （0%在事件1，10%在事件2，90%在事件3）\",\"计算KL散度:\",\"DKL​(P∥Q): 第一项 0.9∗log(0.9/0.0) 会遇到除零错误，无定义。\",\"DKL​(Q∥P): 第三项 0.9∗log(0.9/0.0) 也会遇到除零错误，无定义。\",\"计算JS散度:\",\"首先计算中间分布 M=21​(P+Q)=[0.45,0.1,0.45]\",\"然后计算：DKL​(P∥M)=0.9∗log(0.9/0.45)+0.1∗log(0.1/0.1)+0∗log(0/0.45)≈0.9∗1+0+0=0.9 （这里忽略一些严格定义，但结果是合理的）DKL​(Q∥M)=0∗log(0/0.45)+0.1∗log(0.1/0.1)+0.9∗log(0.9/0.45)≈0+0+0.9=0.9\",\"最终 DJS​(P∥Q)=21​(0.9+0.9)=0.9 （或根据对数底不同，是一个有界值）\",\"这个例子清晰地展示了JS散度在处理零概率问题和不对称性上的巨大优势。\"]},\"55\":{\"h\":\"Norm 范数\",\"t\":[\"好的，我们来一步一步地理解 L1 和 L2 Norm（范数）这两个非常重要且基础的概念。\",\"简单来说，Norm（范数） 是一个数学工具，用来衡量向量的大小或长度。你可以把它想象成一把测量“多维空间中的距离”的尺子。L1 和 L2 是这把尺子上两种最常见的刻度。\"]},\"56\":{\"h\":\"1. 从直观理解开始：从点到点的路径\",\"t\":[\"想象一下你在一个城市里，要从点 A 走到点 B。\",\"L2 Norm (欧几里得距离)：就像一只鸟直线飞过去的距离。这是我们所认为的“最短距离”。 \",\"在城市地图上，这就是连接两点的直线长度。\",\"L1 Norm (曼哈顿距离)：就像你沿着城市的街道网格走路，只能直角转弯。你走过的总路程就是 L1 距离。 \",\"它得名于纽约曼哈顿的棋盘式街道布局，你不能斜着穿过大楼，只能沿着街道走。\"]},\"57\":{\"h\":\"2. 数学定义\",\"t\":[\"现在我们用数学公式来精确地定义它们。假设我们有一个向量 v = [v₁, v₂, v₃, ..., vₙ]，它有 n 个维度。\"]},\"58\":{\"h\":\"L2 Norm (欧几里得范数)\",\"t\":[\"这是我们最熟悉的“长度”公式，是勾股定理在多维空间的扩展。计算公式：||v||₂ = √(v₁² + v₂² + v₃² + ... + vₙ²)\",\"计算示例： 向量 v = [3, 4] 它的 L2 Norm 是：√(3² + 4²) = √(9 + 16) = √25 = 5 这正好是一个 3-4-5 直角三角形的斜边长度。\"]},\"59\":{\"h\":\"L1 Norm (曼哈顿范数)\",\"t\":[\"这是向量所有维度绝对值的总和。计算公式：||v||₁ = |v₁| + |v₂| + |v₃| + ... + |vₙ|\",\"计算示例： 同样向量 v = [3, 4] 它的 L1 Norm 是：|3| + |4| = 3 + 4 = 7 这就是在网格上从 (0,0) 走到 (3,4) 需要走的最短路径（例如，先向东走 3 格，再向北走 4 格）。\"]},\"60\":{\"h\":\"3. 几何意义：单位圆\",\"t\":[\"一个非常好的方式来可视化 L1 和 L2 的区别是看它们的单位圆（Unit Circle）——即所有“长度为 1”的点构成的图形。\",\"L2 单位圆：所有到原点距离为 1 的点。这形成了一个我们熟悉的、光滑的圆形（在 3D 中是球体）。\",\"L1 单位圆：所有到原点曼哈顿距离为 1 的点。这形成了一个菱形（或正菱形）。\",\"从下图可以清晰地看到两者的几何差异：\"]},\"61\":{\"h\":\"4. 关键特性与对比\",\"t\":[\"特性\",\"L1 Norm (曼哈顿距离)\",\"L2 Norm (欧几里得距离)\",\"计算方式\",\"绝对值之和\",\"平方和的平方根\",\"几何形状\",\"菱形（有尖点）\",\"圆形（光滑）\",\"稳健性\",\"对异常值更不敏感（更稳健）\",\"对异常值非常敏感\",\"稀疏性\",\"倾向于产生稀疏解（很多系数为0）\",\"倾向于产生稠密解（系数都很小但非零）\",\"计算效率\",\"计算简单（只需加绝对值）\",\"计算涉及平方和开方，稍复杂\",\"为什么 L1 对异常值更不敏感？ 想象两个点：\",\"A: (1, 1)\",\"B: (1, 5)\",\"L2 距离：√((1-1)² + (5-1)²) = √(0 + 16) = 4\",\"L1 距离：|1-1| + |5-1| = 0 + 4 = 4\",\"现在把 B 点变成一个异常值：B: (1, 10)\",\"L2 距离：√((1-1)² + (10-1)²) = √(0 + 81) = 9 （变化了 5）\",\"L1 距离：|1-1| + |10-1| = 0 + 9 = 9 （变化了 5）\",\"看起来一样？我们再夸张一点，把 A 点放在原点 (0,0)，B 点放在 (1, 100)：\",\"L2 距离：√(1² + 100²) ≈ 100.005（距离几乎完全由巨大的异常值 100 决定）\",\"L1 距离：1 + 100 = 101（巨大值 100 和 小值 1 的贡献是线性叠加的）\",\"L2 因为计算了平方，会极大地放大异常值的影响。而 L1 只是线性地处理每个维度，所以某个维度上的巨大值不会完全主导整个距离的计算。\"]},\"62\":{\"h\":\"5. 在机器学习和数据科学中的应用\",\"t\":[\"正是基于以上特性，L1 和 L2 在不同的场景下各有千秋。\",\"作为损失函数 (Loss Function)\",\"L2 Loss (MSE, 均方误差)：最常用。它的光滑性使得求解梯度更容易，便于使用梯度下降法等优化算法。但对异常值敏感。\",\"L1 Loss (MAE, 平均绝对误差)：当你希望模型对数据中的异常值不敏感时使用（例如，金融数据中偶尔出现的极端波动）。\",\"作为正则化器 (Regularizer) 这是两者最重要的应用之一，用于防止模型过拟合。\",\"L2 正则化 (岭回归, Ridge Regression)：在损失函数中加入模型权重的 L2 Norm 作为惩罚项。它倾向于让所有的权重都变小，但不会让任何一个权重直接变为 0。模型会更平滑、更稳定。\",\"L1 正则化 (Lasso 回归, Lasso Regression)：在损失函数中加入模型权重的 L1 Norm 作为惩罚项。它强大的特性是倾向于产生稀疏解，即它会自动进行“特征选择”，把不重要的特征的权重直接压缩到 0。这对于处理高维数据非常有用。\"]},\"63\":{\"h\":\"总结\",\"t\":[\"L1 Norm\",\"L2 Norm\",\"绰号\",\"曼哈顿距离、 Taxicab Norm\",\"欧几里得距离、 “普通”距离\",\"计算\",\"绝对值之和\",\"平方和的平方根\",\"形状\",\"菱形\",\"圆形\",\"特点\",\"对异常值稳健、产生稀疏性\",\"对异常值敏感、解更平滑\",\"主要用途\",\"L1 损失(MAE)、L1正则化(特征选择)\",\"L2 损失(MSE)、L2正则化(防止过拟合)\",\"希望这个从直观到数学的解释能帮助你彻底理解 L1 和 L2 Norm 的区别！\"]},\"64\":{\"h\":\"Regularization for Deep Learning\",\"t\":[\"这是一个非常棒的问题，它触及了传统机器学习与深度学习之间核心思想的传承。\",\"简单答案是：岭回归（L2正则化）和Lasso回归（L1正则化）的基本思想——正则化（Regularization），不仅适用于深度学习，而且是其不可或缺的核心技术之一。 不过，它们通常不再被直接称为“岭回归”或“Lasso回归”，而是以更一般化的“L1/L2正则化”或“权重衰减”的形式出现。\",\"下面我们来一步步详细解释：\"]},\"65\":{\"h\":\"1. 核心思想的传承：对抗过拟合\",\"t\":[\"无论是线性模型还是深度神经网络，机器学习模型都有一个共同的敌人：过拟合（Overfitting）。即模型过于复杂，完美地记住了训练数据中的噪声和细节，导致在新数据上的泛化能力很差。\",\"岭回归(L2)的核心：通过在损失函数中增加模型权重（参数）的L2范数（平方和）作为惩罚项，迫使所有权重都趋向于变小、变得分散，而不是依赖少数几个巨大的权重。这会让模型变得更简单、更平滑，从而减轻过拟合。\",\"Lasso回归(L1)的核心：通过在损失函数中增加模型权重（参数）的L1范数（绝对值和）作为惩罚项，它倾向于将一些不重要的特征的权重直接压缩到零，从而实现特征选择，产生稀疏模型。\",\"深度学习完全继承了这一思想，并将其应用在规模更大、结构更复杂的模型上。\"]},\"66\":{\"h\":\"2. 在深度学习中的具体应用形式\",\"t\":[\"在深度学习框架（如PyTorch, TensorFlow）中，正则化被直接集成到了优化器里，使用起来非常方便。\"]},\"67\":{\"h\":\"L2正则化 -> “权重衰减”（Weight Decay）\",\"t\":[\"这是最直接、最广泛的应用。在深度学习中，L2正则化几乎等同于“权重衰减”这个概念。\",\"是什么：在每次参数更新时，除了根据梯度调整参数外，还会额外将参数值向零缩小一点点（乘以一个小于1的因子）。\",\"为什么有效：防止权重变得过大，鼓励模型使用所有特征一点点，而不是极度依赖少数特征，从而提高泛化能力。这极大地缓解了过拟合。\",\"如何使用：在定义优化器（如 Adam, SGD）时，直接设置 weight_decay 参数。\",\"# 在PyTorch中，使用L2正则化（权重衰减）非常简单 optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5) # `weight_decay` 参数就是L2正则化的强度系数（λ）\",\"几乎所有成功的深度学习模型（从ResNet到BERT）的训练配置中，你都能找到权重衰减的身影。它是深度学习训练的标配。\"]},\"68\":{\"h\":\"L1正则化\",\"t\":[\"L1在深度学习中也同样可用，但不如L2普遍。\",\"是什么：在损失函数中增加模型权重的绝对值和作为惩罚项。\",\"为什么有效：和Lasso回归一样，它倾向于产生稀疏权重，即让很多权重值直接变为零。这在某些场景下非常有用： \",\"模型压缩（Model Compression）：通过L1正则化，可以“剪枝”掉大量不重要的神经元连接（权重为0），得到一个稀疏模型。这个稀疏模型可以大幅减少内存占用和计算量，便于部署在手机、嵌入式设备等资源受限的环境中。\",\"特征选择：虽然深度学习自动学习特征，但有时我们仍想解释哪些输入特征更重要。L1正则化可以帮助凸显出最重要的输入路径。\",\"如何使用：通常需要手动将其添加到损失函数中，因为主流优化器没有像 weight_decay 那样的直接参数。\",\"# PyTorch中手动添加L1正则化的示例 l1_lambda = 1e-6 # L1正则化强度 l1_loss = 0 for param in model.parameters(): l1_loss += torch.norm(param, p=1) # 计算所有参数的L1范数 total_loss = your_original_loss_function(...) + l1_lambda * l1_loss optimizer.zero_grad() total_loss.backward() # 反向传播 optimizer.step() # 更新参数\"]},\"69\":{\"h\":\"3. 深度学习中的其他正则化“利器”\",\"t\":[\"深度学习的成功不仅仅依赖于L1/L2。由于其模型极其灵活和复杂，研究者们发明了更多样、更强大的正则化技术，这些技术与L1/L2相辅相成，共同对抗过拟合：\",\"Dropout：在训练过程中，随机地“丢弃”（暂时屏蔽）一部分神经元。这迫使网络不能过度依赖任何单个神经元，必须学习到更加鲁棒的特征。这是深度学习中最标志性的正则化方法之一。\",\"批量归一化（Batch Normalization）：虽然其主要目的是稳定训练、加速收敛，但通过引入训练和测试时的不一致性（噪声），它同时也起到了轻微的正则化效果。\",\"数据增强（Data Augmentation）：对输入数据（如图像）进行随机旋转、裁剪、变色等操作，人工扩大训练数据集。这是计算机视觉领域最强大的正则化手段。\",\"早停（Early Stopping）：在训练过程中持续监控验证集性能，一旦性能不再提升就停止训练，防止模型过度记忆训练集。\"]},\"70\":{\"h\":\"总结与对比\",\"t\":[\"技术\",\"源于\",\"在深度学习中的角色\",\"主要目的\",\"L2正则化\",\"岭回归\",\"至关重要，标配（以权重衰减的形式集成）\",\"防止权重过大，提高泛化能力\",\"L1正则化\",\"Lasso回归\",\"有选择地使用\",\"产生稀疏权重，用于模型压缩和特征选择\",\"Dropout/BN/数据增强\",\"深度学习原生\",\"与L1/L2同等重要，甚至更常用\",\"通过各种方式增加模型鲁棒性，防止过拟合\",\"结论：\",\"不要说“岭回归/Lasso回归适用于深度学习”，而应该说“L1和L2正则化的思想是深度学习正则化技术的基石”。\",\"对于绝大多数日常训练深度网络的任务，你应该默认使用L2正则化（权重衰减）。\",\"如果你有明确的模型轻量化、压缩或部署需求，可以尝试L1正则化。\",\"千万不要忽视Dropout、数据增强等其他正则化技术，它们通常能带来比单纯调大权重衰减系数更显著的效果。\",\"希望这个解释帮你理清了传统机器学习与深度学习之间美妙的联系！\"]},\"71\":{\"h\":\"SVD\",\"t\":[\"好的，我们一步一步来思考，彻底解释清楚奇异值分解（SVD）在机器学习中扮演的角色。\"]},\"72\":{\"h\":\"第一步：SVD 是什么？—— 拆解数据的终极工具\",\"t\":[\"想象一下，你有一个复杂的数据矩阵（比如，一个用户-电影评分矩阵，行是用户，列是电影，值是评分）。SVD 可以做一件非常厉害的事情：\",\"它将这个原始数据矩阵 M（形状为 m×n），分解成三个更简单、更有意义的矩阵的乘积：\",\"M = U * Σ * Vᵀ\",\"让我们来拆解这三个部件：\",\"U (左奇异向量矩阵): 一个 m×m 的正交矩阵。你可以把它理解为代表了行空间的“本质特征”。在我们的例子里，每一行是一个用户，所以 U 的每一列代表了一种“用户类型” 或“用户偏好模式”。例如，第一种模式可能是“科幻电影爱好者”，第二种是“浪漫喜剧爱好者”。\",\"Σ (奇异值矩阵): 一个 m×n 的对角矩阵。这是整个分解的核心。对角线上的元素被称为奇异值，它们从大到小排列（σ₁ ≥ σ₂ ≥ σ₃ ≥ ... ≥ 0）。\",\"奇异值的大小衡量了其对应的“模式”在原始数据中有多重要。 最大的奇异值 σ₁ 对应了数据中最主要、最显著的变化趋势（比如，最主流的观影偏好），而较小的奇异值可能对应一些次要趋势或噪声。\",\"Vᵀ (右奇异向量矩阵的转置): 一个 n×n 的正交矩阵。你可以把它理解为代表了列空间的“本质特征”。在我们的例子里，每一列是一部电影，所以 V 的每一行代表了一种“电影类型”。例如，第一种类型可能代表了“科幻程度”，第二种代表了“喜剧程度”。\",\"一个简单的比喻： SVD 就像是用一套完美的解剖工具，将一个复杂的物体（数据矩阵 M）分解成了：\",\"U： 告诉你这个物体由哪些基本成分构成。（有哪些用户类型？）\",\"Σ： 告诉你每种基本成分的含量有多少。（每种类型有多重要？）\",\"Vᵀ： 告诉你这些基本成分的配方。（每部电影属于各种类型的比例是多少？）\"]},\"73\":{\"h\":\"第二步：SVD 为什么在机器学习中如此强大？—— 它的核心能力\",\"t\":[\"基于上述分解，SVD 衍生出几个在机器学习中至关重要的能力：\",\"降维与压缩 (Dimensionality Reduction & Compression)\",\"如何做：由于奇异值从大到小排列，我们可以只保留前 k 个最大的奇异值（以及对应的 U 和 Vᵀ 中的前 k 列/行），从而得到一个原始矩阵 M 的最佳低秩近似矩阵 M_k。\",\"为什么有用：M_k 捕获了数据中最重要、最有信号的 k 个模式，而舍弃了那些不重要的、可能是噪声的模式。这极大地减少了存储量和计算量，同时保留了数据的核心结构。这被称为 Truncated SVD（截断SVD）。\",\"去噪 (Denoising)\",\"如何做：与降维原理相同。小的奇异值通常对应数据中的噪声或无关紧要的细节。通过丢弃它们，我们得到的新矩阵 M_k 就是一个去噪版的数据矩阵。\",\"潜在语义分析 (Latent Semantic Analysis)\",\"如何做：在降维后的新空间（由 U_k 和 V_k 张成的空间）里观察数据。这个空间通常被称为“潜在空间”。\",\"为什么有用：在这个空间里，数据点（用户、电影、词语）之间的关系变得更容易理解。例如，即使两个用户没有对任何同一部电影评分，但如果在潜在空间中他们的向量很接近，我们就可以认为他们有相似偏好。这解决了稀疏性和同义词问题。\",\"矩阵补全 (Matrix Completion)\",\"如何做：著名的推荐系统算法（Netflix Prize 的核心）就基于此。我们有一个不完整的评分矩阵 M（很多缺失值）。SVD 可以帮助我们找到那个“本质上”最应该存在的低秩矩阵 M_k，然后用 M_k 中对应的值来预测M 中的缺失值。\"]},\"74\":{\"h\":\"第三步：SVD 在机器学习中的具体应用场景\",\"t\":[\"现在，让我们把这些能力对应到具体的应用上：\",\"应用领域\",\"如何利用 SVD\",\"解决的问题\",\"推荐系统\",\"对“用户-物品”评分矩阵进行 SVD，预测缺失的评分。\",\"矩阵补全、稀疏性\",\"自然语言处理（NLP）\",\"对“词-文档”矩阵（如 TF-IDF 矩阵）进行 SVD（称为 LSA/LSI）。\",\"语义分析、文档检索、话题建模、降维\",\"计算机视觉（CV）\",\"将一幅图像视为矩阵，进行 SVD 压缩。保留前 k 个奇异值就能较好地重构图像。\",\"图像压缩、去噪\",\"数据预处理\",\"对任何特征矩阵进行 SVD，用得到的 U_k * Σ_k 作为新的、降维后的特征矩阵。\",\"降维、去除特征间的多重共线性\",\"主成分分析（PCA）\",\"PCA 实际上可以通过数据的协方差矩阵的 SVD 来计算！ 这是 SVD 一个极其重要的应用。\",\"数据降维、可视化\"]},\"75\":{\"h\":\"第四步：一个简单的例子——推荐系统\",\"t\":[\"假设我们有一个小小的用户-电影评分矩阵（1-5分）：\",\"电影A（科幻）\",\"电影B（科幻）\",\"电影C（浪漫）\",\"用户1\",\"5\",\"4\",\"1\",\"用户2\",\"4\",\"5\",\"2\",\"用户3\",\"1\",\"1\",\"5\",\"SVD 分解：我们对这个矩阵进行 SVD，发现可能只有一个很大的奇异值 σ₁。\",\"解读：这说明数据中只有一个主导模式。U 的第一列显示用户1和用户2在这个模式上有很高的正值，用户3是负值。Vᵀ 的第一行显示电影A和B有很高的正值，电影C是负值。\",\"潜在语义：这个模式就是 “科幻 vs. 反科幻/浪漫” 。用户1和2是科幻迷，用户3是浪漫片爱好者。\",\"预测：现在，假设用户2没有对电影C评分。但在潜在空间中，我们知道用户2是“科幻迷”，而电影C是“反科幻”的，因此我们可以预测用户2会给电影C一个低分。同理，可以预测用户3会给电影A低分。\"]},\"76\":{\"h\":\"总结\",\"t\":[\"SVD 在机器学习中扮演着 “数据本质提取器” 的角色。它的核心价值在于：\",\"降维：用更少的信息表示最多的数据内涵。\",\"去噪：过滤掉不重要或有害的细节。\",\"揭示潜在关系：发现数据背后隐藏的、无法直接观测到的模式（潜在语义）。\",\"它是一种非常强大且基础的数学工具，为从推荐系统到搜索引擎，再到图像处理的众多机器学习应用提供了坚实的理论基础和高效的算法实现。\"]},\"77\":{\"h\":\"Bayesian LSTM\",\"t\":[\"https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2021WR029772\"]},\"78\":{\"h\":\"Global sparse momentum SGD\",\"t\":[\"https://github.com/DingXiaoH/GSM-SGD\"]},\"79\":{\"h\":\"How to write  literature review\",\"t\":[\"https://pdf.sciencedirectassets.com/271680/1-s2.0-S0148296319X00089/1-s2.0-S0148296319304564/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOr%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQCSnMYVwHB8y4xiTWYZD1JtOi16l4ZzwLS9VmLkeuBWCgIgUAjrZhmEK7ewzRsPKEzEZkEi%2BuR6aXL6ieEwqXbkiUsqswUIYxAFGgwwNTkwMDM1NDY4NjUiDBu5FI67ja8KifcdJyqQBbQoxJ5ZKt1G2OL3hlJdCFel4QdjH3pL809GkAp10E4k1X1VF5kTj77pTmR8flmbqHE9yvysZ9C0TMTGtmXsS1T7xW5XoEdcMJkMdNb2RJNy%2BXPhVKTuAmCD%2BNPFhZE7GSx6n1qXtyfWCSifuvIJosNDR0wbYZ%2BFu1ZFK37vcflyVn%2F5vTXcK055rKrlhDcGcW29GPSXtbmouRYFEeym16L%2FLIe7ZQpUrjUEKEA0MGsoMigKa%2B66g%2FEzzHr4q0lBL8J4xvm5mZSWlmeTH2f5jKR2wEey3XDbH0mQLLw9d7dQWBEM6%2Fe7fISkmQmIxFree%2BazeL1%2FA6WC%2FrDcbZtk%2FisYERVsEWROCLFrHiRjkwrszbhqJ5ia%2BIO9QMK7tQBbSTmW5WticmzujymSoKNNBYDuGmbi12A3iQVH01%2BAhBEk14p%2B5mYHnJfhF7x8YN5jgQ5YfeHeULg3a2OecglsIk96Q%2F8U9zDZI9HHDhUVXmxsArq1nXDUJw6LkYUl2H1psWypJ8lwK99BEveXzQ6JL%2FSKnlJOpsmh%2Bkv0DauPm8%2FoEUAY6xOvAGp5AktNpB9X0XYBZK%2BKz%2BQyfQ5ROqGKIVips3flVRg6GA3hnLq%2BUnqlIudQkFHyQeE51hrxuoUNH%2FSS7CNKexl3RSbF6ftEh%2BWWFH%2BsQNcXR5RlGeK2L2Kvrp9DGXc1s6derMhN6LQ%2Bxq3ygKI6k6m5YVn8gNd1EwVH7Fw6q9yjBOVqaGZtNAGfvIeP7zcGRpxDzFGuB2qOPZ6KfIyV2SIh728GcsERSg4mPHQXk2Sj0CILhj7XxyuZP14GceZAp7RIrRql1TzPpDovDGVJ2ChQ3vxN8VMXxZvUIVNn2Cjr6DimKTNNoe%2FsMIjauLIGOrEBDrnagmyuxxq88QmWYHsxwToajr0UFxFXYllP2MBf2v5htcm5us%2FqGx%2BZYaf8g6y1Drwy9wrdCQ%2FcIc%2FhvHPFTvyUjfYmkavWvKtV2FZvZoF1iw5eAnb%2BymarHiMXm3SQ6jBpvP9ZsuBhVZXFAcTBAIxADXzfL6xzryj7gpHpEQs71F5VuAb8wY6y2lKPMJ933egYc6y41xsU%2BDiubhsXZOZ%2BYL%2B1Wa9aflhjm6iUXYQw&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240522T182749Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYTZLO5ZX7%2F20240522%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=eda871ed11ea3537e664b33ba7b2b5a284ec92a24a315646a740db06b1fada96&hash=a6cb9df501568760933d3f4e845acb3f9f728bfbbe52affe2a433a8ddd93ff4d&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0148296319304564&tid=spdf-76069940-c1c2-47dd-8f4a-1c9bddb9bdb0&sid=dfe244208eb5c1425d1b401-3111e97cd630gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=120d5c5f540a060106570d&rr=887edcc6ba568b3f&cc=hk&kca=eyJrZXkiOiI4azV0eml4NkRkWktZa0x1a2xRL0lYaEtHWTNpZnZxemhWMmJ4MjNMQ2kzckJJRVlkWURGRFJtS0YrNVBJOWIxU3E4OUQ3U1F0UUdSZ0p6Zi9wdk1CVnU3bTVZSmJIdC9vVTRpV0xsTXVEOWovMkZ1WTd5cTUyTThVL0VCZTRobFF1YlI2ZmxqWmhSMHJqVTRaaVRKM0lVa05pU2JuSVkyVFVnOWUzTzg2RXR6aXdwTXBRPT0iLCJpdiI6IjVhNTY4ZTEzNDFjYmMxMDAxN2RkODVhODAyY2ExMjg0In0=_1716402475924\"]},\"80\":{\"h\":\"RL的主要结构种类\",\"t\":[\"好的，这是一个非常核心的问题。强化学习（RL）的算法世界看似纷繁复杂，但它们大多可以根据几个关键的维度进行划分和理解。\",\"我们可以将强化学习算法的结构主要分为三大类，这三类回答了“算法学习的核心是什么？”这个问题。此外，还有两个重要的正交维度，它们描述了“算法如何学习？”。\"]},\"81\":{\"h\":\"核心三大算法结构 (学习什么？)\",\"t\":[\"这三大结构是理解所有RL算法的基石。\"]},\"82\":{\"h\":\"1. 基于价值的算法 (Value-Based)\",\"t\":[\"这类算法不直接学习要执行哪个动作，而是学习一个“价值函数”或“打分函数”，来评估在某个状态下执行某个动作有多好。最终，智能体（Agent）会选择那个预估得分最高的动作。\",\"核心思想：学习一个“得分表”（价值函数Q(s, a)），这个表告诉你，在状态s下执行动作a，长期来看能获得多少奖励。\",\"输出：每个“状态-动作”对的价值（分数）。\",\"决策方式：在当前状态下，查看得分表，选择得分最高的动作执行（贪心策略）。\",\"优点： \",\"在学习的早期阶段通常更稳定。\",\"样本利用效率相对较高（特别是Off-policy类型，如Q-Learning）。\",\"缺点： \",\"通常难以处理连续的动作空间（想象一下，你无法为无限个动作都创建一个得分）。\",\"学习的是确定性策略，无法学习随机策略。\",\"代表算法： \",\"Q-Learning：经典的表格型RL算法。\",\"DQN (Deep Q-Network)：将Q-Learning与深度神经网络结合，开启了深度强化学习的时代，用于处理高维状态空间（例如，从像素玩游戏）。\"]},\"83\":{\"h\":\"2. 基于策略的算法 (Policy-Based)\",\"t\":[\"这类算法不学习每个动作的分数，而是直接学习一个“策略函数”，这个函数直接告诉智能体在某个状态下应该采取什么动作。\",\"核心思想：直接学习一个“行为指南”（策略π(a|s)），这个指南明确指出在状态s下，执行动作a的概率是多少。\",\"输出：一个概率分布，表示在当前状态下选择每个动作的可能性。\",\"决策方式：根据策略函数输出的概率分布进行抽样，选择一个动作。\",\"优点： \",\"能很好地处理连续动作空间。\",\"能够学习随机策略，这在某些环境中（如石头剪刀布）至关重要。\",\"策略更新通常更平滑。\",\"缺点： \",\"训练过程容易不稳定，方差较大（因为奖励信号可能很稀疏或有噪声）。\",\"样本利用效率通常较低（特别是On-policy类型）。\",\"代表算法： \",\"REINFORCE：最基础的策略梯度算法。\",\"PPO (Proximal Policy Optimization) / TRPO (Trust Region Policy Optimization)：REINFORCE的改进版，通过限制每次策略更新的幅度来提高稳定性。\"]},\"84\":{\"h\":\"3. 演员-评论家算法 (Actor-Critic)\",\"t\":[\"这是前两类算法的结合体，旨在取长补短。它包含两个部分：\",\"演员 (Actor)：一个基于策略的网络，负责根据当前状态选择动作。\",\"评论家 (Critic)：一个基于价值的网络，负责评估演员选择的动作有多好，并向演员提供反馈。\",\"核心思想：演员负责行动，评论家负责打分和指导。评论家告诉演员“你这个动作是好是坏，好多少，坏多少”，演员根据这个更明确的指导来优化自己的行为策略。\",\"工作流程：演员做出动作 -> 评论家评估该动作并给出反馈 -> 演员根据反馈更新策略。\",\"优点：\",\"结合了两者的优点：既能处理连续动作空间，又通过评论家的指导降低了训练的方差，比纯策略梯度法更稳定、学习更快。\",\"是目前解决复杂RL问题最主流和最强大的框架。\",\"缺点：\",\"结构更复杂，有两个网络需要训练和协调。\",\"代表算法：\",\"A2C / A3C (Advantage Actor-Critic)：经典的Actor-Critic算法。\",\"DDPG / SAC (Soft Actor-Critic)：专为连续控制任务设计的强大算法。\",\"PPO：实际上，PPO也是一种Actor-Critic的实现，它有一个策略网络（Actor）和一个价值网络（Critic）。\"]},\"85\":{\"h\":\"另外两个重要的分类维度 (如何学？)\",\"t\":[\"这两个维度可以与上述三大结构任意组合。\"]},\"86\":{\"h\":\"A. 无模型 (Model-Free) vs. 有模型 (Model-Based)\",\"t\":[\"无模型 (Model-Free)：算法不试图去理解环境的运作规则（物理模型）。它只通过大量的反复试验来学习价值或策略。就像一个不懂棋理但下了无数盘棋的棋手。\",\"优点：简单直接，适用于环境模型未知或非常复杂的情况。\",\"缺点：通常需要海量的样本才能学好（样本效率低）。\",\"绝大多数流行算法都是Model-Free的，例如DQN, PPO, SAC。\",\"有模型 (Model-Based)：算法会先尝试学习一个环境的模型（即预测“我在状态s下执行动作a，会转移到哪个新状态s'并得到多少奖励r”）。然后，它可以在这个内部“虚拟环境”中进行规划和学习。\",\"优点：样本效率高得多，因为它可以在虚拟环境中无限次尝试。\",\"缺点：如果环境模型学得不准，会导致“差之毫厘，谬以千里”的后果。\",\"代表算法：AlphaGo, AlphaZero（其MCTS树搜索部分就扮演了模型的角色）。\"]},\"87\":{\"h\":\"B. 在线策略 (On-Policy) vs. 离线策略 (Off-Policy)\",\"t\":[\"在线策略 (On-Policy)：学习用的数据必须是由当前正在优化的策略所产生的。简单说就是“边玩边学，用刚玩的数据学”。\",\"优点：训练过程通常更稳定。\",\"缺点：样本利用效率低，因为每次策略更新后，之前的数据就不能再用了，必须重新采集。\",\"代表算法：SARSA, PPO, A2C。\",\"离线策略 (Off-Policy)：学习用的数据可以来自于任何策略，不一定非得是当前策略。可以把过去的所有经验（无论好坏）都存进一个“经验池”，然后反复从中抽取数据来学习。\",\"优点：样本利用效率极高，可以重复利用旧数据。\",\"缺点：训练过程可能不稳定，容易产生高估等问题。\",\"代表算法：Q-Learning, DQN, SAC, DDPG。\"]},\"88\":{\"h\":\"总结\",\"t\":[\"算法结构\",\"核心思想\",\"输出\",\"优点\",\"缺点\",\"代表算法\",\"基于价值\",\"学习每个动作的“得分”\",\"Q值 (分数)\",\"稳定，样本效率较高\",\"难处理连续动作\",\"DQN, Q-Learning\",\"基于策略\",\"直接学习“行为指南”\",\"动作的概率\",\"能处理连续动作，能学随机策略\",\"方差大，不稳定\",\"REINFORCE, PPO\",\"演员-评论家\",\"演员行动，评论家指导\",\"动作概率+状态价值\",\"结合两者优点，稳定且高效\",\"结构复杂\",\"A2C, SAC, PPO\"]},\"89\":{\"h\":\"Generally, DETR\"},\"90\":{\"h\":\"DETR算法：基于Transformer的端到端目标检测新范式\",\"t\":[\"DETR（DEtection TRansformer）是一种基于Transformer架构的端到端目标检测算法，由Facebook AI Research（现为Meta AI）于2020年提出。它的核心思想是将目标检测任务视为一个集合预测问题，从而摒弃了传统目标检测方法中许多需要手动设计的组件，如非极大值抑制（NMS）和锚框（Anchor）。\"]},\"91\":{\"h\":\"\",\"t\":[\"传统的目标检测算法通常需要预设大量的锚框，然后对这些锚框进行分类和回归，最后通过NMS来消除冗余的检测框。DETR则另辟蹊径，其主要创新点在于：\",\"集合预测问题：DETR将目标检测任务直接建模为一个从图像到一组“物体预测”的集合的映射问题。它一次性输出一个固定大小的预测集合，该集合包含了图像中所有物体的信息。\",\"端到端学习：整个模型是一个单一的、端到端的网络，无需任何后处理步骤。\",\"二分图匹配：在训练过程中，DETR使用匈牙利算法（Hungarian algorithm）来寻找预测框与真实框之间的最佳二分匹配，从而实现一对一的分配，避免了重复预测。\"]},\"92\":{\"h\":\"\",\"t\":[\"DETR的整体架构简洁明了，主要由以下三个部分组成：\",\"CNN主干网络（Backbone）：通常使用一个预训练的卷积神经网络（如ResNet）来提取图像的紧凑特征表示。\",\"Transformer编码器-解码器（Encoder-Decoder）：这是DETR的核心。 \",\"编码器（Encoder）：接收来自CNN主干网络的图像特征，并结合位置编码，通过多头自注意力机制来学习图像的全局上下文信息。\",\"解码器（Decoder）：接收编码器的输出以及一组可学习的位置编码，即“对象查询”（Object Queries）。每个对象查询负责预测一个特定的物体。解码器通过自注意力和交叉注意力机制来解码出每个物体的位置和类别信息。\",\"前馈神经网络（Feed-Forward Network, FFN）：解码器的输出会经过一个简单的前馈网络，最终预测出每个物体的类别和边界框坐标。\"]},\"93\":{\"h\":\"\",\"t\":[\"优势：\",\"简化流程：DETR摆脱了对锚框和NMS等复杂手工组件的依赖，使得目标检测流程更加简洁直观。\",\"端到端：模型可以直接输出最终的检测结果，无需后处理，使得训练和推理过程更加一体化。\",\"全局信息建模：得益于Transformer的自注意力机制，DETR能够更好地利用图像的全局上下文信息，这对于理解物体间的关系和减少误检有很大帮助。\",\"挑战与改进：\",\"尽管DETR具有开创性，但其原始版本也存在一些问题：\",\"收敛速度慢：相比于Faster R-CNN等传统检测器，DETR需要更长的训练时间才能收敛。\",\"对小目标的检测性能较差：由于Transformer注意力模块在处理高分辨率特征图时的计算复杂度较高，DETR在小物体上的检测效果不佳。\",\"为了解决这些问题，研究者们提出了一系列改进模型，例如：\",\"Deformable DETR：通过引入可变形注意力模块，只关注参考点周围的一小组关键采样点，从而降低了计算复杂度，加快了收敛速度，并提升了对小目标的检测性能。\",\"DINO (DETR with Improved Denoising Anchor Boxes)：通过改进训练方法和引入对比去噪训练等技巧，显著提升了DETR的性能和收敛速度。\",\"RT-DETR：通过设计混合编码器和实现IoU感知的查询选择，显著提高了检测速度和准确性。\",\"总而言之，DETR的提出为目标检测领域带来了一种全新的思路，推动了该领域向更简洁、更端到端的方向发展。尽管存在一些挑战，但其后续的众多改进模型已经证明了这一方向的巨大潜力和价值。\",\"https://developer.volcengine.com/articles/7382389058494562354#article_title\"]},\"94\":{\"h\":\"I-JEPA and ViT\"},\"95\":{\"h\":\"I-JEPA 深入解析：它不仅是ViT，更是一种创新的自监督学习框架\",\"t\":[\"近期备受关注的I-JEPA（Image-based Joint-Embedding Predictive Architecture，基于图像的联合嵌入预测架构）并非简单等同于Vision Transformer (ViT)，而是一种巧妙运用ViT作为其核心构建模块的自监督学习算法。从本质上讲，I-JEPA是一个学习框架，旨在让模型像人类一样，通过理解世界的基本模式来学习，而不仅仅是记忆像素细节。\"]},\"96\":{\"h\":\"I-JEPA的核心思想：在抽象空间中进行预测\",\"t\":[\"传统的一些自监督学习方法，如掩码自编码器（Masked Autoencoders, MAE），通过让模型重建被遮挡（mask）的图像块的原始像素来学习。然而，I-JEPA另辟蹊径，它的核心思想是在一个抽象的、高维的表示空间中进行预测。\",\"具体来说，I-JEPA会向模型展示一张图片的一部分（称为“上下文块”），然后要求模型预测图片中其他被遮挡部分（称为“目标块”）的表示（representation），而不是它们的具体像素。 这种在抽象空间中进行预测的方式，使得模型能够学习到更加语义化和高级别的特征，因为它不必纠结于生成所有微小的、可能无关紧要的像素细节。 这也使得I-JEPA在计算上比像素重建方法（如MAE）更加高效。\"]},\"97\":{\"h\":\"I-JEPA的架构：由ViT构成的三大组件\",\"t\":[\"I-JEPA的整体架构由三个关键部分组成，而这些部分均基于Vision Transformer（ViT）构建：\",\"上下文编码器 (Context Encoder): 这是一个标准的ViT，负责处理输入的、可见的“上下文块”，并生成其对应的特征表示。\",\"目标编码器 (Target Encoder): 这同样是一个ViT，它的作用是生成被遮挡的“目标块”的真实表示，作为模型预测的目标。值得注意的是，目标编码器的权重并不会通过梯度下降直接进行训练。相反，它是上下文编码器权重的一个指数移动平均（Exponential Moving Average, EMA）。 这种设计有助于稳定训练过程，并让目标表示更加平滑。\",\"预测器 (Predictor): 这是一个规模较小的ViT（通常更窄），它接收来自上下文编码器的输出，并结合目标块的位置信息，来预测目标块的表示。\",\"整个学习过程的目标，就是最小化预测器生成的表示与目标编码器生成的真实表示之间的差异（通常使用L2距离来衡量）。\"]},\"98\":{\"h\":\"多块掩码策略：鼓励学习语义信息\",\"t\":[\"为了引导模型学习到更具全局性和语义性的特征，I-JEPA采用了一种“多块掩码”（multi-block masking）策略。 这意味着它会同时遮挡掉图像中多个、且尺寸较大的区域作为目标块，并使用一个在空间上分布足够广泛的上下文块来进行预测。 这种策略迫使模型不仅仅依赖于局部信息，而是要从更广阔的上下文中理解图像的整体结构和内容。\"]},\"99\":{\"h\":\"I-JEPA与ViT的关系：框架与组件\",\"t\":[\"回到最初的问题，I-JEPA并非就是ViT。更准确地说，I-JEPA是一种自监督学习框架，它使用ViT作为其核心的神经网络架构。\",\"ViT是一种模型架构，它将Transformer模型应用于计算机视觉任务，通过将图像分割成块（patches）并像处理单词一样处理它们，从而捕捉图像中的长距离依赖关系。\",\"I-JEPA是一种训练方法或算法，它定义了模型应该学习什么样的任务（在抽象空间中预测被遮挡部分的表示），以及如何构建和训练这个模型（通过上下文编码器、目标编码器和预测器）。\",\"可以将ViT看作是构建I-JEPA这座“大厦”的“砖块”。I-JEPA巧妙地组织和运用了这些“砖块”，创造出一种高效且强大的自监督学习范式。\"]},\"100\":{\"h\":\"I-JEPA的优势与意义\",\"t\":[\"高效性: 由于在抽象空间进行预测，避免了繁重的像素级重建，I-JEPA的训练效率显著高于MAE等方法。\",\"强大的性能: 经过I-JEPA预训练的模型在多种下游视觉任务中都表现出色，例如在ImageNet上的线性探测（linear probing）和半监督评估。\",\"无需大量数据增强: 与许多依赖于复杂手工数据增强的自监督方法不同，I-JEPA能够在不使用或很少使用数据增强的情况下学习到高质量的语义表示。\",\"总而言之，I-JEPA是自监督学习领域的一项重要进展，它展示了通过在抽象表示空间中进行预测来学习强大视觉特征的巨大潜力。它与ViT的结合，为构建更通用、更高效、更接近人类学习方式的计算机视觉模型开辟了新的道路。\"]},\"101\":{\"h\":\"Mode Collapse\",\"t\":[\"是的，你的这个洞察非常深刻！这种“生成者 vs 评估者”的双网络博弈结构，都内在地容易出现一种核心问题，那就是“多样性丧失”，我们通常称之为模式坍塌（Mode Collapse）。\",\"虽然在GAN和Actor-Critic中这个问题的具体名称和表现形式有所不同，但其根本原因和现象是高度相似的。\"]},\"102\":{\"h\":\"1. GAN中的模式坍塌 (The Classic Case)\",\"t\":[\"在GAN的语境中，“模式坍塌”是一个非常著名且棘手的问题。\",\"现象是什么？ 生成器（Generator）变得“懒惰”和“投机取巧”。它发现只要生成一种或少数几种能够完美欺骗判别器（Discriminator）的样本，就能获得很好的分数。于是，无论给它什么随机噪声作为输入，它都只输出这些“安全”的、单一的样本。\",\"例子：一个训练用来生成人脸的GAN，发生了模式坍塌，结果只会生成同一张网红脸，或者只会生成特定角度的男性头像，完全丧失了生成多样化人脸的能力。\",\"为什么会发生？ 这源于对抗博弈的纳什均衡点难以找到。生成器找到了判别器的一个“盲点”，然后就疯狂利用这个盲点。判别器可能因为训练得不够好，或者梯度信息不够丰富，无法给生成器提供“你应该去探索其他类型样本”的有效指导。于是，整个系统陷入了一个非常糟糕的局部最优解。\"]},\"103\":{\"h\":\"2. Actor-Critic中的“模式坍塌” (The Analogue)\",\"t\":[\"在Actor-Critic或者更广泛的强化学习（RL）中，虽然我们不常用“模式坍塌”这个词，但存在一个完全等价的问题，通常被称为：\",\"过早的策略收敛 (Premature Policy Convergence)\",\"陷入局部最优 (Getting Stuck in a Local Optimum)\",\"现象是什么？ 演员（Actor）发现执行某一个或一小撮特定的动作，可以从评论家（Critic）那里稳定地获得还不错的评估分数。于是，演员的策略迅速变得“确定性”，即在某个状态下，它总是以接近100%的概率选择同一个动作，完全停止了对其他可能更好动作的探索。\",\"例子：一个控制机器人走路的RL智能体，可能发现“每次都先迈左腿，然后小碎步前进”这个策略能够避免摔倒并获得正向奖励。于是它就一直使用这个笨拙但安全的策略，再也不去尝试学习“左右腿交替大步走”这种更优但早期探索时有风险的策略。这就是一种行为模式上的坍塌。\",\"为什么会发生？ 这源于强化学习中一个经典的核心矛盾：探索（Exploration） vs. 利用（Exploitation）。\",\"利用：智能体执行它当前认为最好的动作，以最大化短期回报。这对应了GAN中生成器找到并利用判别器的盲点。\",\"探索：智能体尝试一些新的、未知的动作，希望能发现长期来看更好的策略。\",\"如果算法过分强调“利用”，Actor就会迅速收敛到第一个它发现的“还不错”的策略上，而Critic也会因为只看到这种单一的行为模式，从而强化对这个局部最优策略的评估。整个系统停止了探索，策略的多样性消失了。\"]},\"104\":{\"h\":\"核心共性与解决方案\",\"t\":[\"共性： 两个结构中的“生成者”（Generator/Actor）都过早地停止了探索，收敛到了一个狭窄的、缺乏多样性的输出空间，从而欺骗或满足了当时的“评估者”（Discriminator/Critic）。\",\"解决方案的思路也是相通的：鼓励多样性！\",\"在GAN中：\",\"改进损失函数：例如WGAN（Wasserstein GAN）使用能提供更平滑、更丰富梯度信息的损失函数，避免判别器变得过于“绝对”。\",\"架构调整：例如Minibatch Discrimination，让判别器不仅看单个样本，还看一批样本的统计特征，如果一批样本都差不多，就判定为假。\",\"多生成器/判别器：使用多个模型来避免单一模型的崩溃。\",\"在Actor-Critic (RL)中：\",\"熵正则化 (Entropy Regularization)：这是最核心也最优雅的解决方案。在优化目标里，除了最大化奖励，还额外增加一项“策略熵（Entropy）”作为奖励。熵衡量的是策略的不确定性/随机性。奖励熵，就等于是在直接鼓励Actor保持其动作选择的多样性，不要过早地让策略变得太确定。这会惩罚那些“模式坍塌”的策略。现代的AC算法如SAC（Soft Actor-Critic）就将这个思想作为其核心。\",\"增加探索噪声：在Actor选择动作时，人为地加入一些随机噪声，强制它进行探索。这在DDPG等算法中很常见。\",\"结论：\",\"你的观察是完全正确的。双网络博弈结构确实天然地存在“多样性丧失”的风险。 无论是GAN中的模式坍塌，还是RL中的过早收敛，都是同一个问题的不同表现形式。因此，两个领域也都独立发展出了一系列旨在“鼓励和维持多样性”的先进技术来对抗这一共同的挑战。\"]},\"105\":{\"h\":\"How PPO improved on mode collapsing\",\"t\":[\"非常好，这个问题直击了PPO算法设计的精髓。PPO通过其核心机制间接地、但非常有效地应对了策略上的“模式坍塌”（即过早收敛到局部最优）。\",\"它并非像熵正则化那样直接奖励多样性，而是通过一种更根本的方式——保证优化过程的稳定性——来避免策略过早地变得狭隘和确定。\",\"PPO主要通过以下两个相互关联的机制来做到这一点：\"]},\"106\":{\"h\":\"1. 核心机制：截断的替代目标 (Clipped Surrogate Objective)\",\"t\":[\"这是PPO的灵魂。它的目标是：在不让新策略偏离旧策略太远的前提下，尽可能地提升策略。\",\"让我们把它拆解开来看：\",\"策略比例 (Probability Ratio) r_t(θ)： 这个值的计算方法是：r_t = π_θ(a|s) / π_θ_old(a|s)。\",\"π_θ(a|s) 是新策略（正在优化的策略）在状态 s 下选择动作 a 的概率。\",\"π_θ_old(a|s) 是旧策略（本次更新开始前的策略）的概率。\",\"如果 r_t > 1，说明这个动作在新策略下变得更可能被选中了。\",\"如果 r_t < 1，说明这个动作在新策略下变得更不可能被选中了。\",\"截断 (Clipping)： PPO设置了一个超参数 ε (epsilon，通常是0.1或0.2)。然后，它将策略比例 r_t 限制在一个安全区间内： [1 - ε, 1 + ε]。\",\"这意味着，即使某个动作的“优势”（Advantage）非常高，PPO也不允许新策略将选中该动作的概率无限拔高。最多只能提升到旧策略的 1 + ε 倍。\",\"反之，即使一个动作的优势非常差，PPO也不允许新策略过度惩罚它，最多将概率降低到旧策略的 1 - ε 倍。\",\"这个“截断”机制如何防止模式坍塌？\",\"想象一下，Actor（演员）偶然发现了一个“还不错”的动作，这个动作带来了比预期要好得多的奖励（即高优势）。\",\"没有PPO的普通策略梯度：会像打了兴奋剂一样，疯狂地增加这个动作被选中的概率，可能在一次更新后，策略就从“50%概率选A”变成了“99%概率选A”。策略迅速收敛，探索性急剧下降，这就是模式坍塌。它再也没有机会去发现可能比A更好的动作B了。\",\"有PPO：当新策略试图将动作A的概率大幅提升时，r_t 会迅速增长并超过 1 + ε。此时，截断机制会介入，将r_t强行“按回”到 1 + ε。这意味着，本次更新对这个动作的奖励是有上限的。策略只能从“50%概率选A”温和地变成，比如说，“60%概率选A”。\",\"结论：PPO通过限制单次更新的步长，强制策略进行“小步慢跑”式的优化，而不是“百米冲刺”。这种温和的更新方式，使得策略在更长的时间内保持其随机性和探索性，大大降低了过早陷入局部最优（模式坍塌）的风险。\"]},\"107\":{\"h\":\"2. 优势函数 (Advantage Function) 的使用\",\"t\":[\"PPO作为一种Actor-Critic算法，它不只是看奖励的绝对值，而是使用优势函数 A(s, a) 来指导策略更新。\",\"A(s, a) = Q(s, a) - V(s)\",\"它衡量的不是“动作a好不好”，而是“在状态s下，动作a比平均水平好多少”。\",\"这提供了一个更稳定、方差更低的基线。它避免了智能体仅仅因为某个状态本身价值很高，就错误地加强在该状态下采取的所有动作。通过关注“超出预期的好坏”，优势函数为策略更新提供了更精确的信号，这有助于引导Actor进行更有效的探索，而不是简单地固化任何能带来正奖励的行为。\"]},\"108\":{\"h\":\"总结：PPO与模式坍塌的关系\",\"t\":[\"我们可以用一个比喻来理解：\",\"一个激进的登山者（普通策略梯度），一旦发现一条看起来向上的路，就会不顾一切地冲上去，结果很可能被困在一个小山峰上（局部最优），错过了通往主峰（全局最优）的真正路径。\",\"一个谨慎的登山者（PPO），他每次只向上走一小段固定的距离（由ε限制）。即使他发现了一条看似很好的路，他也会小步前进，并不断地重新评估周围的环境。这种谨慎的策略让他不容易被困住，有更多的机会发现并转向那条通往真正顶峰的道路。\",\"因此，PPO通过其核心的截断机制来维持训练的稳定性，而这种稳定性又间接地保留了策略的探索性，从而成为了对抗策略“模式坍塌”的一道坚固防线。在实际应用中，PPO的目标函数里通常还会显式地加入熵正则化项，双管齐下，进一步鼓励探索，让策略更加稳健。\"]},\"109\":{\"h\":\"mamba算法\",\"t\":[\"好的，我们来详细深入地讲解一下 Mamba 算法及其背后的数学公式。\",\"Mamba 是一种新型的状态空间模型（State Space Model, SSM），它被设计用来处理长序列数据（如语言、音频、基因组等）。它的核心创新在于让模型的关键参数成为输入的函数，从而实现了高效的计算和强大的性能，尤其在语言建模任务上，挑战了 Transformer 的统治地位。\",\"为了更好地理解 Mamba，我们需要分层拆解它的核心组件和数学原理。\"]},\"110\":{\"h\":\"1. 基础：状态空间模型 (SSM)\",\"t\":[\"Mamba 建立在连续时间状态空间模型之上，这些模型通常用于控制系统、信号处理等领域。它们将一个一维的输入信号 u(t) 通过一个隐藏状态 x(t) 映射为一个一维的输出信号 y(t)。这个过程由两个方程描述：\",\"状态方程 (State Equation): 描述了隐藏状态 x(t) 如何随时间演变。dtd​x(t)=Ax(t)+Bu(t)\",\"输出方程 (Output Equation): 描述了如何从当前状态生成输出。y(t)=Cx(t)+Du(t)\",\"其中：\",\"x(t)∈RN 是 N 维的隐藏状态。\",\"u(t)∈R 是标量输入。\",\"y(t)∈R 是标量输出。\",\"A∈RN×N 是状态矩阵，控制系统动力学的演变（例如，如何保留或忘记信息）。\",\"B∈RN×1 是输入矩阵，控制输入如何影响状态。\",\"C∈R1×N 是输出矩阵，控制状态如何贡献到输出。\",\"D∈R 是前馈矩阵，直接将输入连接到输出（通常可忽略或设为 0）。\",\"直观理解：你可以将 SSM 看作一个具有内部记忆（状态 x(t)）的黑盒子。它持续地读取输入，根据输入更新自己的记忆，并基于当前的记忆产生输出。\"]},\"111\":{\"h\":\"2. 离散化：从连续时间到离散序列\",\"t\":[\"上述模型是连续时间的，但我们的数据（如文本）是离散的序列。因此，我们需要将模型离散化。这是通过一个固定的步长 Δ 来完成的，它将连续参数 (A,B) 转换为离散参数 (A,B)。一个常见的方法是零阶保持（ZOH）方法：\",\"A=exp(ΔA)B=(ΔA)−1(exp(ΔA)−I)⋅ΔB\",\"简化版本：通常使用一个更简单的近似，其中 B 的计算会被简化。\",\"离散化后，我们得到适用于离散序列 uk​ 的递归方程：\",\"xk​=Axk−1​+Buk​yk​=Cxk​\",\"其中 k 是时间步索引。这个形式看起来就像一个线性循环神经网络（RNN）。对于每个时间步，它都需要计算上一个状态 xk−1​，因此无法并行训练。\"]},\"112\":{\"h\":\"3. 卷积模式：并行训练\",\"t\":[\"虽然离散 SSM 可以像 RNN 一样递归计算，但研究者发现它也可以被重新表述为一个卷积操作。\",\"离散 SSM 的输出 yk​ 可以展开为输入 uk​ 的无限冲激响应：\",\"yk​=CAkBu0​+CAk−1Bu1​+...+CBuk​\",\"我们可以定义一个核（Kernel）K∈RL，其中 L 是序列长度：\",\"K=(CB,CAB,CA2B,...,CAL−1B)\",\"那么，整个输出序列 y 就是输入序列 u 与这个核 K 的卷积：\",\"y=u∗K\",\"优势：卷积是高度可并行的操作，可以利用 GPU 进行高效计算。这意味着在训练时，我们可以使用卷积模式并行处理整个序列，极大地加快了训练速度。\",\"劣势：卷积核 K 是固定的。一旦 (A,B,C) 参数确定，模型处理所有输入的方式就是固定的。这与 Transformer 中动态的注意力机制形成对比。\"]},\"113\":{\"h\":\"4. Mamba 的核心创新：选择性 SSM\",\"t\":[\"之前的工作（如 S4 模型）使用静态参数：(A,B,C,Δ) 对于所有输入都是固定的、学习好的参数。这意味着模型以相同的方式处理序列中的所有信息，无法根据内容动态地选择保留或忽略信息。\",\"Mamba 的关键突破是引入了选择性（Selectivity） 或输入依赖（Input-dependent） 的参数。简单来说，让 B,C,Δ 成为输入 uk​ 的函数。\",\"sk​=Linear(uk​)Bk​=LinearB​(sk​)Ck​=LinearC​(sk​)Δk​=Softplus(LinearΔ​(sk​))\",\"为什么这如此重要？\",\"内容感知：模型现在可以“阅读”输入，并即时决定如何与之交互。 \",\"B 控制输入如何进入状态。选择性 B 让模型决定是否要将当前输入信息纳入记忆。\",\"C 控制状态如何影响输出。选择性 C 让模型决定此刻应该从记忆中回忆什么信息来产生输出。\",\"Δ 控制状态更新的节奏。选择性 Δ 让模型根据输入调整其内部时钟（例如，遇到重要词时“思考”更久，更新状态更慢）。\",\"解决 SSM 的痛点：传统的 SSM 在需要上下文依赖推理的任务（如复制、检索）上表现很差，因为它无法忽略无关信息（“淹没”在历史中）。选择性机制让模型可以像注意力一样，忽略不相关的历史信息，聚焦于关键信息。\"]},\"114\":{\"h\":\"5. 计算挑战与高效算法\",\"t\":[\"引入选择性后，参数 (Ak​,Bk​) 在每个时间步都不同。这意味着：\",\"卷积核不再固定，无法再使用全局卷积进行并行计算。\",\"递归计算是唯一选择，但简单的 for-loop 递归在 GPU 上非常慢。\",\"Mamba 通过设计一种硬件感知（Hardware-aware） 的高效并行扫描算法解决了这个问题。该算法通过巧妙地将计算重组，利用 GPU 的并行内存层次（SRAM vs HBM）来最小化 IO 操作，从而在保持递归本质的同时实现了高效的并行化。\",\"总结一下 Mamba 的工作流：\",\"对于一个输入序列，模型首先通过线性投影为每个时间步生成对应的 (Bk​,Ck​,Δk​)。\",\"使用 Δk​ 对 A 和 Bk​ 进行离散化，得到 (Ak​,Bk​)。 (A 仍然是静态参数)。\",\"使用高效的并行扫描算法，以循环方式计算整个序列的隐藏状态 xk​。\",\"使用 yk​=Ck​xk​ 计算输出。\"]},\"115\":{\"h\":\"总结与意义\",\"t\":[\"特性\",\"传统 SSM (如 S4)\",\"Mamba (选择性 SSM)\",\"参数\",\"静态\",\"动态 (输入依赖的 B,C,Δ)\",\"核心能力\",\"强大的序列压缩和表示\",\"上下文感知的信息选择\",\"训练模式\",\"卷积 (高效并行)\",\"高效并行扫描 (硬件感知算法)\",\"推理模式\",\"循环 (恒定内存/时间)\",\"循环 (恒定内存/时间)\",\"性能\",\"在长序列上表现好，但不擅长推理\",\"在语言建模、推理等任务上媲美甚至超越 Transformer\",\"Mamba 的意义在于：它成功地将 Transformer 最核心的内容感知能力（注意力机制）与 SSM 的高效长序列处理能力（循环结构）结合在了一起。它提供了线性复杂度的序列建模（训练时 O(L)，推理时 O(1) 状态），打破了 Transformer 二次复杂度的瓶颈，为处理极长序列（如百万长度级别的上下文）开辟了新的道路。\"]},\"116\":{\"h\":\"multi head attention\"},\"117\":{\"h\":\"多头注意力（Multi-Head Attention）数学详解\",\"t\":[\"多头注意力是 Transformer 的核心机制，它通过并行执行多个独立的注意力头（attention heads）来捕捉输入的不同表示子空间。以下是完整的数学推导（所有公式用 $ 包裹）：\"]},\"118\":{\"h\":\"\",\"t\":[\"输入序列矩阵：\",\"X∈Rn×dmodel​\",\"其中：\",\"n：序列长度\",\"dmodel​：模型维度（如 512）\"]},\"119\":{\"h\":\"\",\"t\":[\"对每个头 i∈{1,2,…,h}，使用独立的权重矩阵进行投影：\",\"Qi​Ki​Vi​​=X⋅WiQ​,=X⋅WiK​,=X⋅WiV​,​WiQ​∈Rdmodel​×dk​WiK​∈Rdmodel​×dk​WiV​∈Rdmodel​×dv​​\",\"参数说明：\",\"h：头数（如 8）\",\"dk​=dv​=hdmodel​​：每个头的维度（如 64）\",\"投影后维度：Qi​,Ki​∈Rn×dk​，Vi​∈Rn×dv​\"]},\"120\":{\"h\":\"\",\"t\":[\"对每个头 i 独立计算缩放点积注意力：\",\"headi​=Attention(Qi​,Ki​,Vi​)=softmax(dk​​Qi​KiT​​)Vi​\",\"其中：\",\"dk​​Qi​KiT​​∈Rn×n：缩放后的注意力分数矩阵\",\"输出维度：headi​∈Rn×dv​\"]},\"121\":{\"h\":\"\",\"t\":[\"将所有头的输出在特征维度拼接：\",\"MultiHead(Q,K,V)=Concat(head1​,head2​,…,headh​)\",\"拼接后维度：Concat(⋯)∈Rn×(h⋅dv​)=Rn×dmodel​\"]},\"122\":{\"h\":\"\",\"t\":[\"将拼接结果通过可学习权重矩阵 WO 投影回模型维度：\",\"Output=Concat(head1​,…,headh​)⋅WO,WO∈Rdmodel​×dmodel​\",\"最终输出维度：Rn×dmodel​\"]},\"123\":{\"h\":\"\",\"t\":[\"​MultiHead(X)=Concat(head1​,…,headh​)⋅WOwhereheadi​=softmax(dk​​(XWiQ​)(XWiK​)T​)(XWiV​)with⎩⎨⎧​dk​=dv​=hdmodel​​WiQ​,WiK​∈Rdmodel​×dk​WiV​∈Rdmodel​×dv​WO∈Rdmodel​×dmodel​​​​\"]},\"124\":{\"h\":\"\",\"t\":[\"输入 X: [n, d_model] │ ├─ Head 1 ──┐ │ Q1 = X·W1^Q [n, d_k] │ │ K1 = X·W1^K [n, d_k] → Attn1 [n, n] → head1 [n, d_v] │ V1 = X·W1^V [n, d_v] │ │ ├─ Head 2 ──┤ │ │ ... │ ├→ Concat [n, h·d_v] = [n, d_model] ├─ Head h ──┤ │ │ │ │ └──────────┘ │ │ Output = Concat·W^O [n, d_model] ←────────┘\"]},\"125\":{\"h\":\"\",\"t\":[\"子空间分解 每个头学习不同的投影：\",\"WiQ​,WiK​,WiV​相互独立\",\"使模型关注不同方面的信息（如语法/语义/位置）。\",\"计算效率 虽然头数 h 增加，但单头维度 dk​ 减小：\",\"单头计算量=O(n2dk​)=O(n2hdmodel​​)\",\"总计算量 O(n2dmodel​) 与单头相同。\",\"表达能力增强 输出是多个子空间的非线性组合：\",\"Output=f(i=1∑h​headi​⋅WiO​)\",\"（WO 隐含分解为子矩阵 WiO​）\"]},\"126\":{\"h\":\"\",\"t\":[\"设 dmodel​=4, h=2, 则 dk​=dv​=2 输入 X=[1−0.5​0.51​−10.3​2−2​]\",\"头1计算：\",\"W1Q​=​0.1−0.21.00.5​0.40.3−0.50.2​​,Q1​=XW1Q​=[0.85−0.29​−0.050.74​]\",\"（类似计算 K1​, V1​ 后求 head1​）\",\"头2计算：\",\"W2Q​=​−0.31.1−0.40.7​0.20.60.8−0.1​​,Q2​=XW2Q​=[2.15−1.32​0.651.02​]\",\"拼接与输出：\",\"Concat=[head1(1)​​head1(2)​​head2(1)​​head2(2)​​]\",\"WO=​0.50.1−0.30.9​−0.20.80.7−0.5​1.1−0.40.20.3​0.30.61.0−0.8​​,Output=Concat⋅WO\"]},\"127\":{\"h\":\"\",\"t\":[\"特性\",\"单头注意力\",\"多头注意力\",\"参数量\",\"3dmodel2​\",\"3dmodel2​ (相同)\",\"计算复杂度\",\"O(n2dmodel​)\",\"O(n2dmodel​)\",\"表达能力\",\"单一表示空间\",\"h 个正交子空间\",\"并行性\",\"低\",\"高（头间完全并行）\"]},\"128\":{\"h\":\"\",\"t\":[\"多样性：不同头关注不同模式（如局部依赖 vs 全局依赖）\",\"headi​学习独立的关系表示\",\"维度分解：将高维空间分解为低维子空间，降低学习难度\",\"Rdmodel​→⊕i=1h​Rdk​\",\"残差连接：多头输出通过残差连接传递到前馈网络，保留原始信息：\",\"LayerOutput=LayerNorm(X+MultiHead(X))\",\"多头注意力通过这种\\\"分治策略\\\"，显著提升了模型捕捉复杂依赖关系的能力，成为 Transformer 架构的核心创新。\"]},\"129\":{\"h\":\"self-attention 自注意力\",\"t\":[\"太好了！这是一个核心问题。Self-Attention（自注意力） 是注意力机制的一种特殊形式，也是Transformer模型成功的基石。\",\"简单来说：\",\"普通Attention：用于连接两个不同的序列（例如，源语言句子和目标语言句子）。Query来自一个序列，Key和Value来自另一个序列。\",\"Self-Attention：只在同一个序列内部进行操作。Query, Key, Value全部来自于同一个输入序列。\"]},\"130\":{\"h\":\"核心思想：序列内部的“自我审视”\",\"t\":[\"Self-Attention 让序列中的每一个元素（例如句子中的每一个词）都去“审视”或“关注”序列中的所有其他元素（包括它自己），从而计算出一种新的、融入了全局上下文信息的元素表示。\",\"它的核心目的是：捕捉一个序列内部的内部结构、语义关联和语法依赖关系。\"]},\"131\":{\"h\":\"一个生动的例子：解决指代问题\",\"t\":[\"考虑这个句子： “The animal didn't cross the street because it was too tired.”\",\"（“那个动物没有过马路，因为它太累了。”）\",\"这里的“it”（它）指的是什么？是“street”（马路）还是“animal”（动物）？这对人类来说很简单，但对模型来说是个挑战。\",\"Self-Attention 如何工作：\",\"当模型处理到“it”这个词时，Self-Attention 机制允许“it”这个词的表示（Query）去与句子中的每一个词（Key）计算相关性。\",\"通过计算，它会发现“it”与“animal”和“tired”的关联度（注意力分数）非常高。\",\"因此，在生成“it”的新表示（上下文向量）时，它会大量地融入“animal”和“tired”的信息，而几乎忽略“street”的信息。\",\"这样，模型就能清晰地知道“it”指的是“animal”，而不是“street”。\",\"这个过程就像是序列中的每个词都在和其他所有词“开会交流”，最终每个词都带着对整句话的理解形成一个新的、更丰富的表示。\"]},\"132\":{\"h\":\"工作流程（与普通Attention相同但来源不同）\",\"t\":[\"它的计算公式与普通注意力完全一样，但Q, K, V的来源变了：\",\"SelfAttention(X) = Attention(Q, K, V) = softmax((QK^T) / √d_k) V\",\"关键区别在于：\",\"普通Attention: \",\"Q = 目标序列的表示\",\"K, V = 源序列的表示\",\"Self-Attention: \",\"Q = K = V =同一个输入序列X的线性变换\",\"Q = X * W_Q\",\"K = X * W_K\",\"V = X * W_V\",\"W_Q, W_K, W_V 是可学习的参数矩阵，它们将相同的输入X投影到不同的空间，以便扮演不同的角色。\"]},\"133\":{\"h\":\"为什么Self-Attention如此强大？\",\"t\":[\"强大的长距离依赖建模能力：\",\"传统RNN需要一步步顺序处理，距离较远的词之间的依赖关系容易被弱化（梯度消失/爆炸）。\",\"Self-Attention一步到位：无论两个词在序列中的距离有多远，它们之间的关联计算都是一步完成的。这使得它非常擅长捕捉长距离依赖关系。\",\"极高的并行化程度：\",\"RNN的计算是顺序的，无法并行。\",\"Self-Attention的计算可以完全并行化。因为序列中所有元素对的相似度计算（QK^T）是相互独立的，可以同时进行，这使得它在大规模硬件（如GPU）上效率极高。\",\"可解释性：\",\"通过可视化注意力权重，我们可以看到模型在处理一个词时，具体关注了序列中的哪些其他词。这为了解模型的工作机制提供了宝贵的视角。\"]},\"134\":{\"h\":\"Self-Attention在Transformer中的角色\",\"t\":[\"在Transformer中，Self-Attention有两种主要应用：\",\"编码器中的Self-Attention：\",\"让输入序列的每个词都能充分理解其所在句子的全局上下文信息，生成一个“上下文感知”的词表示。\",\"解码器中的Masked Self-Attention：\",\"为了防止在训练时“作弊”（看到未来的词），解码器的Self-Attention会被屏蔽（Masked）。这意味着在计算位置i的词的注意力时，它只能关注位置1到i的词，而不能关注到i+1及之后的词。\"]},\"135\":{\"h\":\"总结\",\"t\":[\"特性\",\"普通Attention (Encoder-Decoder Attention)\",\"Self-Attention\",\"应用场景\",\"连接两个不同的序列（如翻译）\",\"分析同一个序列的内部结构\",\"Q, K, V 来源\",\"Q 来自序列A，K, V 来自序列B\",\"Q, K, V 全部来自同一个序列X\",\"主要目的\",\"对齐（Alignment）\",\"表征学习（Representation Learning）\",\"总而言之，Self-Attention是一种让序列模型能够高效、并行地捕捉序列内部长距离依赖关系的机制，它是Transformer架构的核心，也是现代大语言模型理解语言上下文的基础。\"]},\"136\":{\"h\":\"self-attention详细计算方法\",\"t\":[\"好的，我们来详细拆解一下 Self-Attention 的计算过程。虽然它的公式和普通注意力一样，但因为它所有输入都来自同一序列，所以理解其计算流程更能体会其精髓。\",\"其最伟大的地方在于：它为序列中的每个词都计算出了一个融入了全局上下文的全新表示。\",\"计算过程可以分为以下几步，我们用一个非常简单的例子来说明：\",\"例句： \\\"Thinking Machines\\\" （两个词）目标：为每个词计算一个全新的向量表示。\"]},\"137\":{\"h\":\"第1步：从输入向量创建Query, Key, Value\",\"t\":[\"这是最关键的一步。我们有三个不同的权重矩阵：WQ, WK, WV。这些矩阵是模型通过训练学习到的。\",\"每个输入词向量（x1​ for \\\"Thinking\\\", x2​ for \\\"Machines\\\") 会分别与这三个矩阵相乘，从而生成三组向量： \",\"Query 向量：表示当前词作为“提问者”的身份。q1​=x1​⋅WQ, q2​=x2​⋅WQ\",\"Key 向量：表示当前词作为“被检索项”的身份。k1​=x1​⋅WK, k2​=x2​⋅WK\",\"Value 向量：表示当前词所包含的“实际信息”。v1​=x1​⋅WV, v2​=x2​⋅WV\",\"意义：这就好比，同一个词“Thinking” (x1​)，在扮演不同角色时（提问、被检索、提供信息），会呈现出不同的侧面（q1​, k1​, v1​）。\",\"为了更直观地理解整个流程，下图描绘了从输入词向量到最终输出的全过程，其中包含了Query、Key、Value的生成，以及注意力分数的计算和加权求和：\"]},\"138\":{\"h\":\"第2步：计算注意力分数\",\"t\":[\"现在，我们想知道这个词序列中不同词之间的关联程度。\",\"我们通过计算 Query 和 Key 的点积来衡量相关性。\",\"对于“Thinking” (x1​) 来说，它需要计算与序列中每个词（包括自己）的分数：\",\"分数_11 = q1​⋅k1T​ （\\\"Thinking\\\" 与 \\\"Thinking\\\" 的关联度）\",\"分数_12 = q1​⋅k2T​ （\\\"Thinking\\\" 与 \\\"Machines\\\" 的关联度）\",\"同样地，对于“Machines” (x2​)：\",\"分数_21 = q2​⋅k1T​\",\"分数_22 = q2​⋅k2T​\"]},\"139\":{\"h\":\"第3步：应用Softmax获得注意力权重\",\"t\":[\"将上一步得到的分数进行缩放（除以 dk​​，其中 dk​ 是 Key 向量的维度）以稳定梯度。\",\"然后，对缩放后的分数应用 Softmax 函数。Softmax 是按行操作的。\",\"对于“Thinking” (x1​) 的分数行 [分数_11, 分数_12]：\",\"权重_11 = softmax(分数_11 / dk​​)\",\"权重_12 = softmax(分数_12 / dk​​)\",\"结果：我们得到了一个概率分布，其和为1。这个分布代表了当编码“Thinking”这个词时，应该分配多少注意力给序列中的每个位置。\"]},\"140\":{\"h\":\"第4步：计算输出向量（上下文向量）\",\"t\":[\"这是最后一步，生成的新向量就是 Self-Attention 的输出。\",\"将上一步得到的每个权重，与对应的 Value 向量相乘，然后求和。\",\"对于“Thinking” (x1​) 的输出 z1​：z1​=权重_11⋅v1​+权重_12⋅v2​\",\"对于“Machines” (x2​) 的输出 z2​：z2​=权重_21⋅v1​+权重_22⋅v2​\",\"这就是 Self-Attention 的魔力所在！\",\"z1​ 不再是单纯的“Thinking”的原始表示 x1​，而是一个全新的向量。它融合了“Thinking”自身的信息 (v1​) 和根据其与“Machines”的关联度加权后的“Machines”的信息 (v2​)。\",\"同理，z2​ 也融合了全局信息。\"]},\"141\":{\"h\":\"总结与关键点\",\"t\":[\"并行计算：上述所有计算都可以通过矩阵运算并行完成，效率极高。这也是Transformer比RNN快得多的原因之一。\",\"角色分离：通过 WQ, WK, WV 三个矩阵，让同一个输入词在不同场景下扮演不同角色。\",\"输出本质：Self-Attention 的输出 Z 是一个向量序列 [z1​,z2​,...,zn​]。序列中的每一个向量都包含了序列中所有其他向量的信息，只是关注的程度（权重）不同。\",\"可视化：注意力权重矩阵（Softmax后的结果）通常可以被可视化，成为理解模型决策过程的窗口，让我们看到模型在处理一个词时到底在“看”句子的哪些部分。这就是所谓的模型可解释性。\",\"这个过程就是 Transformer 构建其强大理解能力的基石。每个词不再是孤立的，而是通过与序列中所有其他词的“交流”，获得了更深层次、更准确的表征。\"]},\"142\":{\"h\":\"单头 single head attention\",\"t\":[\"下面我将详细解释**单头注意力（Single-Head Attention）**的数学机制，包含所有关键公式的推导和说明。整个过程分为 5 个步骤：\"]},\"143\":{\"h\":\"\",\"t\":[\"假设输入序列有 n 个 token，每个 token 的嵌入向量维度为 dmodel​。输入矩阵表示为：\",\"X∈Rn×dmodel​=​x1​x2​⋮xn​​​\",\"其中 (xi​∈R1×dmodel​) 是第 (i) 个 token 的嵌入向量。\"]},\"144\":{\"h\":\"\",\"t\":[\"通过三个权重矩阵 WQ,WK,WV，将输入投影到 Query、Key、Value 空间：\",\"QKV​=X⋅WQ,=X⋅WK,=X⋅WV,​WQ∈Rdmodel​×dk​WK∈Rdmodel​×dk​WV∈Rdmodel​×dv​​\",\"(Q)（Query矩阵）维度： n×dk​\",\"(K)（Key矩阵）维度： n×dk​\",\"(V)（Value矩阵）维度： n×dv​\",\"说明：在标准 Transformer 中，通常设 dk​=dv​。例如当 dmodel​=512 时，单头可能取 dk​=dv​=64。\"]},\"145\":{\"h\":\"\",\"t\":[\"计算 Query 和 Key 的点积，得到原始注意力分数矩阵：\",\"S=Q⋅KT∈Rn×n\",\"其中元素 Sij​=qi​⋅kjT​ 表示第 i 个 token 对第 j个 token 的相似度。\"]},\"146\":{\"h\":\"\"},\"147\":{\"h\":\"(a) ：\",\"t\":[\"Sscaled​=dk​​S​\"]},\"148\":{\"h\":\"(b) ：\",\"t\":[\"对每一行应用 softmax，得到注意力权重矩阵：\",\"A=softmax(Sscaled​)∈Rn×n\",\"具体计算为：\",\"Aij​=∑k=1n​exp(Sscaled,ik​)exp(Sscaled,ij​)​\",\"此时 A 满足 sumj=1n​Aij​=1（每行和为 1）。\"]},\"149\":{\"h\":\"\",\"t\":[\"用注意力权重 (A) 对 Value 矩阵加权求和，得到输出：\",\"Output=A⋅V∈Rn×dv​\",\"第 (i) 个 token 的输出向量为：\",\"outputi​=j=1∑n​Aij​⋅vj​\"]},\"150\":{\"h\":\"\",\"t\":[\"QKVSSscaled​AOutput​=X⋅WQ=X⋅WK=X⋅WV=Q⋅KT=dk​​S​=softmax(Sscaled​)=A⋅V​​\"]},\"151\":{\"h\":\"\",\"t\":[\"Query-Key 点积Sij​=qi​⋅kjT​=∥qi​∥∥kj​∥cosθij​ 夹角 thetaij​ 越小（相似度越高），分数越大。\",\"Softmax 的作用 将分数转化为概率分布，突出重要关系：\",\"加权输出 输出是 Value 向量的凸组合：(\\\\text{output}i = \\\\sum_j A v_j)，其中 (\\\\sum_j A_{ij} = 1)。\"]},\"152\":{\"h\":\"\",\"t\":[\"假设输入序列：(X=[x1​x2​​])（2 个 token），设 (dk​=dv​=2)：\",\"投影：\",\"Q=[12​30.5​],K=[0.51​23​],V=[01​42​]\",\"注意力分数：\",\"S=QKT=[(1)(0.5)+(3)(1)(2)(0.5)+(0.5)(1)​(1)(2)+(3)(3)(2)(2)+(0.5)(3)​]=[3.51.5​115.5​]\",\"缩放（设 (d_k=2)）：\",\"Sscaled​=2​S​=[2.471.06​7.783.89​]\",\"Softmax 归一化（按行）：\",\"A=[e2.47+e7.78e2.47​e1.06+e3.89e1.06​​e2.47+e7.78e7.78​e1.06+e3.89e3.89​​]≈[0.0060.047​0.9940.953​]\",\"输出：\",\"Output=A⋅V=[0.006⋅0+0.994⋅10.047⋅0+0.953⋅1​0.006⋅4+0.994⋅20.047⋅4+0.953⋅2​]≈[0.9940.953​2.0122.094​]\",\"输出 1 接近 (v_2)（因权重 0.994）\",\"输出 2 接近 (v_2)（因权重 0.953）\"]},\"153\":{\"h\":\"\",\"t\":[\"动态权重分配：(A_{ij}) 表示 token (i) 对 token (j) 的依赖强度。\",\"信息融合：输出是全局上下文感知的新表示。\",\"可导性：所有操作可微，支持端到端训练。\",\"计算复杂度：(O(n2dk​))，序列较长时需优化（如稀疏注意力）。\",\"总而言之的数学公式：\"]},\"154\":{\"h\":\"单头attention back propagation\",\"t\":[\"非常好的问题！理解注意力机制的反向传播是理解其如何学习的关键。注意力机制最优雅的地方在于它完全是可微的，这意味着我们可以通过标准的反向传播算法来学习其中的参数。\",\"我们再次回顾前向传播的步骤：\",\"分数（Scores）: S=QKT\",\"缩放与权重（Weights）: A=softmax(dk​​S​)\",\"输出/上下文向量（Output）: O=AV\",\"反向传播的目标是计算最终损失函数 L 关于输入矩阵 Q, K, V 的梯度（即 ∂Q∂L​, ∂K∂L​, ∂V∂L​），以便用梯度下降来更新生成 Q, K, V 的权重参数。\",\"我们从后往前（链式法则）来推导：\"]},\"155\":{\"h\":\"第1步：计算关于 的梯度\",\"t\":[\"这是最简单的一步。根据前向公式 O=AV，我们可以直接写出：\",\"∂V∂L​=AT∂O∂L​\",\"解释：因为 O 是 A 和 V 的矩阵乘法结果，关于 V 的梯度就是注意力权重矩阵 A 的转置乘以上游传回来的梯度 ∂O∂L​。这和我们熟悉的全连接层的反向传播规则一致。\"]},\"156\":{\"h\":\"第2步：计算关于 的梯度\",\"t\":[\"同样根据 O=AV，我们可以得到：\",\"∂A∂L​=∂O∂L​VT\",\"解释：上游梯度 ∂O∂L​ 与 V 的转置相乘，就得到了损失对注意力权重矩阵 A 的梯度。这个梯度告诉我们：“为了减小损失，每个注意力权重应该变大还是变小”。\"]},\"157\":{\"h\":\"第3步：计算关于 的梯度（这是最核心的一步）\",\"t\":[\"这一步是反向传播通过 Softmax 函数。这是一个标准操作，但非常重要。 前向过程是 A=softmax(Sscaled​)=softmax(S/dk​​)。\",\"假设 Sscaled​=S/dk​​，且 A=softmax(Sscaled​)。那么，对于 Softmax 函数的反向传播，有一个非常优雅的结果：\",\"如果有一个向量输入 x 和向量输出 y=softmax(x)，那么雅可比矩阵 ∂x∂y​ 是一个对称矩阵，其元素为：\",\"∂xj​∂yi​​=yi​(δij​−yj​)\",\"其中 δij​ 是克罗内克函数（当 i=j 时为1，否则为0）。\",\"这意味着，从上游传回来的梯度 ∂A∂L​ 在通过 Softmax 层时，会按以下方式转换：\",\"\\\\frac{\\\\partial L}{\\\\partial S_{\\\\text{scaled}}}} = A \\\\circ \\\\left( \\\\frac{\\\\partial L}{\\\\partial A} - \\\\text{sum}\\\\left( \\\\frac{\\\\partial L}{\\\\partial A} \\\\circ A, \\\\text{ axis=1} \\\\right) \\\\right) \",\"这里 ∘ 表示逐元素相乘（Hadamard积），sum(..., axis=1) 表示对每一行进行求和。\",\"直观理解：Softmax 的梯度计算会考虑所有输出元素之间的相互作用。调整一个分数 Sj​，不仅会影响它对应的注意力权重 Aj​，还会影响所有其他权重 Ai=j​（因为 Softmax 要求所有输出之和为1）。这个计算确保了梯度能正确地分配给所有分数。\",\"最后，别忘了我们之前对分数进行了缩放：Sscaled​=S/dk​​。所以还需要继续反向传播通过这个缩放操作：\",\"\\\\frac{\\\\partial L}{\\\\partial S} = \\\\frac{\\\\partial L}{\\\\partial S_{\\\\text{scaled}}}} \\\\cdot \\\\frac{1}{\\\\sqrt{d_k}} \"]},\"158\":{\"h\":\"第4步：计算关于 和 的梯度\",\"t\":[\"现在我们有了关于分数矩阵 S 的梯度 ∂S∂L​。回想前向过程 S=QKT。\",\"关于 Q 的梯度:\",\"∂Q∂L​=∂S∂L​⋅K\",\"这可以理解为：为了计算损失对某个查询向量 q 的梯度，我们需要看这个 q 与所有键向量 k 计算出的分数受到了多大影响。\",\"关于 K 的梯度:\",\"∂K∂L​=(∂S∂L​)T⋅Q\",\"同理，为了计算损失对某个键向量 k 的梯度，我们需要看所有查询向量 q 与这个 k 计算出的分数受到了多大影响。\"]},\"159\":{\"h\":\"总结与直观理解\",\"t\":[\"整个反向传播过程可以总结为以下流程图，展示了梯度是如何从输出 O 一路回溯到输入 Q, K, V 的：\",\"这个过程如何让模型学习？\",\"梯度从输出 O（上下文向量）传回：O 被用来计算损失（例如，预测下一个词的对数概率）。如果预测错了，损失函数就会产生一个梯度，这个梯度意味着“这个上下文向量 O 应该更像正确答案才对”。\",\"梯度指导权重 A 的调整：这个梯度接着传回到注意力权重矩阵 A。它告诉模型：“你在这个时间步关注了错误的信息！你应该更多地关注输入序列中的 那个 词，而不是 这个 词。”\",\"梯度最终调整 Q, K, V 的生成：A 的梯度又进一步传回给 Q, K, V。由于 Q, K, V 本身是由可学习的权重参数（比如线性变换层）生成的，这些梯度会继续反向传播回去，调整那些生成 Q, K, V 的权重参数。\",\"结果：通过无数次这样的调整，模型学会了： \",\"如何生成更好的 Q, K, V 表示。\",\"对于解码器的某个特定位置（某个 Query），应该如何在编码器的所有输出（Keys）上分配注意力（A），才能生成最有助于正确预测的上下文向量（O）。\",\"本质上，反向传播使注意力机制能够自动学习并优化“在哪里关注”以及“关注多少”的策略，而无需任何人工标注的注意力指导。这就是它如此强大和高效的原因。\"]},\"160\":{\"h\":\"Transformer it self\",\"t\":[\"![[Pasted image 20240414153828.png]] Transformer 基于现有的序列-序列模型，使用 「encoder-decoder」 架构。在 encoder-decoder 架构中，编码器（encoder）将输入序列 (x1​,…,xn​) 转换为一个连续的表达 z=(z1​,…,zn​) ，然后解码器再基于该表达生成输出序列 (y1​,…,ym​)。\"]},\"161\":{\"h\":\"\",\"t\":[\"由 6 层组成，每一层包括两个子层：「第一层」 是 multi-head self-attention 层, 「第二层」 是一个简单的全连接前馈网络。在每个子层后，都接了一个 「残差连接以及归一化」 ，即每个子层的输出为 LayerNorm (x+ Sublayer (x)). 为了方便残差连接，模型中的所有子层，包括 embedding 层（初始词嵌入），输出向量维度均为dmodel​=512.\"]},\"162\":{\"h\":\"\",\"t\":[\"同样由 6 层组成，每一层包括三个子层：「第一层」 是 masked multi-head self-attention 层，注意其输入仅包含 「当前位置之前的词语信息」，这样设计的目的是解码器是按顺序解码的，其当前输出只能基于已输出的部分。「第二层」 是 multi-head self-attention 层，其输入包含编码器的输出信息（矩阵 K 和矩阵 V ），「第三层」 是全连接前馈网络。每个子层后同样加入了 「残差连接和归一化」。下图给出了编码器和解码器的内部结构，注意前馈神经网络对于序列每个位置的独立性。\"]},\"163\":{\"h\":\"\",\"t\":[\"解码器的输出被输入到一个线性层中，转化为一个超长向量（词典长度），再输入到 softmax 层中转化为概率，最后运用适当策略（如贪婪搜索或束搜索）选择输出的词语，注意 「一次只输出一个词语」 。已输出的序列会作为解码器的输入。下图给出了贪婪搜索策略下的运行流程。 ![[Pasted image 20240414165245.png]]\"]},\"164\":{\"h\":\"RMSNorm为什么有效\",\"t\":[\"好的，这是一个非常深入且重要的问题。RMSNorm（Root Mean Square Normalization）是深度学习模型中一种重要的归一化技术，我们可以将其理解为对更著名的LayerNorm（层归一化） 的一个简化且高效的改进版本。\",\"它由清华大学团队在2019年的论文《Root Mean Square Layer Normalization》中提出，并已被广泛应用于诸如LLaMA、GPT-NeoX等众多现代大语言模型中。\"]},\"165\":{\"h\":\"一、RMSNorm 是什么？\"},\"166\":{\"h\":\"1. 核心思想\",\"t\":[\"RMSNorm的核心思想非常直观：只对输入向量的均方根（Root Mean Square, RMS）进行缩放，而不像LayerNorm那样先进行中心化（减去均值）再进行缩放。\"]},\"167\":{\"h\":\"2. 数学公式\",\"t\":[\"给定一个输入向量 x，LayerNorm的计算是：\",\"LayerNorm(x)=σx−μ​⋅γ+β\",\"其中：\",\"μ 是 x 的均值。\",\"σ 是 x 的标准差。\",\"γ 和 β 是可学习的缩放和偏移参数。\",\"而RMSNorm则将其简化为：\",\"RMSNorm(x)=RMS(x)x​⋅γ\",\"其中：\",\"RMS(x)=n1​∑i=1n​xi2​​，即均方根值。它衡量的是向量大小的整体水平。\",\"γ 是可学习的缩放参数（通常省略了偏移参数 β）。\",\"简单来说，RMSNorm不再计算均值 μ 并将数据中心化到0，而是直接使用均方根值来对数据进行缩放，使其重新调整到一个稳定的尺度上。\"]},\"168\":{\"h\":\"二、为什么有效？（LayerNorm的痛点与RMSNorm的解决方案）\",\"t\":[\"要理解为什么有效，我们需要先看LayerNorm做了什么，以及RMSNorm如何对其优化。\"]},\"169\":{\"h\":\"1. 重新审视LayerNorm的目标\",\"t\":[\"归一化的根本目的是控制数据分布的尺度和稳定性，从而缓解训练过程中的内部协变量偏移（Internal Covariate Shift） 问题，使梯度更加稳定，加快训练速度。\",\"LayerNorm通过两个步骤实现这一目标：\",\"中心化（Centering）：减去均值 μ。这会将数据分布的中心移动到0。\",\"缩放（Scaling）：除以标准差 σ。这会将数据的尺度（方差）标准化为1。\"]},\"170\":{\"h\":\"2. RMSNorm的洞察：中心化可能不是必须的！\",\"t\":[\"论文作者通过实验和理论分析发现：\",\"LayerNorm的成功主要来自于缩放操作（除以 σ），而非中心化操作（减去 μ）。\",\"减去均值的中心化操作在计算上是有开销的，它需要先计算均值，然后再从每个元素中减去它。\",\"对于具有ReLU这类激活函数的网络（ReLU会将负值置零），分布本身就已经不是零中心的了。强行中心化到0的意义可能被后续的激活函数削弱。\"]},\"171\":{\"h\":\"3. RMSNorm的有效性原理\",\"t\":[\"保持了缩放不变性（Scale Invariance）：这是归一化技术起效的关键。RMSNorm通过除以RMS值，同样能够将输入向量缩放到一个单位尺度附近，从而稳定了层的输入分布，使得网络对参数的初始化和学习率不那么敏感。这带来了稳定的梯度和更快的训练速度。\",\"计算效率更高：\",\"计算量更小：RMSNorm省去了计算均值 μ 和方差 σ2（方差计算也需要均值）的步骤。\",\"根据论文，RMSNorm比LayerNorm减少了约7%～64%的计算时间。在大规模模型中，这点微小的效率提升累积起来是非常可观的。\",\"性能不相上下甚至更好：\",\"论文在多个任务（语言模型、图像分类、机器翻译）上进行了实验，发现RMSNorm的表现与LayerNorm相当甚至更好。\",\"这表明，减去均值的中心化操作所提供的收益，在很多情况下可以被省略，而不会损害模型的表达能力。简化后的操作反而可能让模型更专注于学习真正重要的模式。\"]},\"172\":{\"h\":\"三、总结与对比\",\"t\":[\"特性\",\"LayerNorm\",\"RMSNorm\",\"核心操作\",\"(x−μ)/σ\",\"x/RMS(x)\",\"是否中心化\",\"是\",\"否\",\"计算开销\",\"较高（需算 μ 和 σ）\",\"较低（只需算RMS）\",\"可学习参数\",\"γ, β (缩放和偏移)\",\"γ (仅缩放)\",\"效果\",\"深度学习的基础，非常有效\",\"效果相当，有时更优\",\"流行度\",\"Transformer、BERT等经典模型\",\"LLaMA、GPT-NeoX等新锐模型\"]},\"173\":{\"h\":\"为什么现在这么流行？\",\"t\":[\"RMSNorm的流行与大规模语言模型的发展紧密相关：\",\"效率至上：训练一个千亿参数的模型成本极高，任何能节省计算开销（即使是几个百分点）而又不损失性能的方法都极具吸引力。\",\"实践验证：像LLaMA这样的顶级模型选择了RMSNorm，并用其卓越的性能证明了这种简化归一化的有效性，从而带动了整个业界的采纳。\",\"总而言之，RMSNorm的有效性在于它抓住了归一化问题的关键——缩放（Scaling），并巧妙地移除了可能非必要的中心化（Centering）步骤，从而在保持甚至提升模型表现的同时，实现了更高的计算效率。它是一种“如无必要，勿增实体”奥卡姆剃刀原则在深度学习中的完美体现。\"]},\"174\":{\"h\":\"layer normalization - batch normalization\",\"t\":[\"GPT answer:\",\"Batch Normalization和Layer Normalization都是深度学习中的一种技术，用来防止梯度消失或者梯度爆炸，从而让神经网络更好地学习。它们的主要区别在于归一化的维度不同。\",\"Batch Normalization，简称BN，是在每一层的激活函数之前，对每一批样本的输入进行归一化处理。其原理是通过减去均值并除以标准差来做归一化，然后再引入两个可训练的参数，分别对应缩放因子和偏移量，来弥补归一化过程中损失的模型表达能力。BN的主要目的是为了解决深度神经网络训练过程中的内部协变量偏移问题，使得模型训练更加稳定，收敛更快，同时可以使用更大的学习率，有正则化效果，一定程度上防止过拟合。\",\"而Layer Normalization，简称LN，其主要思想是在每个样本中，对所有神经元的输出进行归一化。与BN不同的是，LN是对单个样本进行归一化，不受batch size的影响，因此在训练和测试阶段的行为是一致的。LN更多的应用在自然语言处理领域，尤其是在处理序列化数据时，如RNN（循环神经网络）和Transformer等模型中。\",\"总的来说，BN和LN的使用取决于数据和任务的特性。BN更适合于图像类的任务，因为图像在batch内的统计特性较为一致，而LN由于其独立于batch size，更适合处理序列化的任务，以及在小batch size的情况下也能很好的工作。\",\"Batch Normalization Batch Normalization的过程可以分为以下几个步骤：\",\"计算均值和方差：首先，我们需要对输入的mini-batch进行处理。假设一个mini-batch包含m个样本，对于每个神经元，我们计算这m个样本的均值和方差。\",\"归一化：对于每个神经元，我们用每个样本的值减去该神经元的均值，然后除以其标准差，得到归一化的输出。这样做的目的是消除不同特征值范围对梯度下降优化算法的影响，使得所有特征值都在相同的数值范围内。\",\"缩放和平移：虽然归一化有助于神经网络的训练，但有时我们可能不希望归一化的数据严格位于0均值和单位标准差。因此，Batch Normalization引入了两个可学习的参数，一个是缩放因子γ，另一个是平移因子β。这两个参数的维度与神经元的数量相同，它们的值通过反向传播和梯度下降来学习。具体的操作是，对于每个神经元，我们用归一化后的值乘以γ，再加上β，得到Batch Normalization的最终输出。\",\"反向传播：在训练过程中，我们需要计算损失函数对于每个参数的梯度，包括γ和β。Batch Normalization的反向传播比较复杂，需要应用链式法则和一些数学技巧。 总的来说，Batch Normalization的主要作用是在训练深度神经网络时，通过减少内部协变量偏移，提高模型的训练速度和性能。\",\"Layer Normalization Layer Normalization是一种常见的深度学习模型中的归一化方法，主要用于处理梯度消失或梯度爆炸问题。它通过归一化层内的每个样本来改善模型的训练，具体步骤如下：\",\"计算输入样本的均值和方差：对于每个样本，我们需要计算其在所有 特征维度 上的均值和方差。设输入样本 X ∈ R^(d1 x d2 x ... x dn)，则其均值μ和方差σ²分别为：\",\"μ = (1/H) * Σ(x)\",\"σ² = (1/H) * Σ((x-μ)²)\",\"其中，H 是特征维度的总数，Σ表示求和。\",\"对输入样本进行归一化：使用上一步计算得到的均值和方差，我们可以对输入样本进行归一化。归一化的输入样本 y 的计算公式为：\",\"y = (x - μ) / sqrt(σ² + ε)\",\"其中，ε 是一个很小的数，用于防止除零错误。\",\"重新调整和平移归一化的样本：归一化后的样本将被缩放和平移，以便能够学习更复杂的模式。调整后的输出 z 的计算公式为：\",\"z = γ*y + β\",\"其中，γ 和 β 是可学习的参数，分别用于缩放和平移。\",\"通过上述步骤，Layer Normalization能够在单个样本级别上进行归一化，使得模型在训练过程中能够更好地学习和适应不同的数据分布，提高模型的泛化性能。\",\"Zhihu answer:https://zhuanlan.zhihu.com/p/54171297https://zhuanlan.zhihu.com/p/54530247\",\"在文章的最后，我们指出BN并不适用于RNN等动态网络和batchsize较小的时候效果不好。Layer Normalization（LN）的提出有效的解决BN的这两个问题。LN和BN不同点是归一化的维度是互相垂直的，如图1所示。在图1中N表示样本轴, C表示通道轴, F是每个通道的特征数量。BN如右侧所示，它是取不同样本的同一个通道的特征做归一化；LN则是如左侧所示，它取的是同一个样本的不同通道做归一化。\",\"![[Pasted image 20240414163730.png]]\"]},\"175\":{\"h\":\"Reference\",\"t\":[\"[1] Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.\",\"[2] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.\"]},\"176\":{\"h\":\"旋转位置编码RoPE详解\",\"t\":[\"作为资深的机器学习专家，我将总结Rotary Position Embedding (RoPE) 这一机器学习问题，并仔细分析其算法优点。RoPE是一种用于Transformer模型的位置编码方法，旨在处理序列数据中的位置信息。它通过旋转矩阵编码位置，使得模型能够捕获相对位置关系，从而提升性能。以下将详细介绍RoPE的数学形式、算法原理，并对比其他嵌入方法突出其优越性。\"]},\"177\":{\"h\":\"1. Rotary Position Embedding (RoPE) 总结\",\"t\":[\"RoPE的核心思想是使用旋转操作将位置信息直接嵌入到查询（query）和键（key）向量中。对于序列中的每个位置 (m)，RoPE应用一个旋转矩阵 (R_{\\\\theta, m}) 到向量上，其中旋转角度依赖于位置 (m) 和维度相关的频率 (\\\\theta)。这种方法确保了注意力得分仅依赖于相对位置，从而实现了相对位置编码。\"]},\"178\":{\"h\":\"LaTeX 算式\",\"t\":[\"设查询向量 (q) 和键向量 (k) 的维度为 (d)。我们将 (d) 维空间分为 (d/2) 个组，每组对应一个旋转角度。对于每个组 (i)（其中 (i = 0, 1, \\\\dots, d/2-1)），频率参数 (\\\\theta_i) 定义为： [ \\\\theta_i = 10000^{-2i/d} ] 对于位置 (m)，旋转矩阵 (R_{\\\\theta, m}) 是一个块对角矩阵，每个块是一个2x2旋转矩阵： [ R_{\\\\theta, m} = \\\\begin{pmatrix} \\\\cos m\\\\theta & -\\\\sin m\\\\theta \\\\ \\\\sin m\\\\theta & \\\\cos m\\\\theta \\\\end{pmatrix} ] 在实际应用中，对于向量 (x)（可以是 (q) 或 (k)），RoPE编码函数为： [ f(x, m) = R_{\\\\theta, m} x ] 在Transformer的注意力机制中，查询和键 after applying RoPE become (f(q, m)) and (f(k, n))， respectively. 注意力得分计算为： [ \\\\langle f(q, m), f(k, n) \\\\rangle = \\\\langle R_{\\\\theta, m} q, R_{\\\\theta, n} k \\\\rangle = q^T R_{\\\\theta, m}^T R_{\\\\theta, n} k ] 由于旋转矩阵的正交性，有： [ R_{\\\\theta, m}^T R_{\\\\theta, n} = R_{\\\\theta, n-m} ] 因此，注意力得分只依赖于相对位置 (n-m)： [ \\\\langle f(q, m), f(k, n) \\\\rangle = q^T R_{\\\\theta, n-m} k ] 这证明了RoPE天然编码了相对位置信息。\"]},\"179\":{\"h\":\"2. 算法优点分析\",\"t\":[\"RoPE相较于其他位置编码方法（如绝对位置编码、正弦位置编码和学习的位置编码）具有显著优越性。以下从几个关键点进行分析：\"]},\"180\":{\"h\":\"2.1 相对位置编码的天然集成\",\"t\":[\"优点：RoPE直接通过旋转矩阵将相对位置信息融入注意力计算，使得模型能够自动捕获序列中元素之间的相对距离。这消除了需要显式设计相对位置偏置的需求（如Transformer-XL），简化了模型结构。\",\"对比：绝对位置编码（如BERT中的学习位置嵌入）只能提供固定位置信息，无法直接处理相对位置，导致模型在长序列或偏移序列上性能下降。正弦位置编码（原始Transformer）虽然提供一些相对位置线索，但不如RoPE直接和有效。\"]},\"181\":{\"h\":\"2.2 旋转不变性和数学优雅\",\"t\":[\"优点：RoPE基于复数旋转操作，具有坚实的数学基础（如群论和正交变换），确保了计算上的稳定性和效率。旋转不变性意味着内积仅依赖于相对位置，这增强了模型对序列长度变化的泛化能力。\",\"对比：学习的位置编码需要额外参数，可能引入过拟合风险，而RoPE是确定性的，无需学习，减少了参数数量并提高了计算效率。\"]},\"182\":{\"h\":\"2.3 长序列处理能力\",\"t\":[\"优点：RoPE的频率参数 (\\\\theta_i) 允许模型处理不同频率的位置信息，从而更好地适应长序列。旋转角度的设计使得模型能够捕获从短到长的各种依赖关系，在长序列任务（如语言建模或代码生成）中表现优异。\",\"对比：其他方法如正弦位置编码在长序列上可能出现频率衰减问题，导致位置信息丢失。学习的位置编码可能无法泛化到训练时未见过的序列长度。\"]},\"183\":{\"h\":\"2.4 计算效率和简洁性\",\"t\":[\"优点：RoPE的计算是并行的，可以通过向量化操作高效实现，与现代硬件（如GPU）兼容。它直接集成到注意力机制中，不增加额外的计算开销。\",\"对比：相对位置编码方法如Transformer-XL需要修改注意力计算公式，引入额外的相对位置偏置矩阵，增加了计算复杂性和内存使用。\"]},\"184\":{\"h\":\"3. 总结\",\"t\":[\"Rotary Position Embedding (RoPE) 是一种创新且高效的位置编码方法，通过旋转操作将相对位置信息嵌入到Transformer模型中。其数学优雅、计算高效、以及出色的长序列处理能力，使其在许多自然语言处理任务中成为首选。相比于传统方法，RoPE提供了更好的泛化性能和模型简洁性，是位置编码领域的重要进展。\"]},\"185\":{\"h\":\"知识蒸馏Knowledge Distillation\",\"t\":[\"知识蒸馏（Knowledge Distillation, KD）是一种模型压缩技术，旨在将大型复杂模型（教师模型）的知识“迁移”到小型轻量模型（学生模型）中，使小模型在保持较高性能的同时显著降低计算成本和存储需求。\"]},\"186\":{\"h\":\"\",\"t\":[\"“教师-学生”范式：\",\"教师模型（Teacher）：通常是庞大、高性能但计算昂贵的模型（如BERT、ResNet）。\",\"学生模型（Student）：小型、高效的模型（如MobileNet、TinyBERT）。\",\"目标：让学生模型模仿教师模型的输出行为，而非仅拟合原始数据标签。\",\"知识的定义：\",\"软标签（Soft Targets）：教师模型输出的概率分布（如分类任务中各类别的概率），包含更多信息（如类别间相似性）。 \",\"例：猫 vs 狗 vs 汽车，教师可能输出 [0.7, 0.29, 0.01]，暗示“猫和狗相似，与汽车差异大”。\",\"硬标签（Hard Labels）：原始训练数据的标签（如 [1, 0, 0]），仅包含正确类别信息。\"]},\"187\":{\"h\":\"\",\"t\":[\"训练教师模型： 在训练集上训练一个高性能的复杂模型。\",\"生成软标签： 用教师模型对训练数据预测，生成概率分布（软标签）。引入温度参数（Temperature, T） 软化概率分布：\",\"q_i = exp(z_i / T) / ∑_j exp(z_j / T)\",\"T > 1：概率分布更平滑，暴露类别间关系（核心知识）。\",\"T = 1：标准Softmax输出。\",\"训练学生模型： 学生模型同时学习：\",\"软目标损失：匹配教师模型的软标签（如KL散度损失）。\",\"硬目标损失：拟合原始数据的真实标签（如交叉熵损失）。\",\"总损失 = α * KL_loss(软标签, 学生输出) + (1-α) * CE_loss(真实标签, 学生输出)\",\"α：平衡两项损失的权重。\",\"推理阶段： 学生模型使用标准Softmax（T=1）进行预测。\"]},\"188\":{\"h\":\"\",\"t\":[\"知识泛化： 软标签揭示了类别间的隐含关系（如“猫狗相似性”），学生模型学到更鲁棒的特征。\",\"正则化作用： 软标签提供平滑的监督信号，避免学生模型过拟合硬标签中的噪声。\",\"优化难度降低： 教师的软标签为优化提供更丰富的梯度信息，帮助学生模型更快收敛。\"]},\"189\":{\"h\":\"\",\"t\":[\"场景\",\"说明\",\"模型压缩\",\"将BERT压缩为TinyBERT，推理速度提升10倍+\",\"模型部署\",\"在手机/嵌入式设备部署轻量模型（如蒸馏版ResNet）\",\"跨模态迁移\",\"教师（多模态模型）→ 学生（单模态模型）\",\"联邦学习\",\"教师整合全局知识，蒸馏给学生本地模型\"]},\"190\":{\"h\":\"\",\"t\":[\"特征蒸馏：让学生中间层特征图匹配教师的特征（如FitNets）。\",\"关系蒸馏：迁移样本间的关系（如RKD）。\",\"自蒸馏：同一模型同时作为教师和学生（如Deep Mutual Learning）。\",\"数据无关蒸馏：无需原始数据，生成合成数据蒸馏（如DAFL）。\"]},\"191\":{\"h\":\"\"},\"192\":{\"h\":\"\",\"t\":[\"知识蒸馏本质是知识的迁移与泛化，通过教师模型的“经验”指导学生模型，使其以更小的体量达到接近教师的性能，是平衡模型效率与效果的利器。尤其在边缘计算、实时系统中，它是不可或缺的技术之一。\",\"如果需要进一步探讨某个细节（如具体算法实现、最新论文方向），欢迎随时提问！ 😊\"]},\"193\":{\"h\":\"\",\"t\":[\"404 Not Found\"]},\"194\":{\"h\":\"Machine Learning\"},\"195\":{\"h\":\"Posts\"},\"196\":{\"h\":\"Posts\"},\"197\":{\"h\":\"Bayesian\"},\"198\":{\"h\":\"CV\"},\"199\":{\"h\":\"EBM\"},\"200\":{\"h\":\"Diffusers\"},\"201\":{\"h\":\"Mathematics\"},\"202\":{\"h\":\"Papers Read\"},\"203\":{\"h\":\"Reinforcement Learning\"},\"204\":{\"h\":\"DETR\"},\"205\":{\"h\":\"JEPA\"},\"206\":{\"h\":\"Actor Critic\"},\"207\":{\"h\":\"Attention\"},\"208\":{\"h\":\"Trick Layers\"},\"209\":{\"h\":\"Transformers\"},\"210\":{\"h\":\"Tricks\"}},\"dirtCount\":0,\"index\":[[\"欢迎随时提问\",{\"1\":{\"192\":1}}],[\"指导学生模型\",{\"1\":{\"192\":1}}],[\"指的是\",{\"1\":{\"131\":1}}],[\"指的是什么\",{\"1\":{\"131\":1}}],[\"蒸馏给学生本地模型\",{\"1\":{\"189\":1}}],[\"联邦学习\",{\"1\":{\"189\":1}}],[\"跨模态迁移\",{\"1\":{\"189\":1}}],[\"场景\",{\"1\":{\"189\":1}}],[\"帮助学生模型更快收敛\",{\"1\":{\"188\":1}}],[\"真实标签\",{\"1\":{\"187\":1}}],[\"真正的距离度量\",{\"1\":{\"52\":1}}],[\"拟合原始数据的真实标签\",{\"1\":{\"187\":1}}],[\"拟合效果反而更差\",{\"1\":{\"47\":1}}],[\"匹配教师模型的软标签\",{\"1\":{\"187\":1}}],[\"暴露类别间关系\",{\"1\":{\"187\":1}}],[\"∑\",{\"1\":{\"187\":1}}],[\"软目标损失\",{\"1\":{\"187\":1}}],[\"软化概率分布\",{\"1\":{\"187\":1}}],[\"软标签提供平滑的监督信号\",{\"1\":{\"188\":1}}],[\"软标签揭示了类别间的隐含关系\",{\"1\":{\"188\":1}}],[\"软标签\",{\"1\":{\"186\":1,\"187\":2}}],[\"硬目标损失\",{\"1\":{\"187\":1}}],[\"硬标签\",{\"1\":{\"186\":1}}],[\"硬件感知算法\",{\"1\":{\"115\":1}}],[\"暗示\",{\"1\":{\"186\":1}}],[\"汽车\",{\"1\":{\"186\":1}}],[\"狗\",{\"1\":{\"186\":1}}],[\"知识泛化\",{\"1\":{\"188\":1}}],[\"知识的定义\",{\"1\":{\"186\":1}}],[\"知识蒸馏本质是知识的迁移与泛化\",{\"1\":{\"192\":1}}],[\"知识蒸馏\",{\"1\":{\"185\":1}}],[\"知识蒸馏knowledge\",{\"0\":{\"185\":1}}],[\"范式\",{\"1\":{\"186\":1}}],[\"范数\",{\"0\":{\"55\":1},\"1\":{\"55\":2}}],[\"教师整合全局知识\",{\"1\":{\"189\":1}}],[\"教师的软标签为优化提供更丰富的梯度信息\",{\"1\":{\"188\":1}}],[\"教师可能输出\",{\"1\":{\"186\":1}}],[\"教师\",{\"1\":{\"186\":1,\"189\":1}}],[\"教师模型输出的概率分布\",{\"1\":{\"186\":1}}],[\"教师模型\",{\"1\":{\"185\":1,\"186\":1}}],[\"学生输出\",{\"1\":{\"187\":2}}],[\"学生\",{\"1\":{\"186\":1,\"189\":1}}],[\"学生模型学到更鲁棒的特征\",{\"1\":{\"188\":1}}],[\"学生模型使用标准softmax\",{\"1\":{\"187\":1}}],[\"学生模型同时学习\",{\"1\":{\"187\":1}}],[\"学生模型\",{\"1\":{\"185\":1,\"186\":1}}],[\"学习的位置编码可能无法泛化到训练时未见过的序列长度\",{\"1\":{\"182\":1}}],[\"学习的位置编码需要额外参数\",{\"1\":{\"181\":1}}],[\"学习的是确定性策略\",{\"1\":{\"82\":1}}],[\"学习好的参数\",{\"1\":{\"113\":1}}],[\"学习每个动作的\",{\"1\":{\"88\":1}}],[\"学习用的数据可以来自于任何策略\",{\"1\":{\"87\":1}}],[\"学习用的数据必须是由当前正在优化的策略所产生的\",{\"1\":{\"87\":1}}],[\"学习更快\",{\"1\":{\"84\":1}}],[\"学习一个\",{\"1\":{\"82\":1}}],[\"学习什么\",{\"0\":{\"81\":1}}],[\"迁移样本间的关系\",{\"1\":{\"190\":1}}],[\"迁移\",{\"1\":{\"185\":1}}],[\"兼容\",{\"1\":{\"183\":1}}],[\"允许模型处理不同频率的位置信息\",{\"1\":{\"182\":1}}],[\"确保了计算上的稳定性和效率\",{\"1\":{\"181\":1}}],[\"确定性变换\",{\"1\":{\"26\":1}}],[\"确定性计算\",{\"1\":{\"24\":2}}],[\"确定性\",{\"1\":{\"13\":1,\"103\":1}}],[\"原始训练数据的标签\",{\"1\":{\"186\":1}}],[\"原始transformer\",{\"1\":{\"180\":1}}],[\"原来关于期望的梯度\",{\"1\":{\"24\":1}}],[\"导致位置信息丢失\",{\"1\":{\"182\":1}}],[\"导致模型在长序列或偏移序列上性能下降\",{\"1\":{\"180\":1}}],[\"导致在新数据上的泛化能力很差\",{\"1\":{\"65\":1}}],[\"具有坚实的数学基础\",{\"1\":{\"181\":1}}],[\"具有显著优越性\",{\"1\":{\"179\":1}}],[\"具体步骤如下\",{\"1\":{\"174\":1}}],[\"具体的操作是\",{\"1\":{\"174\":1}}],[\"具体计算为\",{\"1\":{\"148\":1}}],[\"具体关注了序列中的哪些其他词\",{\"1\":{\"133\":1}}],[\"具体来说\",{\"1\":{\"96\":1}}],[\"具体地\",{\"1\":{\"18\":3}}],[\"具体选择哪种方法取决于任务的需求和可用的计算资源\",{\"1\":{\"13\":1}}],[\"查询和键\",{\"1\":{\"178\":1}}],[\"查看得分表\",{\"1\":{\"82\":1}}],[\"旋转角度的设计使得模型能够捕获从短到长的各种依赖关系\",{\"1\":{\"182\":1}}],[\"旋转不变性意味着内积仅依赖于相对位置\",{\"1\":{\"181\":1}}],[\"旋转不变性和数学优雅\",{\"0\":{\"181\":1}}],[\"旋转矩阵\",{\"1\":{\"178\":1}}],[\"旋转位置编码rope详解\",{\"0\":{\"176\":1}}],[\"频率参数\",{\"1\":{\"178\":1}}],[\"算式\",{\"0\":{\"178\":1}}],[\"算法优点分析\",{\"0\":{\"179\":1}}],[\"算法原理\",{\"1\":{\"176\":1}}],[\"算法及其背后的数学公式\",{\"1\":{\"109\":1}}],[\"算法结构\",{\"1\":{\"88\":1}}],[\"算法会先尝试学习一个环境的模型\",{\"1\":{\"86\":1}}],[\"算法不试图去理解环境的运作规则\",{\"1\":{\"86\":1}}],[\"算法如何学习\",{\"1\":{\"80\":1}}],[\"算法学习的核心是什么\",{\"1\":{\"80\":1}}],[\"算法的详细步骤解析\",{\"0\":{\"30\":1}}],[\"∈\",{\"1\":{\"174\":1}}],[\"∈r\",{\"1\":{\"110\":2}}],[\"∈rn×n\",{\"1\":{\"148\":1}}],[\"∈rn×\",{\"1\":{\"121\":1}}],[\"∈rn\",{\"1\":{\"110\":1}}],[\"另一个是平移因子β\",{\"1\":{\"174\":1}}],[\"另外两个重要的分类维度\",{\"0\":{\"85\":1}}],[\"另外\",{\"1\":{\"20\":1}}],[\"尤其在边缘计算\",{\"1\":{\"192\":1}}],[\"尤其在语言建模任务上\",{\"1\":{\"109\":1}}],[\"尤其是在处理序列化数据时\",{\"1\":{\"174\":1}}],[\"奥卡姆剃刀原则在深度学习中的完美体现\",{\"1\":{\"173\":1}}],[\"勿增实体\",{\"1\":{\"173\":1}}],[\"步骤\",{\"1\":{\"173\":1}}],[\"步骤如下\",{\"1\":{\"14\":1}}],[\"像llama这样的顶级模型选择了rmsnorm\",{\"1\":{\"173\":1}}],[\"实时系统中\",{\"1\":{\"192\":1}}],[\"实现了更高的计算效率\",{\"1\":{\"173\":1}}],[\"实践验证\",{\"1\":{\"173\":1}}],[\"实际信息\",{\"1\":{\"137\":1}}],[\"实际上\",{\"1\":{\"84\":1}}],[\"实际上可以通过数据的协方差矩阵的\",{\"1\":{\"74\":1}}],[\"实际上有很多相关的研究论文\",{\"1\":{\"14\":1}}],[\"任何能节省计算开销\",{\"1\":{\"173\":1}}],[\"流行度\",{\"1\":{\"172\":1}}],[\"较低\",{\"1\":{\"172\":1}}],[\"较高\",{\"1\":{\"172\":1}}],[\"需算\",{\"1\":{\"172\":1}}],[\"需要应用链式法则和一些数学技巧\",{\"1\":{\"174\":1}}],[\"需要走的最短路径\",{\"1\":{\"59\":1}}],[\"需要对真实后验\",{\"1\":{\"47\":1}}],[\"需要很多层才能捕捉到全局信息\",{\"1\":{\"31\":1}}],[\"需要通过一个可训练的线性投射层\",{\"1\":{\"30\":1}}],[\"需要初始化神经网络的权重\",{\"1\":{\"15\":1}}],[\"需要注意的是\",{\"1\":{\"14\":1,\"18\":1}}],[\"否\",{\"1\":{\"172\":1}}],[\"否则为0\",{\"1\":{\"157\":1}}],[\"否则\",{\"1\":{\"15\":1}}],[\"否则以这个比值作为接受新的权重值的概率\",{\"1\":{\"15\":1}}],[\"语言模型\",{\"1\":{\"171\":1}}],[\"语义关联和语法依赖关系\",{\"1\":{\"130\":1}}],[\"语义\",{\"1\":{\"125\":1}}],[\"语义分析\",{\"1\":{\"74\":1}}],[\"论文在多个任务\",{\"1\":{\"171\":1}}],[\"论文作者通过实验和理论分析发现\",{\"1\":{\"170\":1}}],[\"～64\",{\"1\":{\"171\":1}}],[\"问题\",{\"1\":{\"169\":1}}],[\"问题的关键在于\",{\"1\":{\"22\":1}}],[\"β\",{\"1\":{\"167\":2,\"172\":1,\"174\":2}}],[\"γ\",{\"1\":{\"167\":2,\"172\":2,\"174\":2}}],[\"给定一个输入向量\",{\"1\":{\"167\":1}}],[\"给定两个概率分布\",{\"1\":{\"51\":1}}],[\"减少了参数数量并提高了计算效率\",{\"1\":{\"181\":1}}],[\"减去\",{\"1\":{\"170\":1}}],[\"减去均值的中心化操作所提供的收益\",{\"1\":{\"171\":1}}],[\"减去均值的中心化操作在计算上是有开销的\",{\"1\":{\"170\":1}}],[\"减去均值\",{\"1\":{\"166\":1,\"169\":1}}],[\"减小\",{\"1\":{\"125\":1}}],[\"已输出的序列会作为解码器的输入\",{\"1\":{\"163\":1}}],[\"初始词嵌入\",{\"1\":{\"161\":1}}],[\"初始化方式\",{\"1\":{\"20\":1}}],[\"初始化权重\",{\"1\":{\"15\":1}}],[\"初始化\",{\"1\":{\"15\":1,\"18\":1}}],[\"才能生成最有助于正确预测的上下文向量\",{\"1\":{\"159\":1}}],[\"某个\",{\"1\":{\"159\":1}}],[\"某个具体的观测值发生的概率\",{\"1\":{\"18\":1}}],[\"调整后的输出\",{\"1\":{\"174\":1}}],[\"调整那些生成\",{\"1\":{\"159\":1}}],[\"调整一个分数\",{\"1\":{\"157\":1}}],[\"传回\",{\"1\":{\"159\":1}}],[\"传统rnn需要一步步顺序处理\",{\"1\":{\"133\":1}}],[\"传统\",{\"1\":{\"115\":1}}],[\"传统的一些自监督学习方法\",{\"1\":{\"96\":1}}],[\"传统的目标检测算法通常需要预设大量的锚框\",{\"1\":{\"91\":1}}],[\"传统的\",{\"1\":{\"29\":1,\"31\":1,\"113\":1}}],[\"别忘了我们之前对分数进行了缩放\",{\"1\":{\"157\":1}}],[\"∘\",{\"1\":{\"157\":1}}],[\"前向过程是\",{\"1\":{\"157\":1}}],[\"前馈神经网络\",{\"1\":{\"92\":1}}],[\"前馈网络\",{\"1\":{\"30\":1}}],[\"链式法则\",{\"1\":{\"154\":1}}],[\"∂s∂l​\",{\"1\":{\"158\":2}}],[\"∂a∂l​\",{\"1\":{\"157\":1}}],[\"∂a∂l​=∂o∂l​vt\",{\"1\":{\"156\":1}}],[\"∂xj​∂yi​​=yi​\",{\"1\":{\"157\":1}}],[\"∂x∂y​\",{\"1\":{\"157\":1}}],[\"∂o∂l​\",{\"1\":{\"155\":1,\"156\":1}}],[\"∂v∂l​=at∂o∂l​\",{\"1\":{\"155\":1}}],[\"∂v∂l​\",{\"1\":{\"154\":1}}],[\"∂k∂l​=\",{\"1\":{\"158\":1}}],[\"∂k∂l​\",{\"1\":{\"154\":1}}],[\"∂q∂l​=∂s∂l​⋅k\",{\"1\":{\"158\":1}}],[\"∂q∂l​\",{\"1\":{\"154\":1}}],[\"理解注意力机制的反向传播是理解其如何学习的关键\",{\"1\":{\"154\":1}}],[\"支持端到端训练\",{\"1\":{\"153\":1}}],[\"按行\",{\"1\":{\"152\":1}}],[\"按回\",{\"1\":{\"106\":1}}],[\"归一化后的样本将被缩放和平移\",{\"1\":{\"174\":1}}],[\"归一化的输入样本\",{\"1\":{\"174\":1}}],[\"归一化的根本目的是控制数据分布的尺度和稳定性\",{\"1\":{\"169\":1}}],[\"归一化\",{\"1\":{\"152\":1,\"174\":1}}],[\"缩放和平移\",{\"1\":{\"174\":1}}],[\"缩放和偏移\",{\"1\":{\"172\":1}}],[\"缩放与权重\",{\"1\":{\"154\":1}}],[\"缩放\",{\"1\":{\"152\":1,\"169\":1,\"173\":1}}],[\"缩放后的注意力分数矩阵\",{\"1\":{\"120\":1}}],[\"突出重要关系\",{\"1\":{\"151\":1}}],[\"越小\",{\"1\":{\"151\":1}}],[\"越接近真实后验\",{\"1\":{\"46\":1}}],[\"夹角\",{\"1\":{\"151\":1}}],[\"满足\",{\"1\":{\"148\":1}}],[\"说明\",{\"1\":{\"144\":1,\"189\":1}}],[\"说明这个动作在新策略下变得更不可能被选中了\",{\"1\":{\"106\":1}}],[\"说明这个动作在新策略下变得更可能被选中了\",{\"1\":{\"106\":1}}],[\"空间\",{\"1\":{\"144\":1}}],[\"获得了更深层次\",{\"1\":{\"141\":1}}],[\"交流\",{\"1\":{\"141\":1}}],[\"句子的哪些部分\",{\"1\":{\"141\":1}}],[\"三\",{\"0\":{\"172\":1}}],[\"三个矩阵\",{\"1\":{\"141\":1}}],[\"三角不等式\",{\"1\":{\"52\":2}}],[\"角色分离\",{\"1\":{\"141\":1}}],[\"效率至上\",{\"1\":{\"173\":1}}],[\"效率极高\",{\"1\":{\"141\":1}}],[\"效果相当\",{\"1\":{\"172\":1}}],[\"效果\",{\"1\":{\"47\":1,\"172\":1}}],[\"应该如何在编码器的所有输出\",{\"1\":{\"159\":1}}],[\"应该更像正确答案才对\",{\"1\":{\"159\":1}}],[\"应该分配多少注意力给序列中的每个位置\",{\"1\":{\"139\":1}}],[\"应用softmax获得注意力权重\",{\"0\":{\"139\":1}}],[\"应用场景\",{\"1\":{\"135\":1}}],[\"应用领域\",{\"1\":{\"74\":1}}],[\"应用于计算机视觉领域\",{\"1\":{\"28\":1}}],[\"函数的反向传播\",{\"1\":{\"157\":1}}],[\"函数\",{\"1\":{\"139\":1,\"157\":1}}],[\"除以标准差\",{\"1\":{\"169\":1}}],[\"除以\",{\"1\":{\"139\":1,\"170\":1}}],[\"除了最大化奖励\",{\"1\":{\"104\":1}}],[\"除了根据梯度调整参数外\",{\"1\":{\"67\":1}}],[\"除了重参数技巧\",{\"1\":{\"25\":1}}],[\"意义\",{\"1\":{\"137\":1}}],[\"连接两个不同的序列\",{\"1\":{\"135\":1}}],[\"作弊\",{\"1\":{\"134\":1}}],[\"作为资深的机器学习专家\",{\"1\":{\"176\":1}}],[\"作为奖励\",{\"1\":{\"104\":1}}],[\"作为模型预测的目标\",{\"1\":{\"97\":1}}],[\"作为新的\",{\"1\":{\"74\":1}}],[\"作为惩罚项\",{\"1\":{\"62\":2,\"65\":2}}],[\"作为正则化器\",{\"1\":{\"62\":1}}],[\"作为损失函数\",{\"1\":{\"62\":1}}],[\"作为梯度的权重\",{\"1\":{\"25\":1}}],[\"极高的并行化程度\",{\"1\":{\"133\":1}}],[\"极大地加快了训练速度\",{\"1\":{\"112\":1}}],[\"爆炸\",{\"1\":{\"133\":1}}],[\"马路\",{\"1\":{\"131\":1}}],[\"马尔科夫链的定义是下一个状态\",{\"1\":{\"15\":1}}],[\"马尔科夫链蒙特卡洛\",{\"1\":{\"15\":1}}],[\"考虑这个句子\",{\"1\":{\"131\":1}}],[\"捕捉一个序列内部的内部结构\",{\"1\":{\"130\":1}}],[\"捕获了数据中最重要\",{\"1\":{\"73\":1}}],[\"融入了全局上下文信息的元素表示\",{\"1\":{\"130\":1}}],[\"审视\",{\"1\":{\"130\":1}}],[\"太好了\",{\"1\":{\"129\":1}}],[\"残差连接和归一化\",{\"1\":{\"162\":1}}],[\"残差连接以及归一化\",{\"1\":{\"161\":1}}],[\"残差连接\",{\"1\":{\"128\":1}}],[\"残差连接与层归一化\",{\"1\":{\"30\":1}}],[\"低\",{\"1\":{\"127\":1}}],[\"低方差的梯度使得随机梯度下降\",{\"1\":{\"25\":1}}],[\"类似计算\",{\"1\":{\"126\":1}}],[\"头间完全并行\",{\"1\":{\"127\":1}}],[\"头2计算\",{\"1\":{\"126\":1}}],[\"头1计算\",{\"1\":{\"126\":1}}],[\"头数\",{\"1\":{\"119\":1}}],[\"则其均值μ和方差σ²分别为\",{\"1\":{\"174\":1}}],[\"则\",{\"1\":{\"126\":1}}],[\"设查询向量\",{\"1\":{\"178\":1}}],[\"设输入样本\",{\"1\":{\"174\":1}}],[\"设\",{\"1\":{\"126\":1,\"152\":2}}],[\"设定目标分布\",{\"1\":{\"15\":2}}],[\"隐含分解为子矩阵\",{\"1\":{\"125\":1}}],[\"表示对每一行进行求和\",{\"1\":{\"157\":1}}],[\"表示逐元素相乘\",{\"1\":{\"157\":1}}],[\"表示\",{\"1\":{\"153\":1,\"159\":1}}],[\"表示第\",{\"1\":{\"145\":1}}],[\"表示当前词所包含的\",{\"1\":{\"137\":1}}],[\"表示当前词作为\",{\"1\":{\"137\":2}}],[\"表示在当前状态下选择每个动作的可能性\",{\"1\":{\"83\":1}}],[\"表示在给定参数θ的情况下\",{\"1\":{\"18\":1}}],[\"表征学习\",{\"1\":{\"135\":1}}],[\"表达能力\",{\"1\":{\"127\":1}}],[\"表达能力增强\",{\"1\":{\"125\":1}}],[\"位置\",{\"1\":{\"125\":1}}],[\"位置编码\",{\"1\":{\"30\":1}}],[\"子空间分解\",{\"1\":{\"125\":1}}],[\"←────────┘\",{\"1\":{\"124\":1}}],[\"└──────────┘\",{\"1\":{\"124\":1}}],[\"├→\",{\"1\":{\"124\":1}}],[\"├─\",{\"1\":{\"124\":3}}],[\"──┤\",{\"1\":{\"124\":2}}],[\"──┐\",{\"1\":{\"124\":1}}],[\"→\",{\"1\":{\"124\":2,\"189\":1}}],[\"│\",{\"1\":{\"124\":16}}],[\"⋯\",{\"1\":{\"121\":1}}],[\"拼接与输出\",{\"1\":{\"126\":1}}],[\"拼接后维度\",{\"1\":{\"121\":1}}],[\"拼接上一个可学习的\",{\"1\":{\"30\":1}}],[\"独立计算缩放点积注意力\",{\"1\":{\"120\":1}}],[\"独立噪声\",{\"1\":{\"26\":1}}],[\"投影\",{\"1\":{\"152\":1}}],[\"投影回模型维度\",{\"1\":{\"122\":1}}],[\"投影后维度\",{\"1\":{\"119\":1}}],[\"投机取巧\",{\"1\":{\"102\":1}}],[\"$\",{\"1\":{\"117\":1}}],[\"开会交流\",{\"1\":{\"131\":1}}],[\"开辟了新的道路\",{\"1\":{\"115\":1}}],[\"开启了深度强化学习的时代\",{\"1\":{\"82\":1}}],[\"开启了一个全新的研究范式\",{\"1\":{\"31\":1}}],[\"注意\",{\"1\":{\"163\":1}}],[\"注意前馈神经网络对于序列每个位置的独立性\",{\"1\":{\"162\":1}}],[\"注意其输入仅包含\",{\"1\":{\"162\":1}}],[\"注意力得分只依赖于相对位置\",{\"1\":{\"178\":1}}],[\"注意力得分计算为\",{\"1\":{\"178\":1}}],[\"注意力权重矩阵\",{\"1\":{\"141\":1}}],[\"注意力分数\",{\"1\":{\"131\":1,\"152\":1}}],[\"注意力机制最优雅的地方在于它完全是可微的\",{\"1\":{\"154\":1}}],[\"注意力机制\",{\"1\":{\"115\":1}}],[\"注入到系统中\",{\"1\":{\"24\":1}}],[\"性能不相上下甚至更好\",{\"1\":{\"171\":1}}],[\"性能\",{\"1\":{\"115\":1}}],[\"恒定内存\",{\"1\":{\"115\":2}}],[\"循环神经网络\",{\"1\":{\"174\":1}}],[\"循环结构\",{\"1\":{\"115\":1}}],[\"循环\",{\"1\":{\"115\":2}}],[\"动物\",{\"1\":{\"131\":1}}],[\"动态权重分配\",{\"1\":{\"153\":1}}],[\"动态\",{\"1\":{\"115\":1}}],[\"动作a比平均水平好多少\",{\"1\":{\"107\":1}}],[\"动作a好不好\",{\"1\":{\"107\":1}}],[\"动作概率+状态价值\",{\"1\":{\"88\":1}}],[\"动作的概率\",{\"1\":{\"88\":1}}],[\"动作\",{\"1\":{\"82\":1}}],[\"静态\",{\"1\":{\"115\":1}}],[\"仍然是静态参数\",{\"1\":{\"114\":1}}],[\"仍然需要根据具体的问题和数据来选择最合适的方法\",{\"1\":{\"14\":1}}],[\"递归在\",{\"1\":{\"114\":1}}],[\"递归计算是唯一选择\",{\"1\":{\"114\":1}}],[\"聚焦于关键信息\",{\"1\":{\"113\":1}}],[\"聚合整个图像的全局信息\",{\"1\":{\"29\":1}}],[\"忽略不相关的历史信息\",{\"1\":{\"113\":1}}],[\"淹没\",{\"1\":{\"113\":1}}],[\"检索\",{\"1\":{\"113\":1}}],[\"思考\",{\"1\":{\"113\":1}}],[\"遇到重要词时\",{\"1\":{\"113\":1}}],[\"阅读\",{\"1\":{\"113\":1}}],[\"内容感知\",{\"1\":{\"113\":1}}],[\"内部处理的向量维度是固定的\",{\"1\":{\"30\":1}}],[\"内部关于参数\",{\"1\":{\"25\":1}}],[\"劣势\",{\"1\":{\"112\":1}}],[\"卷积\",{\"1\":{\"115\":1}}],[\"卷积核不再固定\",{\"1\":{\"114\":1}}],[\"卷积核\",{\"1\":{\"112\":1}}],[\"卷积是高度可并行的操作\",{\"1\":{\"112\":1}}],[\"卷积模式\",{\"0\":{\"112\":1}}],[\"方法\",{\"1\":{\"111\":1}}],[\"方差计算也需要均值\",{\"1\":{\"171\":1}}],[\"方差\",{\"1\":{\"169\":1}}],[\"方差更低的基线\",{\"1\":{\"107\":1}}],[\"方差大\",{\"1\":{\"88\":1}}],[\"方差较大\",{\"1\":{\"83\":1}}],[\"转化为一个超长向量\",{\"1\":{\"163\":1}}],[\"转化为从一个固定的\",{\"1\":{\"22\":1}}],[\"转换为一个连续的表达\",{\"1\":{\"160\":1}}],[\"转换为离散参数\",{\"1\":{\"111\":1}}],[\"δij​\",{\"1\":{\"157\":1}}],[\"δij​−yj​\",{\"1\":{\"157\":1}}],[\"δk​\",{\"1\":{\"114\":2}}],[\"δk​=softplus\",{\"1\":{\"113\":1}}],[\"δa\",{\"1\":{\"111\":3}}],[\"δ\",{\"1\":{\"111\":1,\"113\":4,\"115\":1}}],[\"离散\",{\"1\":{\"112\":1}}],[\"离散化后\",{\"1\":{\"111\":1}}],[\"离散化\",{\"0\":{\"111\":1}}],[\"离线策略\",{\"0\":{\"87\":1},\"1\":{\"87\":1}}],[\"看\",{\"1\":{\"141\":1}}],[\"看到未来的词\",{\"1\":{\"134\":1}}],[\"看作一个具有内部记忆\",{\"1\":{\"110\":1}}],[\"看起来一样\",{\"1\":{\"61\":1}}],[\"控制状态更新的节奏\",{\"1\":{\"113\":1}}],[\"控制状态如何影响输出\",{\"1\":{\"113\":1}}],[\"控制状态如何贡献到输出\",{\"1\":{\"110\":1}}],[\"控制输入如何进入状态\",{\"1\":{\"113\":1}}],[\"控制输入如何影响状态\",{\"1\":{\"110\":1}}],[\"控制系统动力学的演变\",{\"1\":{\"110\":1}}],[\"描述了如何从当前状态生成输出\",{\"1\":{\"110\":1}}],[\"描述了隐藏状态\",{\"1\":{\"110\":1}}],[\"映射为一个一维的输出信号\",{\"1\":{\"110\":1}}],[\"信息融合\",{\"1\":{\"153\":1}}],[\"信息都会在所有图像块之间进行流动和整合\",{\"1\":{\"30\":1}}],[\"信号处理等领域\",{\"1\":{\"110\":1}}],[\"建立在连续时间状态空间模型之上\",{\"1\":{\"110\":1}}],[\"挑战了\",{\"1\":{\"109\":1}}],[\"挑战与改进\",{\"1\":{\"93\":1}}],[\"基础\",{\"0\":{\"110\":1}}],[\"基因组等\",{\"1\":{\"109\":1}}],[\"基于现有的序列\",{\"1\":{\"160\":1}}],[\"基于图像的联合嵌入预测架构\",{\"1\":{\"95\":1}}],[\"基于transformer的端到端目标检测新范式\",{\"0\":{\"90\":1}}],[\"基于策略\",{\"1\":{\"88\":1}}],[\"基于策略的算法\",{\"0\":{\"83\":1}}],[\"基于价值\",{\"1\":{\"88\":1}}],[\"基于价值的算法\",{\"0\":{\"82\":1}}],[\"基于上述分解\",{\"1\":{\"73\":1}}],[\"音频\",{\"1\":{\"109\":1}}],[\"进一步鼓励探索\",{\"1\":{\"108\":1}}],[\"进行预测\",{\"1\":{\"187\":1}}],[\"进行预训练时\",{\"1\":{\"31\":1}}],[\"进行缩放\",{\"1\":{\"166\":1}}],[\"进行离散化\",{\"1\":{\"114\":1}}],[\"进行高效计算\",{\"1\":{\"112\":1}}],[\"进行\",{\"1\":{\"74\":2}}],[\"进行随机旋转\",{\"1\":{\"69\":1}}],[\"进行积分\",{\"1\":{\"45\":1}}],[\"进行非线性变换\",{\"1\":{\"30\":1}}],[\"双管齐下\",{\"1\":{\"108\":1}}],[\"双网络博弈结构确实天然地存在\",{\"1\":{\"104\":1}}],[\"他也会小步前进\",{\"1\":{\"108\":1}}],[\"他每次只向上走一小段固定的距离\",{\"1\":{\"108\":1}}],[\"错过了通往主峰\",{\"1\":{\"108\":1}}],[\"局部最优\",{\"1\":{\"108\":1}}],[\"超出预期的好坏\",{\"1\":{\"107\":1}}],[\"百米冲刺\",{\"1\":{\"106\":1}}],[\"式的优化\",{\"1\":{\"106\":1}}],[\"温和地变成\",{\"1\":{\"106\":1}}],[\"疯狂地增加这个动作被选中的概率\",{\"1\":{\"106\":1}}],[\"偶然发现了一个\",{\"1\":{\"106\":1}}],[\"机器翻译\",{\"1\":{\"171\":1}}],[\"机器学习模型都有一个共同的敌人\",{\"1\":{\"65\":1}}],[\"机制允许\",{\"1\":{\"131\":1}}],[\"机制如何防止模式坍塌\",{\"1\":{\"106\":1}}],[\"倍\",{\"1\":{\"106\":2}}],[\"限制在一个安全区间内\",{\"1\":{\"106\":1}}],[\"ε\",{\"1\":{\"106\":7,\"174\":2}}],[\"<\",{\"1\":{\"106\":1}}],[\"π\",{\"1\":{\"106\":4}}],[\"尽可能地提升策略\",{\"1\":{\"106\":1}}],[\"尽管存在一些挑战\",{\"1\":{\"93\":1}}],[\"尽管detr具有开创性\",{\"1\":{\"93\":1}}],[\"截断机制会介入\",{\"1\":{\"106\":1}}],[\"截断\",{\"1\":{\"106\":2}}],[\"截断的替代目标\",{\"0\":{\"106\":1}}],[\"截断svd\",{\"1\":{\"73\":1}}],[\"保持了缩放不变性\",{\"1\":{\"171\":1}}],[\"保留原始信息\",{\"1\":{\"128\":1}}],[\"保留前\",{\"1\":{\"74\":1}}],[\"保证优化过程的稳定性\",{\"1\":{\"105\":1}}],[\"两个词\",{\"1\":{\"136\":1}}],[\"两个领域也都独立发展出了一系列旨在\",{\"1\":{\"104\":1}}],[\"两个结构中的\",{\"1\":{\"104\":1}}],[\"人为地加入一些随机噪声\",{\"1\":{\"104\":1}}],[\"人工扩大训练数据集\",{\"1\":{\"69\":1}}],[\"增加了计算复杂性和内存使用\",{\"1\":{\"183\":1}}],[\"增加\",{\"1\":{\"125\":1}}],[\"增加探索噪声\",{\"1\":{\"104\":1}}],[\"增加模型的表达能力\",{\"1\":{\"30\":1}}],[\"奖励熵\",{\"1\":{\"104\":1}}],[\"熵衡量的是策略的不确定性\",{\"1\":{\"104\":1}}],[\"熵正则化\",{\"1\":{\"104\":1}}],[\"判别器\",{\"1\":{\"104\":1}}],[\"判别器可能因为训练得不够好\",{\"1\":{\"102\":1}}],[\"让学生中间层特征图匹配教师的特征\",{\"1\":{\"190\":1}}],[\"让学生模型模仿教师模型的输出行为\",{\"1\":{\"186\":1}}],[\"让同一个输入词在不同场景下扮演不同角色\",{\"1\":{\"141\":1}}],[\"让输入序列的每个词都能充分理解其所在句子的全局上下文信息\",{\"1\":{\"134\":1}}],[\"让序列中的每一个元素\",{\"1\":{\"130\":1}}],[\"让模型根据输入调整其内部时钟\",{\"1\":{\"113\":1}}],[\"让模型决定此刻应该从记忆中回忆什么信息来产生输出\",{\"1\":{\"113\":1}}],[\"让模型决定是否要将当前输入信息纳入记忆\",{\"1\":{\"113\":1}}],[\"让\",{\"1\":{\"113\":1}}],[\"让策略更加稳健\",{\"1\":{\"108\":1}}],[\"让判别器不仅看单个样本\",{\"1\":{\"104\":1}}],[\"让我们看到模型在处理一个词时到底在\",{\"1\":{\"141\":1}}],[\"让我们把它拆解开来看\",{\"1\":{\"106\":1}}],[\"让我们把这些能力对应到具体的应用上\",{\"1\":{\"74\":1}}],[\"让我们来拆解这三个部件\",{\"1\":{\"72\":1}}],[\"让我们逐一验证js散度是否满足这些条件\",{\"1\":{\"52\":1}}],[\"架构中\",{\"1\":{\"160\":1}}],[\"架构\",{\"1\":{\"160\":1}}],[\"架构的核心创新\",{\"1\":{\"128\":1}}],[\"架构调整\",{\"1\":{\"104\":1}}],[\"架构上所做的关键改变\",{\"1\":{\"28\":1}}],[\"改进损失函数\",{\"1\":{\"104\":1}}],[\"改造后\",{\"1\":{\"24\":1}}],[\"改造前\",{\"1\":{\"24\":1}}],[\"缺乏多样性的输出空间\",{\"1\":{\"104\":1}}],[\"缺点\",{\"1\":{\"13\":2,\"82\":1,\"83\":1,\"84\":1,\"86\":2,\"87\":2,\"88\":1}}],[\"收敛更快\",{\"1\":{\"174\":1}}],[\"收敛到了一个狭窄的\",{\"1\":{\"104\":1}}],[\"收敛速度慢\",{\"1\":{\"25\":1,\"93\":1}}],[\"收敛速度\",{\"1\":{\"13\":1}}],[\"共性\",{\"1\":{\"104\":1}}],[\"共同对抗过拟合\",{\"1\":{\"69\":1}}],[\"共同作为\",{\"1\":{\"30\":1}}],[\"希望能发现长期来看更好的策略\",{\"1\":{\"103\":1}}],[\"希望这个解释帮你理清了传统机器学习与深度学习之间美妙的联系\",{\"1\":{\"70\":1}}],[\"希望这个从直观到数学的解释能帮助你彻底理解\",{\"1\":{\"63\":1}}],[\"未知的动作\",{\"1\":{\"103\":1}}],[\"未见过的数据可能会有很大的不确定性\",{\"1\":{\"19\":1}}],[\"未见过的数据时提供有用的不确定性信息是非常有价值的\",{\"1\":{\"12\":1}}],[\"利用\",{\"1\":{\"103\":3,\"114\":1}}],[\"利器\",{\"0\":{\"69\":1}}],[\"探索性急剧下降\",{\"1\":{\"106\":1}}],[\"探索\",{\"1\":{\"103\":2}}],[\"左右腿交替大步走\",{\"1\":{\"103\":1}}],[\"左奇异向量矩阵\",{\"1\":{\"72\":1}}],[\"陷入局部最优\",{\"1\":{\"103\":1}}],[\"盲点\",{\"1\":{\"102\":1}}],[\"完全停止了对其他可能更好动作的探索\",{\"1\":{\"103\":1}}],[\"完全丧失了生成多样化人脸的能力\",{\"1\":{\"102\":1}}],[\"完美地记住了训练数据中的噪声和细节\",{\"1\":{\"65\":1}}],[\"完美地适配了\",{\"1\":{\"29\":1}}],[\"发生了模式坍塌\",{\"1\":{\"102\":1}}],[\"发现rmsnorm的表现与layernorm相当甚至更好\",{\"1\":{\"171\":1}}],[\"发现执行某一个或一小撮特定的动作\",{\"1\":{\"103\":1}}],[\"发现数据背后隐藏的\",{\"1\":{\"76\":1}}],[\"发现可能只有一个很大的奇异值\",{\"1\":{\"75\":1}}],[\"例\",{\"1\":{\"186\":1}}],[\"例句\",{\"1\":{\"136\":1}}],[\"例子\",{\"1\":{\"102\":1,\"103\":1}}],[\"例如当\",{\"1\":{\"144\":1}}],[\"例如句子中的每一个词\",{\"1\":{\"130\":1}}],[\"例如minibatch\",{\"1\":{\"104\":1}}],[\"例如wgan\",{\"1\":{\"104\":1}}],[\"例如在imagenet上的线性探测\",{\"1\":{\"100\":1}}],[\"例如dqn\",{\"1\":{\"86\":1}}],[\"例如高斯分布族\",{\"1\":{\"45\":1}}],[\"例如尺寸为\",{\"1\":{\"30\":1}}],[\"例如均值和方差\",{\"1\":{\"18\":1}}],[\"例如达到预定的迭代次数\",{\"1\":{\"18\":1}}],[\"例如\",{\"1\":{\"13\":1,\"14\":1,\"15\":1,\"18\":2,\"19\":2,\"20\":2,\"22\":1,\"30\":3,\"59\":1,\"62\":1,\"72\":2,\"73\":1,\"82\":1,\"93\":1,\"110\":1,\"113\":1,\"129\":1,\"159\":1}}],[\"单模态模型\",{\"1\":{\"189\":1}}],[\"单一表示空间\",{\"1\":{\"127\":1}}],[\"单一的样本\",{\"1\":{\"102\":1}}],[\"单头attention\",{\"0\":{\"154\":1}}],[\"单头可能取\",{\"1\":{\"144\":1}}],[\"单头\",{\"0\":{\"142\":1}}],[\"单头注意力\",{\"1\":{\"127\":1,\"142\":1}}],[\"单头计算量=o\",{\"1\":{\"125\":1}}],[\"单位圆\",{\"0\":{\"60\":1},\"1\":{\"60\":2}}],[\"安全\",{\"1\":{\"102\":1}}],[\"于是它就一直使用这个笨拙但安全的策略\",{\"1\":{\"103\":1}}],[\"于是\",{\"1\":{\"102\":2,\"103\":1}}],[\"于2020年提出\",{\"1\":{\"90\":1}}],[\"懒惰\",{\"1\":{\"102\":1}}],[\"生成合成数据蒸馏\",{\"1\":{\"190\":1}}],[\"生成概率分布\",{\"1\":{\"187\":1}}],[\"生成软标签\",{\"1\":{\"187\":1}}],[\"生成的\",{\"1\":{\"159\":1}}],[\"生成的新向量就是\",{\"1\":{\"140\":1}}],[\"生成一个\",{\"1\":{\"134\":1}}],[\"生成器找到了判别器的一个\",{\"1\":{\"102\":1}}],[\"生成器\",{\"1\":{\"102\":1}}],[\"生成者\",{\"1\":{\"101\":1,\"104\":1}}],[\"创造出一种高效且强大的自监督学习范式\",{\"1\":{\"99\":1}}],[\"砖块\",{\"1\":{\"99\":2}}],[\"大大降低了过早陷入局部最优\",{\"1\":{\"106\":1}}],[\"大厦\",{\"1\":{\"99\":1}}],[\"大小为\",{\"1\":{\"30\":1}}],[\"回想前向过程\",{\"1\":{\"158\":1}}],[\"回到最初的问题\",{\"1\":{\"99\":1}}],[\"回归\",{\"1\":{\"62\":1}}],[\"框架与组件\",{\"0\":{\"99\":1}}],[\"且\",{\"1\":{\"157\":1}}],[\"且尺寸较大的区域作为目标块\",{\"1\":{\"98\":1}}],[\"且值域固定的散度度量\",{\"1\":{\"50\":1}}],[\"鼓励和维持多样性\",{\"1\":{\"104\":1}}],[\"鼓励多样性\",{\"1\":{\"104\":1}}],[\"鼓励学习语义信息\",{\"0\":{\"98\":1}}],[\"鼓励模型使用所有特征一点点\",{\"1\":{\"67\":1}}],[\"整个反向传播过程可以总结为以下流程图\",{\"1\":{\"159\":1}}],[\"整个过程分为\",{\"1\":{\"142\":1}}],[\"整个输出序列\",{\"1\":{\"112\":1}}],[\"整个系统停止了探索\",{\"1\":{\"103\":1}}],[\"整个系统陷入了一个非常糟糕的局部最优解\",{\"1\":{\"102\":1}}],[\"整个学习过程的目标\",{\"1\":{\"97\":1}}],[\"整个模型是一个单一的\",{\"1\":{\"91\":1}}],[\"构建其强大理解能力的基石\",{\"1\":{\"141\":1}}],[\"构建\",{\"1\":{\"97\":1}}],[\"构建了一个从模型参数到最终损失的全微分计算图\",{\"1\":{\"26\":1}}],[\"目标\",{\"1\":{\"136\":1,\"186\":1}}],[\"目标序列的表示\",{\"1\":{\"132\":1}}],[\"目标编码器和预测器\",{\"1\":{\"99\":1}}],[\"目标编码器的权重并不会通过梯度下降直接进行训练\",{\"1\":{\"97\":1}}],[\"目标编码器\",{\"1\":{\"97\":1}}],[\"目标块\",{\"1\":{\"96\":1,\"97\":1}}],[\"目标语言句子\",{\"1\":{\"29\":1}}],[\"旨在将大型复杂模型\",{\"1\":{\"185\":1}}],[\"旨在处理序列数据中的位置信息\",{\"1\":{\"176\":1}}],[\"旨在让模型像人类一样\",{\"1\":{\"95\":1}}],[\"旨在取长补短\",{\"1\":{\"84\":1}}],[\"近期备受关注的i\",{\"1\":{\"95\":1}}],[\"深入解析\",{\"0\":{\"95\":1}}],[\"深度学习的基础\",{\"1\":{\"172\":1}}],[\"深度学习的成功不仅仅依赖于l1\",{\"1\":{\"69\":1}}],[\"深度学习原生\",{\"1\":{\"70\":1}}],[\"深度学习中的其他正则化\",{\"0\":{\"69\":1}}],[\"深度学习完全继承了这一思想\",{\"1\":{\"65\":1}}],[\"深度神经网络的权重是否符合正态分布这个问题并没有一个定论\",{\"1\":{\"20\":1}}],[\"显著提升了模型捕捉复杂依赖关系的能力\",{\"1\":{\"128\":1}}],[\"显著提升了detr的性能和收敛速度\",{\"1\":{\"93\":1}}],[\"显著提高了检测速度和准确性\",{\"1\":{\"93\":1}}],[\"显著降低梯度估计的方差\",{\"0\":{\"25\":1}}],[\"加快训练速度\",{\"1\":{\"169\":1}}],[\"加快了收敛速度\",{\"1\":{\"93\":1}}],[\"加权输出\",{\"1\":{\"151\":1}}],[\"加速收敛\",{\"1\":{\"69\":1}}],[\"简称ln\",{\"1\":{\"174\":1}}],[\"简称bn\",{\"1\":{\"174\":1}}],[\"简化了模型结构\",{\"1\":{\"180\":1}}],[\"简化后的操作反而可能让模型更专注于学习真正重要的模式\",{\"1\":{\"171\":1}}],[\"简化版本\",{\"1\":{\"111\":1}}],[\"简化流程\",{\"1\":{\"93\":1}}],[\"简单说就是\",{\"1\":{\"87\":1}}],[\"简单直接\",{\"1\":{\"86\":1}}],[\"简单答案是\",{\"1\":{\"64\":1}}],[\"简单来说\",{\"1\":{\"21\":1,\"55\":1,\"113\":1,\"129\":1,\"167\":1}}],[\"端到端\",{\"1\":{\"93\":1}}],[\"端到端的网络\",{\"1\":{\"91\":1}}],[\"端到端学习\",{\"1\":{\"91\":1}}],[\"该算法通过巧妙地将计算重组\",{\"1\":{\"114\":1}}],[\"该集合包含了图像中所有物体的信息\",{\"1\":{\"91\":1}}],[\"该技巧通过将随机变量的生成过程重构为一个确定性函数和一个独立的噪声源\",{\"1\":{\"21\":1}}],[\"集合预测问题\",{\"1\":{\"91\":1}}],[\"现代的ac算法如sac\",{\"1\":{\"104\":1}}],[\"现象是什么\",{\"1\":{\"102\":1,\"103\":1}}],[\"现为meta\",{\"1\":{\"90\":1}}],[\"现在我们有了关于分数矩阵\",{\"1\":{\"158\":1}}],[\"现在我们用数学公式来精确地定义它们\",{\"1\":{\"57\":1}}],[\"现在把\",{\"1\":{\"61\":1}}],[\"现在\",{\"1\":{\"30\":1,\"46\":1,\"74\":1,\"75\":1,\"138\":1}}],[\"难处理连续动作\",{\"1\":{\"88\":1}}],[\"稳定且高效\",{\"1\":{\"88\":1}}],[\"稳定\",{\"1\":{\"88\":1}}],[\"稳健性\",{\"1\":{\"61\":1}}],[\"容易产生高估等问题\",{\"1\":{\"87\":1}}],[\"经验\",{\"1\":{\"192\":1}}],[\"经验池\",{\"1\":{\"87\":1}}],[\"经过i\",{\"1\":{\"100\":1}}],[\"经典的actor\",{\"1\":{\"84\":1}}],[\"经典的表格型rl算法\",{\"1\":{\"82\":1}}],[\"之前的工作\",{\"1\":{\"113\":1}}],[\"之前的数据就不能再用了\",{\"1\":{\"87\":1}}],[\"之间差异的度量\",{\"1\":{\"46\":1}}],[\"之间找到一个好的平衡\",{\"1\":{\"18\":1}}],[\"之间的关系变得更容易理解\",{\"1\":{\"73\":1}}],[\"之间的随机数u\",{\"1\":{\"15\":1}}],[\"之间的kl散度的平均值\",{\"1\":{\"50\":1}}],[\"之间的kl散度\",{\"1\":{\"12\":1}}],[\"边玩边学\",{\"1\":{\"87\":1}}],[\"谬以千里\",{\"1\":{\"86\":1}}],[\"差之毫厘\",{\"1\":{\"86\":1}}],[\"虚拟环境\",{\"1\":{\"86\":1}}],[\"绝对位置编码\",{\"1\":{\"180\":1}}],[\"绝对\",{\"1\":{\"104\":1}}],[\"绝对值和\",{\"1\":{\"65\":1}}],[\"绝对值之和\",{\"1\":{\"61\":1,\"63\":1}}],[\"绝大多数流行算法都是model\",{\"1\":{\"86\":1}}],[\"样本效率较高\",{\"1\":{\"88\":1}}],[\"样本效率高得多\",{\"1\":{\"86\":1}}],[\"样本效率低\",{\"1\":{\"86\":1}}],[\"样本利用效率极高\",{\"1\":{\"87\":1}}],[\"样本利用效率低\",{\"1\":{\"87\":1}}],[\"样本利用效率通常较低\",{\"1\":{\"83\":1}}],[\"样本利用效率相对较高\",{\"1\":{\"82\":1}}],[\"适用于环境模型未知或非常复杂的情况\",{\"1\":{\"86\":1}}],[\"适合\",{\"1\":{\"44\":1}}],[\"物体预测\",{\"1\":{\"91\":1}}],[\"物理模型\",{\"1\":{\"86\":1}}],[\"物品\",{\"1\":{\"74\":1}}],[\"专为连续控制任务设计的强大算法\",{\"1\":{\"84\":1}}],[\"比纯策略梯度法更稳定\",{\"1\":{\"84\":1}}],[\"比如线性变换层\",{\"1\":{\"159\":1}}],[\"比如说\",{\"1\":{\"106\":1}}],[\"比如\",{\"1\":{\"72\":2}}],[\"比如monte\",{\"1\":{\"14\":1}}],[\"比如正态分布或者其他对称分布的情况\",{\"1\":{\"14\":1}}],[\"比如随机梯度下降\",{\"1\":{\"12\":1}}],[\"又通过评论家的指导降低了训练的方差\",{\"1\":{\"84\":1}}],[\"既能处理连续动作空间\",{\"1\":{\"84\":1}}],[\"既然无法直接计算真正的后验\",{\"1\":{\"45\":1}}],[\"工作流程\",{\"0\":{\"132\":1},\"1\":{\"84\":1}}],[\"坏多少\",{\"1\":{\"84\":1}}],[\"好多少\",{\"1\":{\"84\":1}}],[\"好的\",{\"1\":{\"49\":1,\"55\":1,\"71\":1,\"80\":1,\"109\":1,\"136\":1,\"164\":1}}],[\"负责处理输入的\",{\"1\":{\"97\":1}}],[\"负责评估演员选择的动作有多好\",{\"1\":{\"84\":1}}],[\"负责根据当前状态选择动作\",{\"1\":{\"84\":1}}],[\"评估者\",{\"1\":{\"101\":1,\"104\":1}}],[\"评论家指导\",{\"1\":{\"88\":1}}],[\"评论家评估该动作并给出反馈\",{\"1\":{\"84\":1}}],[\"评论家告诉演员\",{\"1\":{\"84\":1}}],[\"评论家负责打分和指导\",{\"1\":{\"84\":1}}],[\"评论家\",{\"1\":{\"84\":1,\"88\":1}}],[\"评论家算法\",{\"0\":{\"84\":1}}],[\"评分矩阵进行\",{\"1\":{\"74\":1}}],[\"演员的策略迅速变得\",{\"1\":{\"103\":1}}],[\"演员行动\",{\"1\":{\"88\":1}}],[\"演员根据反馈更新策略\",{\"1\":{\"84\":1}}],[\"演员根据这个更明确的指导来优化自己的行为策略\",{\"1\":{\"84\":1}}],[\"演员做出动作\",{\"1\":{\"84\":1}}],[\"演员负责行动\",{\"1\":{\"84\":1}}],[\"演员\",{\"0\":{\"84\":1},\"1\":{\"84\":1,\"88\":1,\"103\":1,\"106\":1}}],[\"能学随机策略\",{\"1\":{\"88\":1}}],[\"能处理连续动作\",{\"1\":{\"88\":1}}],[\"能够学习随机策略\",{\"1\":{\"83\":1}}],[\"能够有效处理图像数据\",{\"0\":{\"31\":1}}],[\"能很好地处理连续动作空间\",{\"1\":{\"83\":1}}],[\"执行动作a的概率是多少\",{\"1\":{\"83\":1}}],[\"策略只能从\",{\"1\":{\"106\":1}}],[\"策略迅速收敛\",{\"1\":{\"106\":1}}],[\"策略就从\",{\"1\":{\"106\":1}}],[\"策略比例\",{\"1\":{\"106\":1}}],[\"策略熵\",{\"1\":{\"104\":1}}],[\"策略的多样性消失了\",{\"1\":{\"103\":1}}],[\"策略\",{\"1\":{\"98\":1}}],[\"策略更新通常更平滑\",{\"1\":{\"83\":1}}],[\"策略π\",{\"1\":{\"83\":1}}],[\"策略函数\",{\"1\":{\"83\":1}}],[\"代表算法\",{\"1\":{\"82\":1,\"83\":1,\"84\":1,\"86\":1,\"87\":2,\"88\":1}}],[\"贪心策略\",{\"1\":{\"82\":1}}],[\"决策方式\",{\"1\":{\"82\":1,\"83\":1}}],[\"决定\",{\"1\":{\"61\":1}}],[\"状态方程\",{\"1\":{\"110\":1}}],[\"状态空间模型\",{\"0\":{\"110\":1}}],[\"状态\",{\"1\":{\"82\":1,\"110\":1,\"115\":1}}],[\"长序列处理能力\",{\"0\":{\"182\":1}}],[\"长期来看能获得多少奖励\",{\"1\":{\"82\":1}}],[\"长度为\",{\"1\":{\"60\":1}}],[\"长度\",{\"1\":{\"58\":1}}],[\"智能体尝试一些新的\",{\"1\":{\"103\":1}}],[\"智能体执行它当前认为最好的动作\",{\"1\":{\"103\":1}}],[\"智能体\",{\"1\":{\"82\":1}}],[\"打破了\",{\"1\":{\"115\":1}}],[\"打分函数\",{\"1\":{\"82\":1}}],[\"打通梯度流\",{\"1\":{\"26\":1}}],[\"打通梯度路径\",{\"0\":{\"24\":1}}],[\"价值函数q\",{\"1\":{\"82\":1}}],[\"价值函数\",{\"1\":{\"82\":1}}],[\"此外\",{\"1\":{\"80\":1}}],[\"此时\",{\"1\":{\"30\":1,\"52\":1,\"106\":1,\"148\":1}}],[\"强行中心化到0的意义可能被后续的激活函数削弱\",{\"1\":{\"170\":1}}],[\"强制策略进行\",{\"1\":{\"106\":1}}],[\"强制它进行探索\",{\"1\":{\"104\":1}}],[\"强大的长距离依赖建模能力\",{\"1\":{\"133\":1}}],[\"强大的序列压缩和表示\",{\"1\":{\"115\":1}}],[\"强大的序列建模能力释放到了计算机视觉领域\",{\"1\":{\"31\":1}}],[\"强大的性能\",{\"1\":{\"100\":1}}],[\"强化学习\",{\"1\":{\"80\":1}}],[\"89e3\",{\"1\":{\"152\":1}}],[\"89e1\",{\"1\":{\"152\":1}}],[\"89​​\",{\"1\":{\"152\":1}}],[\"89​\",{\"1\":{\"152\":1}}],[\"8​​\",{\"1\":{\"126\":1}}],[\"80\",{\"1\":{\"126\":1}}],[\"8−0\",{\"1\":{\"126\":1}}],[\"85−0\",{\"1\":{\"126\":1}}],[\"8\",{\"1\":{\"119\":1}}],[\"8f4a\",{\"1\":{\"79\":1}}],[\"81\",{\"1\":{\"61\":1}}],[\"再加上β\",{\"1\":{\"174\":1}}],[\"再进行缩放\",{\"1\":{\"166\":1}}],[\"再输入到\",{\"1\":{\"163\":1}}],[\"再也不去尝试学习\",{\"1\":{\"103\":1}}],[\"再到图像处理的众多机器学习应用提供了坚实的理论基础和高效的算法实现\",{\"1\":{\"76\":1}}],[\"再向北走\",{\"1\":{\"59\":1}}],[\"揭示潜在关系\",{\"1\":{\"76\":1}}],[\"过早的策略收敛\",{\"1\":{\"103\":1}}],[\"过滤掉不重要或有害的细节\",{\"1\":{\"76\":1}}],[\"过拟合\",{\"1\":{\"65\":1}}],[\"预测下一个词的对数概率\",{\"1\":{\"159\":1}}],[\"预测器\",{\"1\":{\"97\":1}}],[\"预测\",{\"1\":{\"75\":1}}],[\"预测缺失的评分\",{\"1\":{\"74\":1}}],[\"浪漫\",{\"1\":{\"75\":2}}],[\"浪漫喜剧爱好者\",{\"1\":{\"72\":1}}],[\"主成分分析\",{\"1\":{\"74\":1}}],[\"主要由以下三个部分组成\",{\"1\":{\"92\":1}}],[\"主要目的\",{\"1\":{\"70\":1,\"135\":1}}],[\"主要用于处理梯度消失或梯度爆炸问题\",{\"1\":{\"174\":1}}],[\"主要用于从复杂的高维度分布中抽样\",{\"1\":{\"15\":1}}],[\"主要用途\",{\"1\":{\"53\":1,\"63\":1}}],[\"主要基于以下两个关键原因\",{\"1\":{\"23\":1}}],[\"主要适用于对于分布形态相对简单\",{\"1\":{\"14\":1}}],[\"压缩\",{\"1\":{\"74\":1}}],[\"压缩或部署需求\",{\"1\":{\"70\":1}}],[\"话题建模\",{\"1\":{\"74\":1}}],[\"矩阵加权求和\",{\"1\":{\"149\":1}}],[\"矩阵\",{\"1\":{\"74\":2,\"162\":1}}],[\"矩阵补全\",{\"1\":{\"73\":1,\"74\":1}}],[\"文档检索\",{\"1\":{\"74\":1}}],[\"文档\",{\"1\":{\"74\":1}}],[\"词典长度\",{\"1\":{\"163\":1}}],[\"词\",{\"1\":{\"74\":1,\"159\":2}}],[\"词语\",{\"1\":{\"73\":1}}],[\"推理速度提升10倍+\",{\"1\":{\"189\":1}}],[\"推理阶段\",{\"1\":{\"187\":1}}],[\"推理时\",{\"1\":{\"115\":1}}],[\"推理等任务上媲美甚至超越\",{\"1\":{\"115\":1}}],[\"推理模式\",{\"1\":{\"115\":1}}],[\"推动了该领域向更简洁\",{\"1\":{\"93\":1}}],[\"推荐系统\",{\"0\":{\"75\":1},\"1\":{\"74\":1}}],[\"推断网络\",{\"1\":{\"18\":1}}],[\"很多缺失值\",{\"1\":{\"73\":1}}],[\"很多系数为0\",{\"1\":{\"61\":1}}],[\"著名的推荐系统算法\",{\"1\":{\"73\":1}}],[\"潜在语义\",{\"1\":{\"75\":1,\"76\":1}}],[\"潜在语义分析\",{\"1\":{\"73\":1}}],[\"潜在空间\",{\"1\":{\"73\":1}}],[\"里观察数据\",{\"1\":{\"73\":1}}],[\"张成的空间\",{\"1\":{\"73\":1}}],[\"小型\",{\"1\":{\"186\":1}}],[\"小步慢跑\",{\"1\":{\"106\":1}}],[\"小的奇异值通常对应数据中的噪声或无关紧要的细节\",{\"1\":{\"73\":1}}],[\"小值\",{\"1\":{\"61\":1}}],[\"列\",{\"1\":{\"73\":1}}],[\"列是电影\",{\"1\":{\"72\":1}}],[\"降低学习难度\",{\"1\":{\"128\":1}}],[\"降低方差\",{\"1\":{\"26\":1}}],[\"降维后的特征矩阵\",{\"1\":{\"74\":1}}],[\"降维\",{\"1\":{\"74\":2,\"76\":1}}],[\"降维与压缩\",{\"1\":{\"73\":1}}],[\"衍生出几个在机器学习中至关重要的能力\",{\"1\":{\"73\":1}}],[\"告诉你这些基本成分的配方\",{\"1\":{\"72\":1}}],[\"告诉你这个物体由哪些基本成分构成\",{\"1\":{\"72\":1}}],[\"告诉你每种基本成分的含量有多少\",{\"1\":{\"72\":1}}],[\"喜剧程度\",{\"1\":{\"72\":1}}],[\"科幻迷\",{\"1\":{\"75\":1}}],[\"科幻\",{\"1\":{\"75\":3}}],[\"科幻程度\",{\"1\":{\"72\":1}}],[\"科幻电影爱好者\",{\"1\":{\"72\":1}}],[\"电影c是负值\",{\"1\":{\"75\":1}}],[\"电影c\",{\"1\":{\"75\":1}}],[\"电影b\",{\"1\":{\"75\":1}}],[\"电影a\",{\"1\":{\"75\":1}}],[\"电影\",{\"1\":{\"73\":1}}],[\"电影类型\",{\"1\":{\"72\":1}}],[\"电影评分矩阵\",{\"1\":{\"72\":1,\"75\":1}}],[\"右奇异向量矩阵的转置\",{\"1\":{\"72\":1}}],[\"奇异值的大小衡量了其对应的\",{\"1\":{\"72\":1}}],[\"奇异值矩阵\",{\"1\":{\"72\":1}}],[\"≥\",{\"1\":{\"72\":4}}],[\"≥0\",{\"1\":{\"46\":1,\"52\":1}}],[\"值得注意的是\",{\"1\":{\"97\":1}}],[\"值是评分\",{\"1\":{\"72\":1}}],[\"值域\",{\"1\":{\"53\":1}}],[\"行为指南\",{\"1\":{\"83\":1,\"88\":1}}],[\"行为模式\",{\"1\":{\"47\":1}}],[\"行\",{\"1\":{\"73\":1}}],[\"行是用户\",{\"1\":{\"72\":1}}],[\"拆解数据的终极工具\",{\"0\":{\"72\":1}}],[\"彻底解释清楚奇异值分解\",{\"1\":{\"71\":1}}],[\"千万不要忽视dropout\",{\"1\":{\"70\":1}}],[\"甚至更常用\",{\"1\":{\"70\":1}}],[\"甚至可能无法收敛\",{\"1\":{\"25\":1}}],[\"甚至可能偏离正态分布\",{\"1\":{\"20\":1}}],[\"提高模型的泛化性能\",{\"1\":{\"174\":1}}],[\"提高模型的训练速度和性能\",{\"1\":{\"174\":1}}],[\"提高泛化能力\",{\"1\":{\"70\":1}}],[\"提供信息\",{\"1\":{\"137\":1}}],[\"提问\",{\"1\":{\"137\":1}}],[\"提问者\",{\"1\":{\"137\":1}}],[\"提取聚合信息\",{\"1\":{\"30\":1}}],[\"至关重要\",{\"1\":{\"70\":1,\"83\":1}}],[\"至少在训练开始时\",{\"1\":{\"20\":1}}],[\"技术\",{\"1\":{\"70\":1}}],[\"早停\",{\"1\":{\"69\":1}}],[\"裁剪\",{\"1\":{\"69\":1}}],[\"噪声\",{\"1\":{\"69\":1}}],[\"批量归一化\",{\"1\":{\"69\":1}}],[\"暂时屏蔽\",{\"1\":{\"69\":1}}],[\"丢弃\",{\"1\":{\"69\":1}}],[\"研究者们提出了一系列改进模型\",{\"1\":{\"93\":1}}],[\"研究者们发明了更多样\",{\"1\":{\"69\":1}}],[\"研究表明\",{\"1\":{\"31\":1}}],[\"61\",{\"1\":{\"126\":1}}],[\"651\",{\"1\":{\"126\":1}}],[\"64\",{\"1\":{\"119\":1}}],[\"60\",{\"1\":{\"106\":1,\"126\":1}}],[\"6\",{\"1\":{\"68\":1,\"161\":1,\"162\":1}}],[\"那个\",{\"1\":{\"159\":1}}],[\"那个动物没有过马路\",{\"1\":{\"131\":1}}],[\"那里稳定地获得还不错的评估分数\",{\"1\":{\"103\":1}}],[\"那就是\",{\"1\":{\"101\":1}}],[\"那样的直接参数\",{\"1\":{\"68\":1}}],[\"那么雅可比矩阵\",{\"1\":{\"157\":1}}],[\"那么\",{\"1\":{\"52\":1,\"112\":1,\"157\":1}}],[\"那么将会得到\",{\"1\":{\"30\":1}}],[\"那么无论采样的\",{\"1\":{\"25\":1}}],[\"那么这一层的权重的分布可能就会偏离正态分布\",{\"1\":{\"20\":1}}],[\"那么我们接受新的权重值\",{\"1\":{\"15\":1}}],[\"那么就用这个新的权重值更新神经网络的权重\",{\"1\":{\"15\":1}}],[\"那么就接受新的权重值\",{\"1\":{\"15\":1}}],[\"那么在贝叶斯推理中\",{\"1\":{\"12\":1}}],[\"嵌入式设备部署轻量模型\",{\"1\":{\"189\":1}}],[\"嵌入式设备等资源受限的环境中\",{\"1\":{\"68\":1}}],[\"嵌入向量\",{\"1\":{\"29\":1,\"30\":1}}],[\"掉大量不重要的神经元连接\",{\"1\":{\"68\":1}}],[\"剪枝\",{\"1\":{\"68\":1}}],[\"几乎所有成功的深度学习模型\",{\"1\":{\"67\":1}}],[\"几何形状\",{\"1\":{\"61\":1}}],[\"几何意义\",{\"0\":{\"60\":1}}],[\"λ\",{\"1\":{\"67\":1}}],[\"`weight\",{\"1\":{\"67\":1}}],[\"防止权重过大\",{\"1\":{\"70\":1}}],[\"防止权重变得过大\",{\"1\":{\"67\":1}}],[\"防止模型过度记忆训练集\",{\"1\":{\"69\":1}}],[\"防止过拟合\",{\"1\":{\"12\":1,\"63\":1,\"70\":1}}],[\"乘以一个小于1的因子\",{\"1\":{\"67\":1}}],[\"还会影响所有其他权重\",{\"1\":{\"157\":1}}],[\"还会额外将参数值向零缩小一点点\",{\"1\":{\"67\":1}}],[\"还是\",{\"1\":{\"131\":1}}],[\"还是rl中的过早收敛\",{\"1\":{\"104\":1}}],[\"还额外增加一项\",{\"1\":{\"104\":1}}],[\"还看一批样本的统计特征\",{\"1\":{\"104\":1}}],[\"还不错\",{\"1\":{\"103\":1,\"106\":1}}],[\"还有两个重要的正交维度\",{\"1\":{\"80\":1}}],[\"还有另一种处理随机节点梯度的技术\",{\"1\":{\"25\":1}}],[\"还有一个描述权重不确定性的分布\",{\"1\":{\"12\":1}}],[\"结构复杂\",{\"1\":{\"88\":1}}],[\"结构更复杂\",{\"1\":{\"84\":1}}],[\"结构更复杂的模型上\",{\"1\":{\"65\":1}}],[\"结合在了一起\",{\"1\":{\"115\":1}}],[\"结合两者优点\",{\"1\":{\"88\":1}}],[\"结合了两者的优点\",{\"1\":{\"84\":1}}],[\"结论\",{\"1\":{\"70\":1,\"104\":1,\"106\":1}}],[\"结果\",{\"1\":{\"139\":1,\"159\":1}}],[\"结果很可能被困在一个小山峰上\",{\"1\":{\"108\":1}}],[\"结果只会生成同一张网红脸\",{\"1\":{\"102\":1}}],[\"结果必然相同\",{\"1\":{\"52\":1}}],[\"结果可能是在两个峰之间形成一个宽而不准的\",{\"1\":{\"47\":1}}],[\"产生稀疏权重\",{\"1\":{\"70\":1}}],[\"产生稀疏模型\",{\"1\":{\"65\":1}}],[\"产生稀疏性\",{\"1\":{\"63\":1}}],[\"迫使所有权重都趋向于变小\",{\"1\":{\"65\":1}}],[\"损失函数就会产生一个梯度\",{\"1\":{\"159\":1}}],[\"损失\",{\"1\":{\"63\":2}}],[\"解释\",{\"1\":{\"155\":1,\"156\":1}}],[\"解决指代问题\",{\"0\":{\"131\":1}}],[\"解决\",{\"1\":{\"113\":1}}],[\"解决方案的思路也是相通的\",{\"1\":{\"104\":1}}],[\"解决的问题\",{\"1\":{\"74\":1}}],[\"解码器的输出被输入到一个线性层中\",{\"1\":{\"163\":1}}],[\"解码器的输出会经过一个简单的前馈网络\",{\"1\":{\"92\":1}}],[\"解码器的self\",{\"1\":{\"134\":1}}],[\"解码器中的masked\",{\"1\":{\"134\":1}}],[\"解码器通过自注意力和交叉注意力机制来解码出每个物体的位置和类别信息\",{\"1\":{\"92\":1}}],[\"解码器\",{\"1\":{\"92\":2}}],[\"解更平滑\",{\"1\":{\"63\":1}}],[\"解读\",{\"1\":{\"51\":1,\"75\":1}}],[\"形状为\",{\"1\":{\"72\":1}}],[\"形状\",{\"1\":{\"63\":1}}],[\"形成了后验分布\",{\"1\":{\"18\":2}}],[\"普通attention\",{\"1\":{\"129\":1,\"132\":1,\"135\":1}}],[\"普通策略梯度\",{\"1\":{\"108\":1}}],[\"普通\",{\"1\":{\"63\":1}}],[\"绰号\",{\"1\":{\"63\":1}}],[\"岭回归\",{\"1\":{\"62\":1,\"64\":2,\"65\":1,\"70\":2}}],[\"金融数据中偶尔出现的极端波动\",{\"1\":{\"62\":1}}],[\"便于部署在手机\",{\"1\":{\"68\":1}}],[\"便于使用梯度下降法等优化算法\",{\"1\":{\"62\":1}}],[\"便于解释和比较\",{\"1\":{\"53\":1}}],[\"均方误差\",{\"1\":{\"62\":1}}],[\"均值寻求\",{\"1\":{\"47\":1}}],[\"巨大值\",{\"1\":{\"61\":1}}],[\"≈\",{\"1\":{\"61\":1,\"152\":2}}],[\"≈0+0+0\",{\"1\":{\"54\":1}}],[\"≈0\",{\"1\":{\"54\":1}}],[\"把不重要的特征的权重直接压缩到\",{\"1\":{\"62\":1}}],[\"把\",{\"1\":{\"61\":1}}],[\"²\",{\"1\":{\"61\":4,\"174\":1}}],[\"稍复杂\",{\"1\":{\"61\":1}}],[\"系数都很小但非零\",{\"1\":{\"61\":1}}],[\"倾向于产生稠密解\",{\"1\":{\"61\":1}}],[\"倾向于产生稀疏解\",{\"1\":{\"61\":1}}],[\"稀疏性\",{\"1\":{\"61\":1,\"74\":1}}],[\"光滑\",{\"1\":{\"61\":1}}],[\"光滑的圆形\",{\"1\":{\"60\":1}}],[\"圆形\",{\"1\":{\"61\":1,\"63\":1}}],[\"菱形\",{\"1\":{\"61\":1,\"63\":1}}],[\"所有操作可微\",{\"1\":{\"153\":1}}],[\"所有公式用\",{\"1\":{\"117\":1}}],[\"所有到原点曼哈顿距离为\",{\"1\":{\"60\":1}}],[\"所有到原点距离为\",{\"1\":{\"60\":1}}],[\"所以还需要继续反向传播通过这个缩放操作\",{\"1\":{\"157\":1}}],[\"所以理解其计算流程更能体会其精髓\",{\"1\":{\"136\":1}}],[\"所以某个维度上的巨大值不会完全主导整个距离的计算\",{\"1\":{\"61\":1}}],[\"所以它可以被用作一个有效的\",{\"1\":{\"52\":1}}],[\"所以两个非负kl散度的平均值也必然非负\",{\"1\":{\"52\":1}}],[\"所以\",{\"1\":{\"46\":1,\"48\":1,\"52\":1,\"72\":2}}],[\"所以可以直接通过反向传播来计算这些梯度\",{\"1\":{\"18\":1}}],[\"所以这个优化问题可以等价为最大化证据下界\",{\"1\":{\"13\":1}}],[\"格\",{\"1\":{\"59\":2}}],[\"先向东走\",{\"1\":{\"59\":1}}],[\"先验分布是必需的\",{\"1\":{\"18\":1}}],[\"走到\",{\"1\":{\"59\":1}}],[\"走到点\",{\"1\":{\"56\":1}}],[\"78​e1\",{\"1\":{\"152\":1}}],[\"78e7\",{\"1\":{\"152\":1}}],[\"78e2\",{\"1\":{\"152\":1}}],[\"783\",{\"1\":{\"152\":1}}],[\"7−0\",{\"1\":{\"126\":1}}],[\"7​0\",{\"1\":{\"126\":1}}],[\"74​\",{\"1\":{\"126\":1}}],[\"7382389058494562354\",{\"1\":{\"93\":1}}],[\"76069940\",{\"1\":{\"79\":1}}],[\"768\",{\"1\":{\"30\":1}}],[\"7\",{\"1\":{\"59\":1,\"186\":1}}],[\"曼哈顿范数\",{\"0\":{\"59\":1}}],[\"曼哈顿距离\",{\"1\":{\"56\":1,\"61\":1,\"63\":1}}],[\"向量中\",{\"1\":{\"177\":1}}],[\"向量的凸组合\",{\"1\":{\"151\":1}}],[\"向量的维度\",{\"1\":{\"139\":1}}],[\"向量相乘\",{\"1\":{\"140\":1}}],[\"向量\",{\"1\":{\"58\":1,\"137\":3}}],[\"√d\",{\"1\":{\"132\":1}}],[\"√25\",{\"1\":{\"58\":1}}],[\"√\",{\"1\":{\"58\":3,\"61\":5}}],[\"公式\",{\"1\":{\"58\":1}}],[\"欧几里得范数\",{\"0\":{\"58\":1}}],[\"欧几里得距离\",{\"1\":{\"56\":1,\"61\":1,\"63\":1}}],[\"距离较远的词之间的依赖关系容易被弱化\",{\"1\":{\"133\":1}}],[\"距离几乎完全由巨大的异常值\",{\"1\":{\"61\":1}}],[\"距离\",{\"1\":{\"56\":1,\"61\":6,\"63\":1}}],[\"想象一下\",{\"1\":{\"72\":1,\"82\":1,\"106\":1}}],[\"想象一下你在一个城市里\",{\"1\":{\"56\":1}}],[\"想象两个点\",{\"1\":{\"61\":1}}],[\"想象真实后验\",{\"1\":{\"47\":1}}],[\"47​e1\",{\"1\":{\"152\":1}}],[\"47+e7\",{\"1\":{\"152\":2}}],[\"471\",{\"1\":{\"152\":1}}],[\"47dd\",{\"1\":{\"79\":1}}],[\"404\",{\"1\":{\"193\":1}}],[\"40\",{\"1\":{\"126\":3}}],[\"4²\",{\"1\":{\"58\":1}}],[\"4\",{\"0\":{\"61\":1,\"113\":1,\"183\":1},\"1\":{\"58\":2,\"59\":4,\"61\":3,\"75\":2}}],[\"45\",{\"1\":{\"54\":6}}],[\"4d3f\",{\"1\":{\"9\":1}}],[\"无需原始数据\",{\"1\":{\"190\":1}}],[\"无需学习\",{\"1\":{\"181\":1}}],[\"无需大量数据增强\",{\"1\":{\"100\":1}}],[\"无需后处理\",{\"1\":{\"93\":1}}],[\"无需任何后处理步骤\",{\"1\":{\"91\":1}}],[\"无论两个词在序列中的距离有多远\",{\"1\":{\"133\":1}}],[\"无论是gan中的模式坍塌\",{\"1\":{\"104\":1}}],[\"无论是线性模型还是深度神经网络\",{\"1\":{\"65\":1}}],[\"无论给它什么随机噪声作为输入\",{\"1\":{\"102\":1}}],[\"无论好坏\",{\"1\":{\"87\":1}}],[\"无模型\",{\"0\":{\"86\":1},\"1\":{\"86\":1}}],[\"无法直接处理相对位置\",{\"1\":{\"180\":1}}],[\"无法直接观测到的模式\",{\"1\":{\"76\":1}}],[\"无法并行\",{\"1\":{\"133\":1}}],[\"无法再使用全局卷积进行并行计算\",{\"1\":{\"114\":1}}],[\"无法根据内容动态地选择保留或忽略信息\",{\"1\":{\"113\":1}}],[\"无法给生成器提供\",{\"1\":{\"102\":1}}],[\"无法学习随机策略\",{\"1\":{\"82\":1}}],[\"无定义\",{\"1\":{\"54\":2}}],[\"无关的常数\",{\"1\":{\"46\":1}}],[\"无关的\",{\"1\":{\"22\":1}}],[\"无关的简单分布\",{\"1\":{\"22\":1}}],[\"曾是生成对抗网络\",{\"1\":{\"53\":1}}],[\"衡量两个分布之间的相似性\",{\"1\":{\"53\":1}}],[\"衡量信息损失\",{\"1\":{\"53\":1}}],[\"衡量了我们近似分布与真实后验的差距\",{\"1\":{\"46\":1}}],[\"源序列的表示\",{\"1\":{\"132\":1}}],[\"源于\",{\"1\":{\"70\":1}}],[\"源于信息论\",{\"1\":{\"53\":1}}],[\"源语言句子和目标语言句子\",{\"1\":{\"129\":1}}],[\"源语言句子\",{\"1\":{\"29\":1}}],[\"到小型轻量模型\",{\"1\":{\"185\":1}}],[\"到向量上\",{\"1\":{\"177\":1}}],[\"到\",{\"1\":{\"53\":3,\"106\":1}}],[\"到最终的损失函数\",{\"1\":{\"24\":1}}],[\"避免学生模型过拟合硬标签中的噪声\",{\"1\":{\"188\":1}}],[\"避免判别器变得过于\",{\"1\":{\"104\":1}}],[\"避免了繁重的像素级重建\",{\"1\":{\"100\":1}}],[\"避免了重复预测\",{\"1\":{\"91\":1}}],[\"避免了零概率问题\",{\"1\":{\"53\":1}}],[\"避免在\",{\"1\":{\"47\":2}}],[\"平衡两项损失的权重\",{\"1\":{\"187\":1}}],[\"平方和\",{\"1\":{\"65\":1}}],[\"平方和的平方根\",{\"1\":{\"61\":1,\"63\":1}}],[\"平均绝对误差\",{\"1\":{\"62\":1}}],[\"平均分布\",{\"1\":{\"50\":1}}],[\"平滑了\",{\"1\":{\"53\":1}}],[\"趋于无穷\",{\"1\":{\"53\":1}}],[\"时为1\",{\"1\":{\"157\":1}}],[\"时间\",{\"1\":{\"115\":2}}],[\"时\",{\"1\":{\"53\":1,\"67\":1,\"131\":1,\"144\":1}}],[\"=dkl​\",{\"1\":{\"53\":1}}],[\"仅包含正确类别信息\",{\"1\":{\"186\":1}}],[\"仅缩放\",{\"1\":{\"172\":1}}],[\"仅仅是\",{\"1\":{\"52\":1}}],[\"仅使用编码器\",{\"1\":{\"31\":1}}],[\"反科幻\",{\"1\":{\"75\":2}}],[\"反向传播使注意力机制能够自动学习并优化\",{\"1\":{\"159\":1}}],[\"反向传播的目标是计算最终损失函数\",{\"1\":{\"154\":1}}],[\"反向传播\",{\"1\":{\"68\":1,\"174\":1}}],[\"反向kl\",{\"1\":{\"48\":1}}],[\"反之\",{\"1\":{\"52\":1,\"106\":1}}],[\"✅\",{\"1\":{\"52\":4}}],[\"≤djs​\",{\"1\":{\"52\":1}}],[\"同时可以使用更大的学习率\",{\"1\":{\"174\":1}}],[\"同时保留了数据的核心结构\",{\"1\":{\"73\":1}}],[\"同一模型同时作为教师和学生\",{\"1\":{\"190\":1}}],[\"同一个词\",{\"1\":{\"137\":1}}],[\"同一性\",{\"1\":{\"52\":2}}],[\"同理\",{\"1\":{\"75\":1,\"140\":1,\"158\":1}}],[\"同样能够将输入向量缩放到一个单位尺度附近\",{\"1\":{\"171\":1}}],[\"同样由\",{\"1\":{\"162\":1}}],[\"同样根据\",{\"1\":{\"156\":1}}],[\"同样地\",{\"1\":{\"138\":1}}],[\"同样向量\",{\"1\":{\"59\":1}}],[\"同样在图像块序列的最前面添加了一个可学习的\",{\"1\":{\"29\":1}}],[\"散度\",{\"1\":{\"52\":1}}],[\"混合而成\",{\"1\":{\"51\":1}}],[\"什么是jensen\",{\"0\":{\"50\":1}}],[\"正弦位置编码\",{\"1\":{\"180\":1}}],[\"正弦位置编码和学习的位置编码\",{\"1\":{\"179\":1}}],[\"正在优化的策略\",{\"1\":{\"106\":1}}],[\"正则化作用\",{\"1\":{\"188\":1}}],[\"正则化被直接集成到了优化器里\",{\"1\":{\"66\":1}}],[\"正则化\",{\"1\":{\"62\":2,\"64\":1}}],[\"正因为js散度同时满足了以上四个条件\",{\"1\":{\"52\":1}}],[\"正是基于以上特性\",{\"1\":{\"62\":1}}],[\"正是为了克服kl散度的一些主要缺点而设计的\",{\"1\":{\"49\":1}}],[\"正是因为它与elbo有着不可分割的\",{\"1\":{\"48\":1}}],[\"正态分布的参数是均值μ和标准差σ\",{\"1\":{\"18\":1}}],[\"你在这个时间步关注了错误的信息\",{\"1\":{\"159\":1}}],[\"你可以将\",{\"1\":{\"110\":1}}],[\"你可以把它理解为代表了列空间的\",{\"1\":{\"72\":1}}],[\"你可以把它理解为代表了行空间的\",{\"1\":{\"72\":1}}],[\"你可以把它想象成一把测量\",{\"1\":{\"55\":1}}],[\"你应该更多地关注输入序列中的\",{\"1\":{\"159\":1}}],[\"你应该去探索其他类型样本\",{\"1\":{\"102\":1}}],[\"你应该默认使用l2正则化\",{\"1\":{\"70\":1}}],[\"你的观察是完全正确的\",{\"1\":{\"104\":1}}],[\"你的这个洞察非常深刻\",{\"1\":{\"101\":1}}],[\"你的直觉是正确的\",{\"1\":{\"48\":1}}],[\"你这个动作是好是坏\",{\"1\":{\"84\":1}}],[\"你无法为无限个动作都创建一个得分\",{\"1\":{\"82\":1}}],[\"你有一个复杂的数据矩阵\",{\"1\":{\"72\":1}}],[\"你都能找到权重衰减的身影\",{\"1\":{\"67\":1}}],[\"你不能斜着穿过大楼\",{\"1\":{\"56\":1}}],[\"你走过的总路程就是\",{\"1\":{\"56\":1}}],[\"你触及了变分推断\",{\"1\":{\"44\":1}}],[\"二\",{\"0\":{\"168\":1}}],[\"二次复杂度的瓶颈\",{\"1\":{\"115\":1}}],[\"二分图匹配\",{\"1\":{\"91\":1}}],[\"二是在最小化近似分布与真实后验之间的kl散度\",{\"1\":{\"48\":1}}],[\"二阶导数矩阵\",{\"1\":{\"14\":1}}],[\"非常有效\",{\"1\":{\"172\":1}}],[\"非常高\",{\"1\":{\"106\":1,\"131\":1}}],[\"非常好的问题\",{\"1\":{\"154\":1}}],[\"非常好\",{\"1\":{\"105\":1}}],[\"非常好且深刻的问题\",{\"1\":{\"44\":1}}],[\"非常简单\",{\"1\":{\"67\":1}}],[\"非对称\",{\"1\":{\"53\":1}}],[\"非对称性的优势\",{\"1\":{\"48\":1}}],[\"非负性\",{\"1\":{\"52\":2}}],[\"自蒸馏\",{\"1\":{\"190\":1}}],[\"自身的信息\",{\"1\":{\"140\":1}}],[\"自我审视\",{\"0\":{\"130\":1}}],[\"自注意力\",{\"0\":{\"129\":1},\"1\":{\"129\":1}}],[\"自注意力和前馈网络\",{\"1\":{\"30\":1}}],[\"自然语言处理\",{\"1\":{\"74\":1}}],[\"自然而然地成为了elbo的一部分\",{\"1\":{\"48\":1}}],[\"被用来计算损失\",{\"1\":{\"159\":1}}],[\"被用于计算损失函数\",{\"1\":{\"24\":1}}],[\"被检索\",{\"1\":{\"137\":1}}],[\"被检索项\",{\"1\":{\"137\":1}}],[\"被选为\",{\"1\":{\"48\":1}}],[\"根据论文\",{\"1\":{\"171\":1}}],[\"根据前向公式\",{\"1\":{\"155\":1}}],[\"根据输入更新自己的记忆\",{\"1\":{\"110\":1}}],[\"根据策略函数输出的概率分布进行抽样\",{\"1\":{\"83\":1}}],[\"根据贝叶斯定理\",{\"1\":{\"15\":1,\"45\":1}}],[\"根据贝叶斯规则\",{\"1\":{\"13\":1}}],[\"根本原因\",{\"1\":{\"48\":1}}],[\"驼峰\",{\"1\":{\"47\":1}}],[\"去与句子中的每一个词\",{\"1\":{\"131\":1}}],[\"去除特征间的多重共线性\",{\"1\":{\"74\":1}}],[\"去噪\",{\"1\":{\"73\":1,\"74\":1,\"76\":1}}],[\"去覆盖两个波峰\",{\"1\":{\"47\":1}}],[\"去拟合其中一个主要的波峰\",{\"1\":{\"47\":1}}],[\"会按以下方式转换\",{\"1\":{\"157\":1}}],[\"会呈现出不同的侧面\",{\"1\":{\"137\":1}}],[\"会分别与这三个矩阵相乘\",{\"1\":{\"137\":1}}],[\"会迅速增长并超过\",{\"1\":{\"106\":1}}],[\"会像打了兴奋剂一样\",{\"1\":{\"106\":1}}],[\"会导致\",{\"1\":{\"86\":1}}],[\"会转移到哪个新状态s\",{\"1\":{\"86\":1}}],[\"会选择那个预估得分最高的动作\",{\"1\":{\"82\":1}}],[\"会极大地放大异常值的影响\",{\"1\":{\"61\":1}}],[\"会遇到除零错误\",{\"1\":{\"54\":1}}],[\"会迫使单个高斯分布\",{\"1\":{\"47\":1}}],[\"会让近似分布\",{\"1\":{\"47\":1}}],[\"会让\",{\"1\":{\"47\":2}}],[\"有\",{\"1\":{\"178\":1}}],[\"有正则化效果\",{\"1\":{\"174\":1}}],[\"有时更优\",{\"1\":{\"172\":1}}],[\"有时被用来形成一个更符合直观欧几里得空间性质的距离函数\",{\"1\":{\"52\":1}}],[\"有一个非常优雅的结果\",{\"1\":{\"157\":1}}],[\"有更多的机会发现并转向那条通往真正顶峰的道路\",{\"1\":{\"108\":1}}],[\"有ppo\",{\"1\":{\"106\":1}}],[\"有模型\",{\"0\":{\"86\":1},\"1\":{\"86\":1}}],[\"有两个网络需要训练和协调\",{\"1\":{\"84\":1}}],[\"有两个波峰\",{\"1\":{\"47\":1}}],[\"有哪些用户类型\",{\"1\":{\"72\":1}}],[\"有选择地使用\",{\"1\":{\"70\":1}}],[\"有尖点\",{\"1\":{\"61\":1}}],[\"有界\",{\"1\":{\"53\":1}}],[\"有定义的点上\",{\"1\":{\"51\":1}}],[\"有概率的区域\",{\"1\":{\"47\":1}}],[\"求期望\",{\"1\":{\"47\":3,\"48\":1}}],[\"必须重新采集\",{\"1\":{\"87\":1}}],[\"必须学习到更加鲁棒的特征\",{\"1\":{\"69\":1}}],[\"必须覆盖所有\",{\"1\":{\"47\":1}}],[\"必须为序列中的每一个嵌入向量\",{\"1\":{\"30\":1}}],[\"要从点\",{\"1\":{\"56\":1}}],[\"要求所有输出之和为1\",{\"1\":{\"157\":1}}],[\"要求\",{\"1\":{\"47\":1}}],[\"要理解为什么有效\",{\"1\":{\"168\":1}}],[\"要理解\",{\"1\":{\"28\":1}}],[\"没有ppo的普通策略梯度\",{\"1\":{\"106\":1}}],[\"没有概率质量\",{\"1\":{\"47\":1}}],[\"没有重参数技巧\",{\"1\":{\"26\":1}}],[\"概率分布更平滑\",{\"1\":{\"187\":1}}],[\"概率选a\",{\"1\":{\"106\":4}}],[\"概率高的地方\",{\"1\":{\"47\":1}}],[\"概率低的地方放置\",{\"1\":{\"47\":1}}],[\"概率密度函数描述了在给定参数的情况下\",{\"1\":{\"18\":1}}],[\"覆盖\",{\"1\":{\"47\":1}}],[\"峰值\",{\"1\":{\"47\":1}}],[\"抓住\",{\"1\":{\"47\":1}}],[\"模式坍塌\",{\"0\":{\"103\":1},\"1\":{\"102\":1,\"103\":1,\"104\":1,\"105\":1,\"106\":1,\"108\":1}}],[\"模式\",{\"1\":{\"72\":1}}],[\"模式寻求\",{\"1\":{\"47\":1}}],[\"模型部署\",{\"1\":{\"189\":1}}],[\"模型中的所有子层\",{\"1\":{\"161\":1}}],[\"模型中的隐变量\",{\"1\":{\"45\":1}}],[\"模型学会了\",{\"1\":{\"159\":1}}],[\"模型就能清晰地知道\",{\"1\":{\"131\":1}}],[\"模型首先通过线性投影为每个时间步生成对应的\",{\"1\":{\"114\":1}}],[\"模型现在可以\",{\"1\":{\"113\":1}}],[\"模型处理所有输入的方式就是固定的\",{\"1\":{\"112\":1}}],[\"模型可以直接输出最终的检测结果\",{\"1\":{\"93\":1}}],[\"模型压缩\",{\"1\":{\"68\":1,\"189\":1}}],[\"模型会更平滑\",{\"1\":{\"62\":1}}],[\"模型性能会持续提升\",{\"1\":{\"31\":1}}],[\"模型至关重要\",{\"1\":{\"30\":1}}],[\"模型维度\",{\"1\":{\"30\":1,\"118\":1}}],[\"模型主要用于处理序列到序列的任务\",{\"1\":{\"29\":1}}],[\"模型\",{\"1\":{\"28\":1,\"29\":1,\"113\":1}}],[\"模型对于新的\",{\"1\":{\"19\":1}}],[\"模型对数据的对数似然的期望\",{\"1\":{\"12\":1}}],[\"模型不确定性\",{\"1\":{\"19\":2}}],[\"模型参数为θ\",{\"1\":{\"18\":1}}],[\"特征蒸馏\",{\"1\":{\"190\":1}}],[\"特征维度\",{\"1\":{\"174\":1}}],[\"特征选择\",{\"1\":{\"62\":1,\"63\":1,\"68\":1}}],[\"特点\",{\"1\":{\"63\":1}}],[\"特性往往能产生更实际\",{\"1\":{\"48\":1}}],[\"特性\",{\"1\":{\"47\":1,\"53\":1,\"61\":1,\"115\":1,\"127\":1,\"135\":1,\"172\":1}}],[\"特别是on\",{\"1\":{\"83\":1}}],[\"特别是off\",{\"1\":{\"82\":1}}],[\"特别是\",{\"1\":{\"13\":1}}],[\"特别是在处理复杂模型或分布时\",{\"1\":{\"13\":1}}],[\"特别是在存在高度相关参数或复杂后验分布的情况下\",{\"1\":{\"13\":1}}],[\"特别是在大数据集和高维参数空间中\",{\"1\":{\"13\":1}}],[\"称为\",{\"1\":{\"74\":1,\"96\":2}}],[\"称为反向kl\",{\"1\":{\"47\":1}}],[\"称为证据\",{\"1\":{\"45\":1}}],[\"证据\",{\"1\":{\"46\":1}}],[\"证据下界\",{\"1\":{\"18\":1}}],[\"移项后得到\",{\"1\":{\"46\":1}}],[\"上的均值和方差\",{\"1\":{\"174\":1}}],[\"上进行了实验\",{\"1\":{\"171\":1}}],[\"上分配注意力\",{\"1\":{\"159\":1}}],[\"上游梯度\",{\"1\":{\"156\":1}}],[\"上述所有计算都可以通过矩阵运算并行完成\",{\"1\":{\"141\":1}}],[\"上述模型是连续时间的\",{\"1\":{\"111\":1}}],[\"上效率极高\",{\"1\":{\"133\":1}}],[\"上非常慢\",{\"1\":{\"114\":1}}],[\"上表现很差\",{\"1\":{\"113\":1}}],[\"上下文感知\",{\"1\":{\"134\":1}}],[\"上下文感知的信息选择\",{\"1\":{\"115\":1}}],[\"上下文向量\",{\"0\":{\"140\":1},\"1\":{\"131\":1,\"154\":1,\"159\":1}}],[\"上下文编码器\",{\"1\":{\"97\":1}}],[\"上下文块\",{\"1\":{\"96\":1,\"97\":1}}],[\"上面的等式就变成了\",{\"1\":{\"46\":1}}],[\"上一步得到的展平向量维度\",{\"1\":{\"30\":1}}],[\"令\",{\"1\":{\"46\":1}}],[\"令牌进行信息聚合\",{\"1\":{\"31\":1}}],[\"令牌和位置信息的嵌入序列会依次通过所有\",{\"1\":{\"30\":1}}],[\"令牌与位置编码\",{\"1\":{\"30\":1}}],[\"令牌对应的输出向量\",{\"1\":{\"29\":1,\"30\":1}}],[\"令牌不代表任何具体的图像块\",{\"1\":{\"29\":1}}],[\"令牌被添加到序列的开头\",{\"1\":{\"29\":1}}],[\"令牌\",{\"1\":{\"29\":1,\"30\":2}}],[\"∥p\",{\"1\":{\"46\":7}}],[\"接收编码器的输出以及一组可学习的位置编码\",{\"1\":{\"92\":1}}],[\"接收来自cnn主干网络的图像特征\",{\"1\":{\"92\":1}}],[\"接近\",{\"1\":{\"45\":1,\"152\":2}}],[\"接受率是新的权重值和当前权重值在目标分布下的概率比\",{\"1\":{\"15\":1}}],[\"找到一个与\",{\"1\":{\"45\":1}}],[\"找到最大后验概率\",{\"1\":{\"14\":1}}],[\"了出来\",{\"1\":{\"44\":1}}],[\"浮现\",{\"1\":{\"44\":1}}],[\"数学公式\",{\"0\":{\"167\":1}}],[\"数学详解\",{\"0\":{\"117\":1}}],[\"数学定义\",{\"0\":{\"51\":1,\"57\":1}}],[\"数学上的本质联系\",{\"1\":{\"48\":1}}],[\"数学上完全等价于最大化elbo\",{\"1\":{\"46\":1}}],[\"数学必然地成为了证据下界\",{\"1\":{\"44\":1}}],[\"数据无关蒸馏\",{\"1\":{\"190\":1}}],[\"数据本质提取器\",{\"1\":{\"76\":1}}],[\"数据降维\",{\"1\":{\"74\":1}}],[\"数据预处理\",{\"1\":{\"74\":1}}],[\"数据点\",{\"1\":{\"73\":1}}],[\"数据矩阵\",{\"1\":{\"72\":1}}],[\"数据增强等其他正则化技术\",{\"1\":{\"70\":1}}],[\"数据增强\",{\"1\":{\"69\":1,\"70\":1}}],[\"数据不确定性可以通过模型的预测分布来表示\",{\"1\":{\"19\":1}}],[\"数据不确定性\",{\"1\":{\"19\":1}}],[\"数据x是已知的\",{\"1\":{\"18\":1}}],[\"^t\",{\"1\":{\"178\":2}}],[\"^\",{\"1\":{\"34\":4}}],[\">0\",{\"1\":{\"53\":1}}],[\">\",{\"0\":{\"67\":1},\"1\":{\"33\":3,\"84\":2,\"106\":1,\"187\":1}}],[\"成为理解模型决策过程的窗口\",{\"1\":{\"141\":1}}],[\"成为\",{\"1\":{\"128\":1}}],[\"成为输入\",{\"1\":{\"113\":1}}],[\"成功地将\",{\"1\":{\"31\":1}}],[\"成一维的向量\",{\"1\":{\"29\":1}}],[\"展示了梯度是如何从输出\",{\"1\":{\"159\":1}}],[\"展现出比\",{\"1\":{\"31\":1}}],[\"展平为一个一维的向量\",{\"1\":{\"30\":1}}],[\"展平\",{\"1\":{\"30\":1}}],[\"灵活性与可扩展性\",{\"1\":{\"31\":1}}],[\"天然的全局感受野\",{\"1\":{\"31\":1}}],[\"全部来自同一个序列x\",{\"1\":{\"135\":1}}],[\"全局依赖\",{\"1\":{\"128\":1}}],[\"全局最优\",{\"1\":{\"108\":1}}],[\"全局信息建模\",{\"1\":{\"93\":1}}],[\"全局感受野\",{\"1\":{\"31\":1}}],[\"全新的输入处理方式\",{\"1\":{\"29\":1}}],[\"处理图像的几个关键挑战\",{\"1\":{\"31\":1}}],[\"处进行二阶泰勒展开\",{\"1\":{\"14\":1}}],[\"送入分类头\",{\"1\":{\"30\":1}}],[\"送入一个简单的分类头\",{\"1\":{\"29\":1}}],[\"当前位置之前的词语信息\",{\"1\":{\"162\":1}}],[\"当模型处理到\",{\"1\":{\"131\":1}}],[\"当新策略试图将动作a的概率大幅提升时\",{\"1\":{\"106\":1}}],[\"当你希望模型对数据中的异常值不敏感时使用\",{\"1\":{\"62\":1}}],[\"当\",{\"1\":{\"53\":1,\"157\":1}}],[\"当我们的近似分布\",{\"1\":{\"46\":1}}],[\"当我们的模型在训练数据上过拟合时\",{\"1\":{\"19\":1}}],[\"当拥有足够大的数据集\",{\"1\":{\"31\":1}}],[\"当序列通过所有\",{\"1\":{\"30\":1}}],[\"层归一化\",{\"1\":{\"164\":1}}],[\"层中转化为概率\",{\"1\":{\"163\":1}}],[\"层组成\",{\"1\":{\"161\":1,\"162\":1}}],[\"层时\",{\"1\":{\"157\":1}}],[\"层后\",{\"1\":{\"30\":1}}],[\"层\",{\"1\":{\"30\":1,\"161\":2,\"162\":2}}],[\"层之后\",{\"1\":{\"29\":1}}],[\"包裹\",{\"1\":{\"117\":1}}],[\"包含更多信息\",{\"1\":{\"186\":1}}],[\"包含所有关键公式的推导和说明\",{\"1\":{\"142\":1}}],[\"包含\",{\"1\":{\"30\":1}}],[\"包括γ和β\",{\"1\":{\"174\":1}}],[\"包括自己\",{\"1\":{\"138\":1}}],[\"包括它自己\",{\"1\":{\"130\":1}}],[\"包括\",{\"1\":{\"30\":1,\"161\":1}}],[\"包括网络的初始化方式\",{\"1\":{\"20\":1}}],[\"包括相关的数学公式\",{\"1\":{\"18\":1}}],[\"包括那些无法解析计算的分布\",{\"1\":{\"13\":1}}],[\"都接了一个\",{\"1\":{\"161\":1}}],[\"都去\",{\"1\":{\"130\":1}}],[\"都是同一个问题的不同表现形式\",{\"1\":{\"104\":1}}],[\"都是用于估计后验分布的技术\",{\"1\":{\"13\":1}}],[\"都过早地停止了探索\",{\"1\":{\"104\":1}}],[\"都内在地容易出现一种核心问题\",{\"1\":{\"101\":1}}],[\"都存进一个\",{\"1\":{\"87\":1}}],[\"都使用了残差连接\",{\"1\":{\"30\":1}}],[\"猫狗相似性\",{\"1\":{\"188\":1}}],[\"猫和狗相似\",{\"1\":{\"186\":1}}],[\"猫\",{\"1\":{\"30\":1,\"186\":1}}],[\"识别出图像中一只猫的耳朵和尾巴虽然相距很远\",{\"1\":{\"30\":1}}],[\"关于\",{\"1\":{\"155\":1,\"158\":2}}],[\"关于输入矩阵\",{\"1\":{\"154\":1}}],[\"关于变分分布参数的梯度\",{\"1\":{\"18\":1}}],[\"关键区别在于\",{\"1\":{\"132\":1}}],[\"关键特性与对比\",{\"0\":{\"61\":1}}],[\"关注多少\",{\"1\":{\"159\":1}}],[\"关注\",{\"1\":{\"130\":1}}],[\"关注并加权融合序列中所有其他元素的信息\",{\"1\":{\"30\":1}}],[\"关系蒸馏\",{\"1\":{\"190\":1}}],[\"关系\",{\"1\":{\"44\":1}}],[\"核心知识\",{\"1\":{\"187\":1}}],[\"核心操作\",{\"1\":{\"172\":1}}],[\"核心能力\",{\"1\":{\"115\":1}}],[\"核心机制\",{\"0\":{\"106\":1}}],[\"核心共性与解决方案\",{\"0\":{\"104\":1}}],[\"核心思想\",{\"0\":{\"130\":1,\"166\":1},\"1\":{\"82\":1,\"83\":1,\"84\":1,\"88\":1}}],[\"核心思想的传承\",{\"0\":{\"65\":1}}],[\"核心三大算法结构\",{\"0\":{\"81\":1}}],[\"核心结构\",{\"1\":{\"30\":1}}],[\"核心要点\",{\"0\":{\"22\":1}}],[\"哪个在中间\",{\"1\":{\"30\":1}}],[\"哪个块在左上角\",{\"1\":{\"30\":1}}],[\"添加一个\",{\"1\":{\"30\":1}}],[\"添加位置编码\",{\"1\":{\"30\":1}}],[\"添加\",{\"1\":{\"30\":2}}],[\"块嵌入\",{\"1\":{\"30\":1}}],[\"维空间分为\",{\"1\":{\"178\":1}}],[\"维的隐藏状态\",{\"1\":{\"110\":1}}],[\"维的向量送入一个分类头\",{\"1\":{\"30\":1}}],[\"维向量\",{\"1\":{\"30\":1}}],[\"维度\",{\"1\":{\"144\":3}}],[\"维度分解\",{\"1\":{\"128\":1}}],[\"维度也是\",{\"1\":{\"30\":1}}],[\"维度鸿沟\",{\"1\":{\"29\":1}}],[\"维\",{\"1\":{\"30\":1}}],[\"维映射到\",{\"1\":{\"30\":1}}],[\"本身是由可学习的权重参数\",{\"1\":{\"159\":1}}],[\"本身是相对平滑的\",{\"1\":{\"25\":1}}],[\"本次更新对这个动作的奖励是有上限的\",{\"1\":{\"106\":1}}],[\"本次更新开始前的策略\",{\"1\":{\"106\":1}}],[\"本质上\",{\"1\":{\"73\":1,\"159\":1}}],[\"本质上是一个全连接层\",{\"1\":{\"30\":1}}],[\"本质特征\",{\"1\":{\"72\":2}}],[\"线性投射与嵌入\",{\"1\":{\"30\":1}}],[\"第\",{\"1\":{\"149\":1}}],[\"第四步\",{\"0\":{\"75\":1}}],[\"第三层\",{\"1\":{\"162\":1}}],[\"第三步\",{\"0\":{\"74\":1}}],[\"第三项\",{\"1\":{\"54\":1}}],[\"第二层\",{\"1\":{\"161\":1,\"162\":1}}],[\"第二步\",{\"0\":{\"73\":1}}],[\"第二种代表了\",{\"1\":{\"72\":1}}],[\"第二种是\",{\"1\":{\"72\":1}}],[\"第一层\",{\"1\":{\"161\":1,\"162\":1}}],[\"第一种类型可能代表了\",{\"1\":{\"72\":1}}],[\"第一种模式可能是\",{\"1\":{\"72\":1}}],[\"第一步\",{\"0\":{\"72\":1}}],[\"第一项\",{\"1\":{\"54\":1}}],[\"第5步\",{\"1\":{\"30\":1}}],[\"第4步\",{\"0\":{\"140\":1,\"158\":1},\"1\":{\"30\":1}}],[\"第3步\",{\"0\":{\"139\":1,\"157\":1},\"1\":{\"30\":1}}],[\"第2步\",{\"0\":{\"138\":1,\"156\":1},\"1\":{\"30\":1}}],[\"第1步\",{\"0\":{\"137\":1,\"155\":1},\"1\":{\"30\":1}}],[\"3​0\",{\"1\":{\"126\":1}}],[\"3​2−2​\",{\"1\":{\"126\":1}}],[\"30\",{\"1\":{\"126\":2}}],[\"300m\",{\"1\":{\"31\":1}}],[\"32​0\",{\"1\":{\"126\":1}}],[\"31\",{\"1\":{\"126\":1}}],[\"3111e97cd630gxrqa\",{\"1\":{\"79\":1}}],[\"3−0\",{\"1\":{\"126\":1}}],[\"3dmodel2​\",{\"1\":{\"127\":2}}],[\"3d\",{\"1\":{\"60\":1}}],[\"3²\",{\"1\":{\"58\":1}}],[\"3\",{\"0\":{\"35\":1,\"47\":1,\"60\":1,\"69\":1,\"84\":1,\"112\":1,\"171\":1,\"182\":1,\"184\":1},\"1\":{\"30\":1,\"58\":2,\"59\":4,\"67\":1,\"152\":5}}],[\"个组\",{\"1\":{\"178\":1}}],[\"个\",{\"1\":{\"143\":2,\"145\":1,\"149\":1,\"152\":1}}],[\"个步骤\",{\"1\":{\"142\":1}}],[\"个正交子空间\",{\"1\":{\"127\":1}}],[\"个奇异值就能较好地重构图像\",{\"1\":{\"74\":1}}],[\"个模式\",{\"1\":{\"73\":1}}],[\"个最大的奇异值\",{\"1\":{\"73\":1}}],[\"个维度\",{\"1\":{\"57\":1}}],[\"个块嵌入序列的最前面\",{\"1\":{\"30\":1}}],[\"个一维向量组成的序列\",{\"1\":{\"30\":1}}],[\"个图像块\",{\"1\":{\"30\":1}}],[\"个大小均为\",{\"1\":{\"30\":1}}],[\"操作\",{\"1\":{\"30\":3,\"114\":1}}],[\"宽度以及注意力头的数量\",{\"1\":{\"31\":1}}],[\"宽\",{\"1\":{\"30\":1}}],[\"高性能但计算昂贵的模型\",{\"1\":{\"186\":1}}],[\"高效的模型\",{\"1\":{\"186\":1}}],[\"高效并行扫描\",{\"1\":{\"115\":1}}],[\"高效并行\",{\"1\":{\"115\":1}}],[\"高效性\",{\"1\":{\"100\":1}}],[\"高维的表示空间中进行预测\",{\"1\":{\"96\":1}}],[\"高\",{\"1\":{\"30\":1,\"127\":1}}],[\"高方差的梯度估计会导致模型训练过程非常不稳定\",{\"1\":{\"25\":1}}],[\"×\",{\"1\":{\"30\":8}}],[\"输入矩阵表示为\",{\"1\":{\"143\":1}}],[\"输入序列矩阵\",{\"1\":{\"118\":1}}],[\"输入依赖的\",{\"1\":{\"115\":1}}],[\"输入\",{\"1\":{\"30\":2,\"113\":1,\"124\":1,\"126\":1}}],[\"输出向量维度均为dmodel​=512\",{\"1\":{\"161\":1}}],[\"输出是全局上下文感知的新表示\",{\"1\":{\"153\":1}}],[\"输出是\",{\"1\":{\"151\":1}}],[\"输出是多个子空间的非线性组合\",{\"1\":{\"125\":1}}],[\"输出本质\",{\"1\":{\"141\":1}}],[\"输出维度\",{\"1\":{\"120\":1}}],[\"输出方程\",{\"1\":{\"110\":1}}],[\"输出一个类别标签\",{\"1\":{\"29\":1}}],[\"输出y\",{\"1\":{\"19\":1}}],[\"输出\",{\"1\":{\"18\":1,\"30\":1,\"82\":1,\"83\":1,\"88\":1,\"152\":3,\"154\":1}}],[\"图像分类\",{\"1\":{\"171\":1}}],[\"图像分块与展平\",{\"1\":{\"30\":1}}],[\"图像压缩\",{\"1\":{\"74\":1}}],[\"图像块\",{\"1\":{\"29\":1}}],[\"图像块化\",{\"1\":{\"29\":1}}],[\"借鉴了这一思想\",{\"1\":{\"29\":1}}],[\"常用于变分推断\",{\"1\":{\"53\":1}}],[\"常常会有一个特殊的\",{\"1\":{\"29\":1}}],[\"常见的mcmc抽样方法包括metropolis\",{\"1\":{\"15\":1}}],[\"引入温度参数\",{\"1\":{\"187\":1}}],[\"引入额外的相对位置偏置矩阵\",{\"1\":{\"183\":1}}],[\"引入选择性后\",{\"1\":{\"114\":1}}],[\"引入\",{\"1\":{\"29\":1,\"31\":1}}],[\"引入了\",{\"1\":{\"29\":1}}],[\"序列模型\",{\"1\":{\"160\":1}}],[\"序列较长时需优化\",{\"1\":{\"153\":1}}],[\"序列中的每一个向量都包含了序列中所有其他向量的信息\",{\"1\":{\"141\":1}}],[\"序列中的所有其他元素\",{\"1\":{\"130\":1}}],[\"序列内部的\",{\"0\":{\"130\":1}}],[\"序列长度\",{\"1\":{\"118\":1}}],[\"序列的长度变成了\",{\"1\":{\"30\":1}}],[\"序列\",{\"1\":{\"29\":1}}],[\"将bert压缩为tinybert\",{\"1\":{\"189\":1}}],[\"将输入序列\",{\"1\":{\"160\":1}}],[\"将输入投影到\",{\"1\":{\"144\":1}}],[\"将分数转化为概率分布\",{\"1\":{\"151\":1}}],[\"将上一步得到的每个权重\",{\"1\":{\"140\":1}}],[\"将上一步得到的分数进行缩放\",{\"1\":{\"139\":1}}],[\"将高维空间分解为低维子空间\",{\"1\":{\"128\":1}}],[\"将拼接结果通过可学习权重矩阵\",{\"1\":{\"122\":1}}],[\"将所有头的输出在特征维度拼接\",{\"1\":{\"121\":1}}],[\"将r\",{\"1\":{\"106\":1}}],[\"将q\",{\"1\":{\"82\":1}}],[\"将一幅图像视为矩阵\",{\"1\":{\"74\":1}}],[\"将一个复杂的物体\",{\"1\":{\"72\":1}}],[\"将图像块化为序列\",{\"1\":{\"31\":1}}],[\"将图像分割成块\",{\"1\":{\"31\":1}}],[\"将图像转化为序列\",{\"1\":{\"29\":1}}],[\"将这个\",{\"1\":{\"30\":1}}],[\"将这张图像分割成\",{\"1\":{\"30\":1}}],[\"将每个展平后的图像块向量从\",{\"1\":{\"30\":1}}],[\"将每个二维的图像块\",{\"1\":{\"30\":1}}],[\"将从\",{\"1\":{\"22\":1}}],[\"部分\",{\"1\":{\"29\":1}}],[\"编码器中的self\",{\"1\":{\"134\":1}}],[\"编码器后\",{\"1\":{\"29\":1}}],[\"编码器的作用是深入理解输入图像各个部分之间的关系\",{\"1\":{\"29\":1}}],[\"编码器\",{\"1\":{\"29\":1,\"30\":1,\"92\":1,\"160\":1}}],[\"多模态模型\",{\"1\":{\"189\":1}}],[\"多样性\",{\"1\":{\"128\":1}}],[\"多样性丧失\",{\"1\":{\"101\":1,\"104\":1}}],[\"多头输出通过残差连接传递到前馈网络\",{\"1\":{\"128\":1}}],[\"多头注意力通过这种\",{\"1\":{\"128\":1}}],[\"多头注意力是\",{\"1\":{\"117\":1}}],[\"多头注意力\",{\"0\":{\"117\":1},\"1\":{\"127\":1}}],[\"多头自注意力\",{\"1\":{\"30\":1}}],[\"多生成器\",{\"1\":{\"104\":1}}],[\"多块掩码\",{\"1\":{\"98\":1}}],[\"多块掩码策略\",{\"0\":{\"98\":1}}],[\"多维空间中的距离\",{\"1\":{\"55\":1}}],[\"多个像素\",{\"1\":{\"29\":1}}],[\"多对一\",{\"1\":{\"29\":1}}],[\"多峰的后验分布\",{\"1\":{\"14\":1}}],[\"抛弃解码器\",{\"1\":{\"29\":1}}],[\"对比\",{\"1\":{\"180\":1,\"181\":1,\"182\":1,\"183\":1}}],[\"对输入样本进行归一化\",{\"1\":{\"174\":1}}],[\"对输入数据\",{\"1\":{\"69\":1}}],[\"对所有神经元的输出进行归一化\",{\"1\":{\"174\":1}}],[\"对每一批样本的输入进行归一化处理\",{\"1\":{\"174\":1}}],[\"对每一行应用\",{\"1\":{\"148\":1}}],[\"对每个头\",{\"1\":{\"119\":1,\"120\":1}}],[\"对第\",{\"1\":{\"145\":1}}],[\"对缩放后的分数应用\",{\"1\":{\"139\":1}}],[\"对齐\",{\"1\":{\"135\":1}}],[\"对小目标的检测性能较差\",{\"1\":{\"93\":1}}],[\"对象查询\",{\"1\":{\"92\":1}}],[\"对的价值\",{\"1\":{\"82\":1}}],[\"对任何特征矩阵进行\",{\"1\":{\"74\":1}}],[\"对应了数据中最主要\",{\"1\":{\"72\":1}}],[\"对角线上的元素被称为奇异值\",{\"1\":{\"72\":1}}],[\"对抗过拟合\",{\"0\":{\"65\":1}}],[\"对异常值敏感\",{\"1\":{\"63\":1}}],[\"对异常值稳健\",{\"1\":{\"63\":1}}],[\"对异常值非常敏感\",{\"1\":{\"61\":1}}],[\"对异常值更不敏感\",{\"1\":{\"61\":2}}],[\"对称\",{\"1\":{\"53\":1}}],[\"对称性\",{\"1\":{\"52\":2,\"53\":1}}],[\"对近似分布\",{\"1\":{\"48\":1}}],[\"对\",{\"1\":{\"47\":2,\"74\":2,\"114\":1,\"149\":1,\"153\":1}}],[\"对传统\",{\"0\":{\"29\":1}}],[\"对于向量\",{\"1\":{\"178\":1}}],[\"对于位置\",{\"1\":{\"178\":1}}],[\"对于序列中的每个位置\",{\"1\":{\"177\":1}}],[\"对于每个组\",{\"1\":{\"178\":1}}],[\"对于每个样本\",{\"1\":{\"174\":1}}],[\"对于每个神经元\",{\"1\":{\"174\":3}}],[\"对于每个时间步\",{\"1\":{\"111\":1}}],[\"对于具有relu这类激活函数的网络\",{\"1\":{\"170\":1}}],[\"对于解码器的某个特定位置\",{\"1\":{\"159\":1}}],[\"对于\",{\"1\":{\"138\":2,\"139\":1,\"140\":2,\"157\":1}}],[\"对于所有输入都是固定的\",{\"1\":{\"113\":1}}],[\"对于绝大多数日常训练深度网络的任务\",{\"1\":{\"70\":1}}],[\"对于权重的分布\",{\"1\":{\"20\":1}}],[\"对于深度神经网络来说他们的权重符合正态分布么\",{\"1\":{\"20\":1}}],[\"对于回归问题\",{\"1\":{\"19\":1}}],[\"对于一个输入序列\",{\"1\":{\"114\":1}}],[\"对于一个具有权重w的神经网络\",{\"1\":{\"19\":1}}],[\"对于一个具有d个权重的网络\",{\"1\":{\"18\":1}}],[\"对于一个有n个样本的数据集d\",{\"1\":{\"18\":1}}],[\"对于长尾分布\",{\"1\":{\"14\":1}}],[\"对于复杂的\",{\"1\":{\"14\":1}}],[\"对于神经网络权重更新使用laplace\",{\"1\":{\"14\":1}}],[\"相对位置编码方法如transformer\",{\"1\":{\"183\":1}}],[\"相对位置编码的天然集成\",{\"0\":{\"180\":1}}],[\"相似度越高\",{\"1\":{\"151\":1}}],[\"相同\",{\"1\":{\"127\":1}}],[\"相反\",{\"1\":{\"97\":1}}],[\"相比于传统方法\",{\"1\":{\"184\":1}}],[\"相比于faster\",{\"1\":{\"93\":1}}],[\"相比于得分函数估计器等替代方法\",{\"1\":{\"26\":1}}],[\"相媲美的性能\",{\"1\":{\"28\":1}}],[\"领域大放异彩的\",{\"1\":{\"28\":1}}],[\"标准softmax输出\",{\"1\":{\"187\":1}}],[\"标准化为1\",{\"1\":{\"169\":1}}],[\"标准差为\",{\"1\":{\"22\":1}}],[\"标配\",{\"1\":{\"70\":1}}],[\"标志着深度学习领域的一个重要转折点\",{\"1\":{\"28\":1}}],[\"许多现代深度生成模型和概率编程框架的成功将是难以想象的\",{\"1\":{\"26\":1}}],[\"等一系列精巧的改造\",{\"1\":{\"31\":1}}],[\"等模型的关键\",{\"1\":{\"26\":1}}],[\"等优化算法能够更稳定\",{\"1\":{\"25\":1}}],[\"用教师模型对训练数据预测\",{\"1\":{\"187\":1}}],[\"用注意力权重\",{\"1\":{\"149\":1}}],[\"用刚玩的数据学\",{\"1\":{\"87\":1}}],[\"用更少的信息表示最多的数据内涵\",{\"1\":{\"76\":1}}],[\"用得到的\",{\"1\":{\"74\":1}}],[\"用户3是浪漫片爱好者\",{\"1\":{\"75\":1}}],[\"用户3是负值\",{\"1\":{\"75\":1}}],[\"用户3\",{\"1\":{\"75\":1}}],[\"用户2\",{\"1\":{\"75\":1}}],[\"用户1和2是科幻迷\",{\"1\":{\"75\":1}}],[\"用户1\",{\"1\":{\"75\":1}}],[\"用户\",{\"1\":{\"73\":1,\"74\":1}}],[\"用户偏好模式\",{\"1\":{\"72\":1}}],[\"用户类型\",{\"1\":{\"72\":1}}],[\"用于防止除零错误\",{\"1\":{\"174\":1}}],[\"用于防止模型过拟合\",{\"1\":{\"62\":1}}],[\"用于连接两个不同的序列\",{\"1\":{\"129\":1}}],[\"用于处理高维状态空间\",{\"1\":{\"82\":1}}],[\"用于模型压缩和特征选择\",{\"1\":{\"70\":1}}],[\"用于下游的分类任务\",{\"1\":{\"29\":1}}],[\"用于后续的分类\",{\"1\":{\"29\":1}}],[\"用于贝叶斯变分推断的重参数技巧是一种优雅而强大的技术\",{\"1\":{\"26\":1}}],[\"用来防止梯度消失或者梯度爆炸\",{\"1\":{\"174\":1}}],[\"用来衡量向量的大小或长度\",{\"1\":{\"55\":1}}],[\"用来衡量变分分布和先验分布的相似程度\",{\"1\":{\"18\":1}}],[\"用来近似权重的真实后验分布\",{\"1\":{\"18\":1}}],[\"总损失\",{\"1\":{\"187\":1}}],[\"总计算量\",{\"1\":{\"125\":1}}],[\"总有定义\",{\"1\":{\"53\":1}}],[\"总是有定义的\",{\"1\":{\"50\":1}}],[\"总是\",{\"1\":{\"46\":1}}],[\"总而言之的数学公式\",{\"1\":{\"153\":1}}],[\"总而言之\",{\"1\":{\"26\":1,\"31\":1,\"93\":1,\"100\":1,\"135\":1,\"173\":1}}],[\"总结与直观理解\",{\"0\":{\"159\":1}}],[\"总结与关键点\",{\"0\":{\"141\":1}}],[\"总结与意义\",{\"0\":{\"115\":1}}],[\"总结与对比\",{\"0\":{\"70\":1,\"172\":1}}],[\"总结一下\",{\"1\":{\"114\":1}}],[\"总结\",{\"0\":{\"26\":1,\"48\":1,\"63\":1,\"76\":1,\"88\":1,\"108\":1,\"135\":1,\"177\":1,\"184\":1}}],[\"总的来说\",{\"1\":{\"13\":1,\"14\":1,\"174\":2}}],[\"更适合处理序列化的任务\",{\"1\":{\"174\":1}}],[\"更准确的表征\",{\"1\":{\"141\":1}}],[\"更准确地说\",{\"1\":{\"99\":1}}],[\"更丰富的表示\",{\"1\":{\"131\":1}}],[\"更丰富梯度信息的损失函数\",{\"1\":{\"104\":1}}],[\"更久\",{\"1\":{\"113\":1}}],[\"更接近人类学习方式的计算机视觉模型开辟了新的道路\",{\"1\":{\"100\":1}}],[\"更加高效\",{\"1\":{\"96\":1}}],[\"更是一种创新的自监督学习框架\",{\"0\":{\"95\":1}}],[\"更端到端的方向发展\",{\"1\":{\"93\":1}}],[\"更有意义的矩阵的乘积\",{\"1\":{\"72\":1}}],[\"更强大的正则化技术\",{\"1\":{\"69\":1}}],[\"更强的可扩展性\",{\"1\":{\"31\":1}}],[\"更新状态更慢\",{\"1\":{\"113\":1}}],[\"更新参数\",{\"1\":{\"68\":1}}],[\"更新权重\",{\"1\":{\"15\":2,\"18\":1}}],[\"更平滑\",{\"1\":{\"65\":1}}],[\"更稳定\",{\"1\":{\"62\":1}}],[\"更稳健\",{\"1\":{\"61\":1}}],[\"更精确的近似\",{\"1\":{\"48\":1}}],[\"更安全的近似\",{\"1\":{\"47\":1}}],[\"更大规模的\",{\"1\":{\"31\":1}}],[\"更高效\",{\"1\":{\"26\":1,\"100\":1}}],[\"更高效地进行\",{\"1\":{\"25\":1}}],[\"更深层的生成模型\",{\"1\":{\"25\":1}}],[\"就得到了损失对注意力权重矩阵\",{\"1\":{\"156\":1}}],[\"就会不顾一切地冲上去\",{\"1\":{\"108\":1}}],[\"就会导致梯度估计的方差随之增大\",{\"1\":{\"25\":1}}],[\"就错误地加强在该状态下采取的所有动作\",{\"1\":{\"107\":1}}],[\"就将这个思想作为其核心\",{\"1\":{\"104\":1}}],[\"就等于是在直接鼓励actor保持其动作选择的多样性\",{\"1\":{\"104\":1}}],[\"就判定为假\",{\"1\":{\"104\":1}}],[\"就能获得很好的分数\",{\"1\":{\"102\":1}}],[\"就是输入序列\",{\"1\":{\"112\":1}}],[\"就是最小化预测器生成的表示与目标编码器生成的真实表示之间的差异\",{\"1\":{\"97\":1}}],[\"就是一个去噪版的数据矩阵\",{\"1\":{\"73\":1}}],[\"就基于此\",{\"1\":{\"73\":1}}],[\"就像一个不懂棋理但下了无数盘棋的棋手\",{\"1\":{\"86\":1}}],[\"就像一只鸟直线飞过去的距离\",{\"1\":{\"56\":1}}],[\"就像是用一套完美的解剖工具\",{\"1\":{\"72\":1}}],[\"就像你沿着城市的街道网格走路\",{\"1\":{\"56\":1}}],[\"就被转化为了对期望内部函数的梯度\",{\"1\":{\"24\":1}}],[\"得益于transformer的自注意力机制\",{\"1\":{\"93\":1}}],[\"得分\",{\"1\":{\"88\":1}}],[\"得分表\",{\"1\":{\"82\":1}}],[\"得分函数估计器的梯度形式为\",{\"1\":{\"25\":1}}],[\"得到batch\",{\"1\":{\"174\":1}}],[\"得到归一化的输出\",{\"1\":{\"174\":1}}],[\"得到输出\",{\"1\":{\"149\":1}}],[\"得到注意力权重矩阵\",{\"1\":{\"148\":1}}],[\"得到原始注意力分数矩阵\",{\"1\":{\"145\":1}}],[\"得到\",{\"1\":{\"114\":1}}],[\"得到一个稀疏模型\",{\"1\":{\"68\":1}}],[\"得到一个虽然不完整但很精确的局部近似\",{\"1\":{\"47\":1}}],[\"得到了一个对称的\",{\"1\":{\"50\":1}}],[\"得到高斯近似\",{\"1\":{\"14\":1}}],[\"只对输入向量的均方根\",{\"1\":{\"166\":1}}],[\"只是关注的程度\",{\"1\":{\"141\":1}}],[\"只是线性地处理每个维度\",{\"1\":{\"61\":1}}],[\"只在同一个序列内部进行操作\",{\"1\":{\"129\":1}}],[\"只关注参考点周围的一小组关键采样点\",{\"1\":{\"93\":1}}],[\"只需算rms\",{\"1\":{\"172\":1}}],[\"只需加绝对值\",{\"1\":{\"61\":1}}],[\"只需要对\",{\"1\":{\"47\":1}}],[\"只能提供固定位置信息\",{\"1\":{\"180\":1}}],[\"只能沿着街道走\",{\"1\":{\"56\":1}}],[\"只能直角转弯\",{\"1\":{\"56\":1}}],[\"只有在\",{\"1\":{\"47\":1}}],[\"只使用了\",{\"1\":{\"29\":1}}],[\"只要\",{\"1\":{\"25\":1,\"51\":1}}],[\"只依赖于当前状态\",{\"1\":{\"15\":1}}],[\"叫做得分函数估计器\",{\"1\":{\"25\":1}}],[\"∇ϕ​logqϕ​\",{\"1\":{\"25\":1}}],[\"∇ϕ​f\",{\"1\":{\"24\":1,\"25\":1}}],[\"∇ϕ​eqϕ​\",{\"1\":{\"22\":1,\"24\":1}}],[\"固定的外部噪声源\",{\"1\":{\"24\":1}}],[\"梯度最终调整\",{\"1\":{\"159\":1}}],[\"梯度指导权重\",{\"1\":{\"159\":1}}],[\"梯度从输出\",{\"1\":{\"159\":1}}],[\"梯度消失\",{\"1\":{\"133\":1}}],[\"梯度\",{\"1\":{\"25\":1}}],[\"梯度可以直接推入期望内部\",{\"1\":{\"24\":1}}],[\"梯度在采样处中断\",{\"1\":{\"24\":1}}],[\"梯度信息丢失了\",{\"1\":{\"24\":1}}],[\"采样​z→loss\",{\"1\":{\"24\":1}}],[\"采样和前向传播\",{\"1\":{\"18\":1}}],[\"定义为\",{\"1\":{\"178\":1}}],[\"定义域问题\",{\"1\":{\"53\":1}}],[\"定义了分布\",{\"1\":{\"24\":1}}],[\"定义优化目标\",{\"1\":{\"18\":1}}],[\"分别用于缩放和平移\",{\"1\":{\"174\":1}}],[\"分别对应缩放因子和偏移量\",{\"1\":{\"174\":1}}],[\"分别与它们的\",{\"1\":{\"51\":1}}],[\"分布本身就已经不是零中心的了\",{\"1\":{\"170\":1}}],[\"分布么\",{\"1\":{\"14\":1}}],[\"分析同一个序列的内部结构\",{\"1\":{\"135\":1}}],[\"分治策略\",{\"1\":{\"128\":1}}],[\"分数越大\",{\"1\":{\"151\":1}}],[\"分数\",{\"1\":{\"82\":1,\"88\":1,\"138\":4,\"139\":4,\"154\":1}}],[\"分解\",{\"1\":{\"75\":1}}],[\"分解成了\",{\"1\":{\"72\":1}}],[\"分解成三个更简单\",{\"1\":{\"72\":1}}],[\"分母\",{\"1\":{\"45\":1}}],[\"分类\",{\"1\":{\"30\":1}}],[\"分离随机性与参数\",{\"0\":{\"24\":1}}],[\"详细解释\",{\"0\":{\"23\":1}}],[\"变成了\",{\"1\":{\"106\":1}}],[\"变成了关于\",{\"1\":{\"22\":1}}],[\"变得\",{\"1\":{\"102\":1}}],[\"变得分散\",{\"1\":{\"65\":1}}],[\"变色等操作\",{\"1\":{\"69\":1}}],[\"变化了\",{\"1\":{\"61\":2}}],[\"变分分布\",{\"1\":{\"18\":1}}],[\"变分分布和权重的先验分布都是关于神经网络权重的概率分布\",{\"1\":{\"18\":1}}],[\"变分推断要解决的是一个贝叶斯推理中的核心计算难题\",{\"1\":{\"45\":1}}],[\"变分推断则计算效率高且易于扩展\",{\"1\":{\"13\":1}}],[\"变分推断涉及到一个非凸优化问题\",{\"1\":{\"13\":1}}],[\"变分推断往往会低估后验分布的方差\",{\"1\":{\"13\":1}}],[\"变分推断需要选择一个逼近分布\",{\"1\":{\"13\":1}}],[\"变分推断提供了一个确定性的逼近分布\",{\"1\":{\"13\":1}}],[\"变分推断更容易扩展到大数据集\",{\"1\":{\"13\":1}}],[\"变分推断通常比mcmc更快\",{\"1\":{\"13\":1}}],[\"变分推断\",{\"0\":{\"45\":1},\"1\":{\"13\":2}}],[\"随机地\",{\"1\":{\"69\":1}}],[\"随机性\",{\"1\":{\"104\":1}}],[\"随机性被作为一个独立的输入变量\",{\"1\":{\"24\":1}}],[\"随机性完全来自于与参数\",{\"1\":{\"22\":1}}],[\"随机采样\",{\"1\":{\"24\":1}}],[\"随着训练的进行\",{\"1\":{\"20\":1}}],[\"+cbuk​\",{\"1\":{\"112\":1}}],[\"+bu\",{\"1\":{\"110\":1}}],[\"+=\",{\"1\":{\"68\":1}}],[\"+0∗log\",{\"1\":{\"54\":1}}],[\"+0\",{\"1\":{\"54\":3}}],[\"+∞\",{\"1\":{\"53\":1}}],[\"+du\",{\"1\":{\"110\":1}}],[\"+djs​\",{\"1\":{\"52\":1}}],[\"+dkl​\",{\"1\":{\"46\":1}}],[\"+21​dkl​\",{\"1\":{\"51\":1}}],[\"+ez∼q​\",{\"1\":{\"46\":1}}],[\"+\",{\"1\":{\"26\":1,\"58\":6,\"59\":6,\"61\":10,\"68\":1,\"106\":4,\"152\":4,\"174\":2,\"187\":1}}],[\"+σϕ​\",{\"1\":{\"22\":1}}],[\"+logp\",{\"1\":{\"13\":1,\"46\":3}}],[\"直接将输入连接到输出\",{\"1\":{\"110\":1}}],[\"直接学习\",{\"1\":{\"88\":1}}],[\"直接学习一个\",{\"1\":{\"83\":1}}],[\"直接设置\",{\"1\":{\"67\":1}}],[\"直接从这个分布采样是不可导的\",{\"1\":{\"22\":1}}],[\"直角三角形的斜边长度\",{\"1\":{\"58\":1}}],[\"直观理解\",{\"1\":{\"110\":1,\"157\":1}}],[\"直观图示\",{\"1\":{\"47\":1}}],[\"直观地理解\",{\"1\":{\"25\":1}}],[\"直到马尔科夫链达到平稳分布\",{\"1\":{\"15\":2}}],[\"ϵ∼p\",{\"1\":{\"22\":1,\"24\":1}}],[\"ϵ\",{\"1\":{\"22\":8,\"24\":5,\"25\":2}}],[\"与汽车差异大\",{\"1\":{\"186\":1}}],[\"与现代硬件\",{\"1\":{\"183\":1}}],[\"与bn不同的是\",{\"1\":{\"174\":1}}],[\"与这个\",{\"1\":{\"158\":1}}],[\"与这个核\",{\"1\":{\"112\":1}}],[\"与所有键向量\",{\"1\":{\"158\":1}}],[\"与对应的\",{\"1\":{\"140\":1}}],[\"与普通attention相同但来源不同\",{\"0\":{\"132\":1}}],[\"与单头相同\",{\"1\":{\"125\":1}}],[\"与\",{\"1\":{\"115\":1,\"131\":1,\"138\":2,\"156\":1}}],[\"与许多依赖于复杂手工数据增强的自监督方法不同\",{\"1\":{\"100\":1}}],[\"与降维原理相同\",{\"1\":{\"73\":1}}],[\"与l1\",{\"1\":{\"70\":1}}],[\"与它们的中间分布\",{\"1\":{\"50\":1}}],[\"与卷积核大小固定的\",{\"1\":{\"31\":1}}],[\"与块嵌入相加后\",{\"1\":{\"30\":1}}],[\"与参数\",{\"1\":{\"22\":1}}],[\"与变分推断不同\",{\"1\":{\"13\":1}}],[\"期望的计算涉及到从\",{\"1\":{\"22\":1}}],[\"zhuanlan\",{\"1\":{\"174\":2}}],[\"zhihu\",{\"1\":{\"174\":3}}],[\"zn​\",{\"1\":{\"141\":1,\"160\":1}}],[\"z2​=权重\",{\"1\":{\"140\":1}}],[\"z2​\",{\"1\":{\"140\":2,\"141\":1}}],[\"z1​=权重\",{\"1\":{\"140\":1}}],[\"z1​\",{\"1\":{\"140\":2,\"141\":1,\"160\":1}}],[\"zoh\",{\"1\":{\"111\":1}}],[\"zero\",{\"1\":{\"68\":1}}],[\"z→loss\",{\"1\":{\"24\":1}}],[\"z=\",{\"1\":{\"160\":1}}],[\"z=μϕ​+σϕ​⋅ϵ​z\",{\"1\":{\"24\":1}}],[\"z=μϕ​\",{\"1\":{\"22\":1}}],[\"z=gϕ​\",{\"1\":{\"22\":1}}],[\"z∼qϕ​\",{\"1\":{\"22\":1}}],[\"z\",{\"1\":{\"22\":6,\"24\":3,\"25\":6,\"34\":3,\"37\":2,\"45\":6,\"46\":21,\"47\":4,\"141\":1,\"174\":2,\"187\":2}}],[\"z∣x\",{\"1\":{\"22\":7,\"24\":2,\"25\":1,\"45\":4,\"46\":12,\"47\":4}}],[\"重新调整和平移归一化的样本\",{\"1\":{\"174\":1}}],[\"重新审视layernorm的目标\",{\"0\":{\"169\":1}}],[\"重写为\",{\"1\":{\"22\":1}}],[\"重参数技巧直接利用了函数\",{\"1\":{\"25\":1}}],[\"重参数技巧得到的梯度估计器通常具有低得多的方差\",{\"1\":{\"25\":1}}],[\"重参数技巧通过重构计算图解决了这个问题\",{\"1\":{\"24\":1}}],[\"重参数技巧之所以有效\",{\"1\":{\"23\":1}}],[\"重参数技巧的核心思想是\",{\"1\":{\"22\":1}}],[\"重参数技巧是什么\",{\"0\":{\"22\":1}}],[\"重参数技巧\",{\"1\":{\"21\":1}}],[\"重复步骤3\",{\"1\":{\"18\":1}}],[\"重复步骤\",{\"1\":{\"18\":1}}],[\"观察其分布的变化\",{\"1\":{\"20\":1}}],[\"观测到数据集x的概率\",{\"1\":{\"18\":1}}],[\"观测到数据x的概率\",{\"1\":{\"18\":1}}],[\"观测到当前数据的概率\",{\"1\":{\"18\":1}}],[\"观测数据为x\",{\"1\":{\"18\":1}}],[\"一定程度上防止过拟合\",{\"1\":{\"174\":1}}],[\"一\",{\"0\":{\"165\":1}}],[\"一次只输出一个词语\",{\"1\":{\"163\":1}}],[\"一路回溯到输入\",{\"1\":{\"159\":1}}],[\"一样递归计算\",{\"1\":{\"112\":1}}],[\"一旦\",{\"1\":{\"112\":1}}],[\"一旦发现一条看起来向上的路\",{\"1\":{\"108\":1}}],[\"一旦性能不再提升就停止训练\",{\"1\":{\"69\":1}}],[\"一部分神经元\",{\"1\":{\"69\":1}}],[\"一是在逼近真实的证据\",{\"1\":{\"48\":1}}],[\"一张\",{\"1\":{\"30\":1}}],[\"一张二维图像\",{\"1\":{\"30\":1}}],[\"一张图像就巧妙地转化为了一个\",{\"1\":{\"29\":1}}],[\"一步步拆解\",{\"1\":{\"30\":1}}],[\"一种将随机变量的生成过程分解为\",{\"1\":{\"26\":1}}],[\"一种常见的研究方法是在训练过程中对权重进行可视化\",{\"1\":{\"20\":1}}],[\"一个是缩放因子γ\",{\"1\":{\"174\":1}}],[\"一个生动的例子\",{\"0\":{\"131\":1}}],[\"一个常见的方法是零阶保持\",{\"1\":{\"111\":1}}],[\"一个常用的方法来量化模型不确定性是贝叶斯神经网络\",{\"1\":{\"19\":1}}],[\"一个谨慎的登山者\",{\"1\":{\"108\":1}}],[\"一个激进的登山者\",{\"1\":{\"108\":1}}],[\"一个控制机器人走路的rl智能体\",{\"1\":{\"103\":1}}],[\"一个训练用来生成人脸的gan\",{\"1\":{\"102\":1}}],[\"一个基于价值的网络\",{\"1\":{\"84\":1}}],[\"一个基于策略的网络\",{\"1\":{\"84\":1}}],[\"一个概率分布\",{\"1\":{\"83\":1}}],[\"一个极其重要的应用\",{\"1\":{\"74\":1}}],[\"一个简单的比喻\",{\"1\":{\"72\":1}}],[\"一个简单的例子\",{\"0\":{\"54\":1,\"75\":1}}],[\"一个\",{\"1\":{\"72\":3}}],[\"一个用户\",{\"1\":{\"72\":1}}],[\"一个非常好的方式来可视化\",{\"1\":{\"60\":1}}],[\"一个真正的距离度量必须满足以下四个条件\",{\"1\":{\"52\":1}}],[\"一个由\",{\"1\":{\"30\":1}}],[\"一个结果\",{\"1\":{\"29\":1}}],[\"一个最经典的例子是高斯分布\",{\"1\":{\"22\":1}}],[\"训练学生模型\",{\"1\":{\"187\":1}}],[\"训练教师模型\",{\"1\":{\"187\":1}}],[\"训练一个千亿参数的模型成本极高\",{\"1\":{\"173\":1}}],[\"训练时\",{\"1\":{\"115\":1}}],[\"训练模式\",{\"1\":{\"115\":1}}],[\"训练过程可能不稳定\",{\"1\":{\"87\":1}}],[\"训练过程通常更稳定\",{\"1\":{\"87\":1}}],[\"训练过程容易不稳定\",{\"1\":{\"83\":1}}],[\"训练过程中使用的优化算法也会影响权重的分布\",{\"1\":{\"20\":1}}],[\"训练数据和优化算法等因素的影响\",{\"1\":{\"20\":1}}],[\"训练数据的分布以及训练过程中的优化算法等\",{\"1\":{\"20\":1}}],[\"网络结构和训练数据\",{\"1\":{\"20\":1}}],[\"网络的结构\",{\"1\":{\"20\":1}}],[\"权重\",{\"1\":{\"139\":2,\"141\":1}}],[\"权重为0\",{\"1\":{\"68\":1}}],[\"权重衰减\",{\"0\":{\"67\":1},\"1\":{\"64\":1,\"67\":2,\"70\":1}}],[\"权重会近似地服从这种分布\",{\"1\":{\"20\":1}}],[\"权重的分布可能会发生变化\",{\"1\":{\"20\":1}}],[\"权重的分布会受到训练数据和网络结构的影响\",{\"1\":{\"20\":1}}],[\"权重的先验分布\",{\"1\":{\"18\":1}}],[\"权重的概率分布\",{\"1\":{\"15\":1}}],[\"ym​\",{\"1\":{\"160\":1}}],[\"y1​\",{\"1\":{\"160\":1}}],[\"y=softmax\",{\"1\":{\"157\":1}}],[\"y=u∗k\",{\"1\":{\"112\":1}}],[\"yk​=ck​xk​\",{\"1\":{\"114\":1}}],[\"yk​=cakbu0​+cak−1bu1​+\",{\"1\":{\"112\":1}}],[\"yk​\",{\"1\":{\"112\":1}}],[\"your\",{\"1\":{\"34\":1,\"36\":2,\"37\":5,\"39\":1,\"68\":1}}],[\"you\",{\"1\":{\"33\":1,\"34\":1,\"36\":2,\"37\":2,\"38\":1,\"39\":2}}],[\"y\",{\"1\":{\"19\":2,\"110\":3,\"112\":1,\"174\":3}}],[\"years\",{\"1\":{\"2\":1}}],[\"为每个词计算一个全新的向量表示\",{\"1\":{\"136\":1}}],[\"为处理极长序列\",{\"1\":{\"115\":1}}],[\"为构建更通用\",{\"1\":{\"100\":1}}],[\"为从推荐系统到搜索引擎\",{\"1\":{\"76\":1}}],[\"为什么现在这么流行\",{\"0\":{\"173\":1}}],[\"为什么self\",{\"0\":{\"133\":1}}],[\"为什么这如此重要\",{\"1\":{\"113\":1}}],[\"为什么会发生\",{\"1\":{\"102\":1,\"103\":1}}],[\"为什么有用\",{\"1\":{\"73\":2}}],[\"为什么有效\",{\"0\":{\"168\":1},\"1\":{\"67\":1,\"68\":1}}],[\"为什么在机器学习中如此强大\",{\"0\":{\"73\":1}}],[\"为什么js散度可以作为距离\",{\"0\":{\"52\":1}}],[\"为什么是\",{\"0\":{\"47\":1}}],[\"为什么\",{\"0\":{\"31\":1},\"1\":{\"61\":1}}],[\"为什么能起作用\",{\"1\":{\"26\":1}}],[\"为什么重参数技巧能起作用\",{\"0\":{\"23\":1}}],[\"为此\",{\"1\":{\"22\":1}}],[\"为均值\",{\"1\":{\"19\":1}}],[\"为了方便残差连接\",{\"1\":{\"161\":1}}],[\"为了计算损失对某个键向量\",{\"1\":{\"158\":1}}],[\"为了计算损失对某个查询向量\",{\"1\":{\"158\":1}}],[\"为了减小损失\",{\"1\":{\"156\":1}}],[\"为了更直观地理解整个流程\",{\"1\":{\"137\":1}}],[\"为了更好地理解\",{\"1\":{\"109\":1}}],[\"为了防止在训练时\",{\"1\":{\"134\":1}}],[\"为了引导模型学习到更具全局性和语义性的特征\",{\"1\":{\"98\":1}}],[\"为了解决这些问题\",{\"1\":{\"93\":1}}],[\"为了让模型知道每个图像块的原始空间位置\",{\"1\":{\"30\":1}}],[\"为了跨越这个\",{\"1\":{\"29\":1}}],[\"为了实现这个\",{\"1\":{\"12\":1}}],[\"为了找到这个最优的参数θ\",{\"1\":{\"12\":1}}],[\"也融合了全局信息\",{\"1\":{\"140\":1}}],[\"也是现代大语言模型理解语言上下文的基础\",{\"1\":{\"135\":1}}],[\"也是transformer模型成功的基石\",{\"1\":{\"129\":1}}],[\"也是kl散度非对称性在vi中体现出的巨大优势\",{\"1\":{\"47\":1}}],[\"也会遇到除零错误\",{\"1\":{\"54\":1}}],[\"也必然要求\",{\"1\":{\"52\":1}}],[\"也有概率时才有概率\",{\"1\":{\"47\":1}}],[\"也常被称为reinforce\",{\"1\":{\"25\":1}}],[\"也就是\",{\"1\":{\"30\":1}}],[\"也就是说\",{\"1\":{\"22\":1}}],[\"也就是使得elbo更大\",{\"1\":{\"12\":1}}],[\"也称为边缘似然性\",{\"1\":{\"19\":1}}],[\"后求\",{\"1\":{\"126\":1}}],[\"后验分布可以表示为p\",{\"1\":{\"19\":1}}],[\"后处理\",{\"1\":{\"15\":2}}],[\"参数量\",{\"1\":{\"127\":1}}],[\"参数说明\",{\"1\":{\"119\":1}}],[\"参数确定\",{\"1\":{\"112\":1}}],[\"参数就是l2正则化的强度系数\",{\"1\":{\"67\":1}}],[\"参数效率\",{\"1\":{\"31\":1}}],[\"参数\",{\"1\":{\"24\":1,\"65\":2,\"67\":1,\"114\":1,\"115\":1}}],[\"参数取某个值的可能性\",{\"1\":{\"18\":1}}],[\"参数的不确定性\",{\"1\":{\"18\":1}}],[\"虽然提供一些相对位置线索\",{\"1\":{\"180\":1}}],[\"虽然归一化有助于神经网络的训练\",{\"1\":{\"174\":1}}],[\"虽然它的公式和普通注意力一样\",{\"1\":{\"136\":1}}],[\"虽然头数\",{\"1\":{\"125\":1}}],[\"虽然离散\",{\"1\":{\"112\":1}}],[\"虽然我们不常用\",{\"1\":{\"103\":1}}],[\"虽然我们可以通过初始化的方式使权重的分布在训练开始时近似正态分布\",{\"1\":{\"20\":1}}],[\"虽然在gan和actor\",{\"1\":{\"101\":1}}],[\"虽然其主要目的是稳定训练\",{\"1\":{\"69\":1}}],[\"虽然深度学习自动学习特征\",{\"1\":{\"68\":1}}],[\"虽然js散度的名字是\",{\"1\":{\"52\":1}}],[\"虽然似然函数和概率密度函数看起来非常相似\",{\"1\":{\"18\":1}}],[\"虽然laplace\",{\"1\":{\"14\":2}}],[\"29\",{\"1\":{\"186\":1}}],[\"29​−0\",{\"1\":{\"126\":1}}],[\"2i\",{\"1\":{\"178\":1}}],[\"22⋅v2​\",{\"1\":{\"140\":1}}],[\"22\",{\"1\":{\"138\":1}}],[\"224\",{\"1\":{\"30\":2}}],[\"224x224\",{\"1\":{\"30\":1}}],[\"21⋅v1​+权重\",{\"1\":{\"140\":1}}],[\"21\",{\"1\":{\"126\":1,\"138\":1}}],[\"2b1wa9aflhjm6iuxyqw\",{\"1\":{\"79\":1}}],[\"2byl\",{\"1\":{\"79\":1}}],[\"2bymarhimxm3sq6jbpvp9zsubhvzxfactbaixadxzfl6xzryj7gphpeqs71f5vuab8wy6y2lkpmj933egyc6y41xsu\",{\"1\":{\"79\":1}}],[\"2bdiubhsxzoz\",{\"1\":{\"79\":1}}],[\"2bzyaf8g6y1drwy9wrdcq\",{\"1\":{\"79\":1}}],[\"2bxq3ygki6k6m5yvn8gnd1ewvh7fw6q9yjbovqagztnagfviep7zcgrpxdzfgub2qopz6kfiyv2sih728gcsersg4mphqxk2sj0cilhj7xxyuzp14gcezap7rirrql1tzppdovdgvj2chq3vxn8vmxxzvuivnn2cjr6dimktnnoe\",{\"1\":{\"79\":1}}],[\"2bxphvktuamcd\",{\"1\":{\"79\":1}}],[\"2bsqncxr5rlgek2l2kvrp9dgxc1s6dermhn6lq\",{\"1\":{\"79\":1}}],[\"2bwwfh\",{\"1\":{\"79\":1}}],[\"2bunqliudqkfhyqee51hrxuounh\",{\"1\":{\"79\":1}}],[\"2bur6axl6ieewqxbkiusqswuiyxafggwwntkwmdm1ndy4njuidbu5fi67ja8kifcdjyqqbbqoxj5zkt1g2ol3hljdcfel4qdjh3pl809gkap10e4k1x1vf5ktj77ptmr8flmbqhe9yvysz9c0tmtgtmxss1t7xw5xoedcmjkmdnb2rjny\",{\"1\":{\"79\":1}}],[\"2bqyfq5roqgkivips3flvrg6ga3hnlq\",{\"1\":{\"79\":1}}],[\"2bkz\",{\"1\":{\"79\":1}}],[\"2bkv0daupm8\",{\"1\":{\"79\":1}}],[\"2b5myhnjfhf7x8yn5jgq5yfeheulg3a2oecglsik96q\",{\"1\":{\"79\":1}}],[\"2bahbek14p\",{\"1\":{\"79\":1}}],[\"2bazel1\",{\"1\":{\"79\":1}}],[\"2bio9qmk7tqbbstmw5wticmzujymsoknnbydugmbi12a3iqvh01\",{\"1\":{\"79\":1}}],[\"2b66g\",{\"1\":{\"79\":1}}],[\"2bfu1zfk37vcflyvn\",{\"1\":{\"79\":1}}],[\"2bnpfhze7gsx6n1qxtyfwcsifuvijosndr0wbyz\",{\"1\":{\"79\":1}}],[\"2faws4\",{\"1\":{\"79\":1}}],[\"2fa6wc\",{\"1\":{\"79\":1}}],[\"2fus\",{\"1\":{\"79\":1}}],[\"2f20240522\",{\"1\":{\"79\":1}}],[\"2fhvhpftvyujfymkavwvktv2fzvzof1iw5eanb\",{\"1\":{\"79\":1}}],[\"2fcic\",{\"1\":{\"79\":1}}],[\"2fqgx\",{\"1\":{\"79\":1}}],[\"2fs3\",{\"1\":{\"79\":1}}],[\"2fsmijauligorebdrnagmyuxxq88qmwyhsxwtoajr0ufxfxyllp2mbf2v5htcm5us\",{\"1\":{\"79\":1}}],[\"2fss7cnkexl3rsbf6fteh\",{\"1\":{\"79\":1}}],[\"2fsknljopsmh\",{\"1\":{\"79\":1}}],[\"2foeuay6xovagp5aktnpb9x0xybzk\",{\"1\":{\"79\":1}}],[\"2f8u9zdzi9hhdhuvxmxsarq1nxdujw6lkyul2h1pswypj8lwk99bevexzq6jl\",{\"1\":{\"79\":1}}],[\"2fisyervsewroclfrhirjkwrszbhqj5ia\",{\"1\":{\"79\":1}}],[\"2frdcbztk\",{\"1\":{\"79\":1}}],[\"2fe7fiskmqmixfree\",{\"1\":{\"79\":1}}],[\"2fezzhr4q0lbl8j4xvm5mzswlmeth2f5jkr2weey3xdbh0mqllw9d7dqwbem6\",{\"1\":{\"79\":1}}],[\"2flie7zqpurjuekea0mgsomigka\",{\"1\":{\"79\":1}}],[\"2f5vtxck055rkrlhdcgcw29gpsxtbmouryfeeym16l\",{\"1\":{\"79\":1}}],[\"2fweacxvzlwvhc3qtmsjhmeuciqcsnmyvwhb8y4xitwyzd1jtoi16l4zzwls9vmlkeubwcgiguajrzhmek7ewzrspkezezkei\",{\"1\":{\"79\":1}}],[\"2f\",{\"1\":{\"79\":9}}],[\"271680\",{\"1\":{\"79\":1}}],[\"2\",{\"0\":{\"25\":1,\"34\":1,\"37\":1,\"46\":1,\"52\":1,\"57\":1,\"66\":1,\"83\":1,\"103\":1,\"107\":1,\"111\":1,\"167\":1,\"170\":1,\"179\":1,\"180\":1,\"181\":2,\"182\":1,\"183\":1},\"1\":{\"22\":1,\"53\":1,\"75\":1,\"106\":1,\"119\":1,\"124\":1,\"126\":3,\"152\":9,\"175\":1,\"178\":2}}],[\"2​​\",{\"1\":{\"126\":1}}],[\"2​\",{\"1\":{\"18\":1}}],[\"20\",{\"1\":{\"126\":3}}],[\"20240414163730\",{\"1\":{\"174\":1}}],[\"20240414165245\",{\"1\":{\"163\":1}}],[\"20240414153828\",{\"1\":{\"160\":1}}],[\"2021wr029772\",{\"1\":{\"77\":1}}],[\"2020\",{\"1\":{\"4\":1,\"20\":1}}],[\"2015\",{\"1\":{\"175\":1}}],[\"2016\",{\"1\":{\"7\":1,\"175\":1}}],[\"2019\",{\"1\":{\"6\":1}}],[\"似然函数与先验分布相结合\",{\"1\":{\"18\":1}}],[\"似然函数在参数估计中起着重要的作用\",{\"1\":{\"18\":1}}],[\"似然函数是\",{\"1\":{\"18\":1}}],[\"似然函数是一个在统计学和机器学习中非常重要的概念\",{\"1\":{\"18\":1}}],[\"似然函数l\",{\"1\":{\"18\":1}}],[\"似然函数可以通过神经网络的损失函数来定义\",{\"1\":{\"15\":1}}],[\"由ε限制\",{\"1\":{\"108\":1}}],[\"由vit构成的三大组件\",{\"0\":{\"97\":1}}],[\"由facebook\",{\"1\":{\"90\":1}}],[\"由\",{\"1\":{\"73\":1,\"161\":1}}],[\"由多个相同的层堆叠而成\",{\"1\":{\"30\":1}}],[\"由神经网络\",{\"1\":{\"18\":1}}],[\"由于旋转矩阵的正交性\",{\"1\":{\"178\":1}}],[\"由于在抽象空间进行预测\",{\"1\":{\"100\":1}}],[\"由于transformer注意力模块在处理高分辨率特征图时的计算复杂度较高\",{\"1\":{\"93\":1}}],[\"由于奇异值从大到小排列\",{\"1\":{\"73\":1}}],[\"由于其模型极其灵活和复杂\",{\"1\":{\"69\":1}}],[\"由于\",{\"1\":{\"51\":1,\"159\":1}}],[\"由于期望是线性的\",{\"1\":{\"24\":1}}],[\"由于我们使用了重参数化技巧\",{\"1\":{\"18\":1}}],[\"由于mcmc的前几次迭代可能还没有达到平稳分布\",{\"1\":{\"15\":1}}],[\"由于laplace\",{\"1\":{\"14\":1}}],[\"它直接集成到注意力机制中\",{\"1\":{\"183\":1}}],[\"它取的是同一个样本的不同通道做归一化\",{\"1\":{\"174\":1}}],[\"它取决于多种因素\",{\"1\":{\"20\":1}}],[\"它需要先计算均值\",{\"1\":{\"170\":1}}],[\"它需要计算与序列中每个词\",{\"1\":{\"138\":1}}],[\"它衡量的是向量大小的整体水平\",{\"1\":{\"167\":1}}],[\"它衡量的不是\",{\"1\":{\"107\":1}}],[\"它由清华大学团队在2019年的论文\",{\"1\":{\"164\":1}}],[\"它告诉模型\",{\"1\":{\"159\":1}}],[\"它融合了\",{\"1\":{\"140\":1}}],[\"它为序列中的每个词都计算出了一个融入了全局上下文的全新表示\",{\"1\":{\"136\":1}}],[\"它只能关注位置1到i的词\",{\"1\":{\"134\":1}}],[\"它只通过大量的反复试验来学习价值或策略\",{\"1\":{\"86\":1}}],[\"它会大量地融入\",{\"1\":{\"131\":1}}],[\"它会发现\",{\"1\":{\"131\":1}}],[\"它\",{\"1\":{\"131\":1}}],[\"它提供了线性复杂度的序列建模\",{\"1\":{\"115\":1}}],[\"它成功地将\",{\"1\":{\"115\":1}}],[\"它成功地将原本在自然语言处理\",{\"1\":{\"28\":1}}],[\"它都需要计算上一个状态\",{\"1\":{\"111\":1}}],[\"它都只输出这些\",{\"1\":{\"102\":1}}],[\"它持续地读取输入\",{\"1\":{\"110\":1}}],[\"它被设计用来处理长序列数据\",{\"1\":{\"109\":1}}],[\"它避免了智能体仅仅因为某个状态本身价值很高\",{\"1\":{\"107\":1}}],[\"它再也没有机会去发现可能比a更好的动作b了\",{\"1\":{\"106\":1}}],[\"它并非像熵正则化那样直接奖励多样性\",{\"1\":{\"105\":1}}],[\"它总是以接近100\",{\"1\":{\"103\":1}}],[\"它发现只要生成一种或少数几种能够完美欺骗判别器\",{\"1\":{\"102\":1}}],[\"它与vit的结合\",{\"1\":{\"100\":1}}],[\"它展示了通过在抽象表示空间中进行预测来学习强大视觉特征的巨大潜力\",{\"1\":{\"100\":1}}],[\"它定义了模型应该学习什么样的任务\",{\"1\":{\"99\":1}}],[\"它使用vit作为其核心的神经网络架构\",{\"1\":{\"99\":1}}],[\"它接收来自上下文编码器的输出\",{\"1\":{\"97\":1}}],[\"它一次性输出一个固定大小的预测集合\",{\"1\":{\"91\":1}}],[\"它可以在这个内部\",{\"1\":{\"86\":1}}],[\"它包含两个部分\",{\"1\":{\"84\":1}}],[\"它包含一个编码器\",{\"1\":{\"29\":1}}],[\"它们的值通过反向传播和梯度下降来学习\",{\"1\":{\"174\":1}}],[\"它们的主要区别在于归一化的维度不同\",{\"1\":{\"174\":1}}],[\"它们之间的关联计算都是一步完成的\",{\"1\":{\"133\":1}}],[\"它们将相同的输入x投影到不同的空间\",{\"1\":{\"132\":1}}],[\"它们将一个一维的输入信号\",{\"1\":{\"110\":1}}],[\"它们描述了\",{\"1\":{\"80\":1}}],[\"它们从大到小排列\",{\"1\":{\"72\":1}}],[\"它们通常能带来比单纯调大权重衰减系数更显著的效果\",{\"1\":{\"70\":1}}],[\"它们通常不再被直接称为\",{\"1\":{\"64\":1}}],[\"它同时也起到了轻微的正则化效果\",{\"1\":{\"69\":1}}],[\"它倾向于产生稀疏权重\",{\"1\":{\"68\":1}}],[\"它倾向于将一些不重要的特征的权重直接压缩到零\",{\"1\":{\"65\":1}}],[\"它倾向于让所有的权重都变小\",{\"1\":{\"62\":1}}],[\"它触及了传统机器学习与深度学习之间核心思想的传承\",{\"1\":{\"64\":1}}],[\"它强大的特性是倾向于产生稀疏解\",{\"1\":{\"62\":1}}],[\"它有一个策略网络\",{\"1\":{\"84\":1}}],[\"它有\",{\"1\":{\"57\":1}}],[\"它得名于纽约曼哈顿的棋盘式街道布局\",{\"1\":{\"56\":1}}],[\"它满足距离度量的所有条件\",{\"1\":{\"53\":1}}],[\"它的计算公式与普通注意力完全一样\",{\"1\":{\"132\":1}}],[\"它的目标是\",{\"1\":{\"106\":1}}],[\"它的作用是生成被遮挡的\",{\"1\":{\"97\":1}}],[\"它的核心目的是\",{\"1\":{\"130\":1}}],[\"它的核心创新在于让模型的关键参数成为输入的函数\",{\"1\":{\"109\":1}}],[\"它的核心价值在于\",{\"1\":{\"76\":1}}],[\"它的核心能力\",{\"0\":{\"73\":1}}],[\"它的核心思想是在一个抽象的\",{\"1\":{\"96\":1}}],[\"它的核心思想是将目标检测任务视为一个集合预测问题\",{\"1\":{\"90\":1}}],[\"它的核心思想是\",{\"1\":{\"50\":1}}],[\"它的光滑性使得求解梯度更容易\",{\"1\":{\"62\":1}}],[\"它的\",{\"1\":{\"58\":1,\"59\":1}}],[\"它的平方根\",{\"1\":{\"52\":1}}],[\"它保证了在任何\",{\"1\":{\"51\":1}}],[\"它本身就是一个可优化的目标函数\",{\"1\":{\"48\":1}}],[\"它允许序列中的每个元素\",{\"1\":{\"30\":1}}],[\"它携带着位置信息\",{\"1\":{\"30\":1}}],[\"它不只是看奖励的绝对值\",{\"1\":{\"107\":1}}],[\"它不仅是vit\",{\"0\":{\"95\":1}}],[\"它不直接处理单个像素\",{\"1\":{\"29\":1}}],[\"它不需要像机器翻译那样生成一个序列\",{\"1\":{\"29\":1}}],[\"它不需要函数\",{\"1\":{\"25\":1}}],[\"它能提供方差更低的梯度估计\",{\"1\":{\"26\":1}}],[\"它通过旋转矩阵编码位置\",{\"1\":{\"176\":1}}],[\"它通过归一化层内的每个样本来改善模型的训练\",{\"1\":{\"174\":1}}],[\"它通过并行执行多个独立的注意力头\",{\"1\":{\"117\":1}}],[\"它通过对称化和平滑处理\",{\"1\":{\"50\":1}}],[\"它通过将随机采样操作移出梯度计算路径\",{\"1\":{\"26\":1}}],[\"它通常是由问题的背景知识和经验选择的\",{\"1\":{\"18\":1}}],[\"它是不可或缺的技术之一\",{\"1\":{\"192\":1}}],[\"它是取不同样本的同一个通道的特征做归一化\",{\"1\":{\"174\":1}}],[\"它是transformer架构的核心\",{\"1\":{\"135\":1}}],[\"它是上下文编码器权重的一个指数移动平均\",{\"1\":{\"97\":1}}],[\"它是一种\",{\"1\":{\"173\":1}}],[\"它是一种非常强大且基础的数学工具\",{\"1\":{\"76\":1}}],[\"它是一个参数化的分布\",{\"1\":{\"18\":1}}],[\"它是深度学习训练的标配\",{\"1\":{\"67\":1}}],[\"它是什么\",{\"1\":{\"26\":1}}],[\"它解决了在随机模型中进行梯度优化的核心障碍\",{\"1\":{\"26\":1}}],[\"它将连续参数\",{\"1\":{\"111\":1}}],[\"它将策略比例\",{\"1\":{\"106\":1}}],[\"它将transformer模型应用于计算机视觉任务\",{\"1\":{\"99\":1}}],[\"它将这个原始数据矩阵\",{\"1\":{\"72\":1}}],[\"它将函数值\",{\"1\":{\"25\":1}}],[\"它将每个权重视为随机变量\",{\"1\":{\"19\":1}}],[\"它阻断了损失函数到参数\",{\"1\":{\"24\":1}}],[\"它巧妙地解决了在随机计算图中进行梯度反向传播的难题\",{\"1\":{\"21\":1}}],[\"它描述了在给定模型参数下\",{\"1\":{\"18\":1}}],[\"它假设后验分布在最大后验概率\",{\"1\":{\"14\":1}}],[\"54530247\",{\"1\":{\"174\":1}}],[\"54171297https\",{\"1\":{\"174\":1}}],[\"5​\",{\"1\":{\"152\":2}}],[\"5​115\",{\"1\":{\"152\":1}}],[\"5​1\",{\"1\":{\"126\":1}}],[\"5​0\",{\"1\":{\"126\":2}}],[\"51\",{\"1\":{\"152\":1}}],[\"51​23​\",{\"1\":{\"152\":1}}],[\"51​−10\",{\"1\":{\"126\":1}}],[\"512\",{\"1\":{\"118\":1}}],[\"50\",{\"1\":{\"106\":2,\"126\":2}}],[\"5分\",{\"1\":{\"75\":1}}],[\"5\",{\"0\":{\"62\":1,\"114\":1},\"1\":{\"58\":2,\"61\":4,\"67\":1,\"75\":3,\"142\":1,\"152\":4}}],[\"5直到满足停止条件\",{\"1\":{\"18\":1}}],[\"5+\",{\"1\":{\"2\":1}}],[\"α\",{\"1\":{\"187\":3}}],[\"α是学习率\",{\"1\":{\"18\":1}}],[\"α=min\",{\"1\":{\"15\":1}}],[\"或输入依赖\",{\"1\":{\"113\":1}}],[\"或正菱形\",{\"1\":{\"60\":1}}],[\"或根据对数底不同\",{\"1\":{\"54\":1}}],[\"或\",{\"1\":{\"51\":1,\"64\":2,\"72\":1,\"82\":1,\"130\":1,\"178\":1}}],[\"或求和\",{\"1\":{\"45\":1}}],[\"或边缘似然\",{\"1\":{\"45\":1}}],[\"或称为偶发不确定性\",{\"1\":{\"19\":1}}],[\"或称为认知不确定性\",{\"1\":{\"19\":1}}],[\"或其变体\",{\"1\":{\"18\":1}}],[\"或者梯度信息不够丰富\",{\"1\":{\"102\":1}}],[\"或者只会生成特定角度的男性头像\",{\"1\":{\"102\":1}}],[\"或者elbo收敛到一个稳定值\",{\"1\":{\"18\":1}}],[\"或者达到预设的迭代次数\",{\"1\":{\"15\":2}}],[\"或者期望\",{\"1\":{\"14\":1}}],[\"或者需要更快的计算速度\",{\"1\":{\"13\":1}}],[\"或者处理的是复杂的模型或分布\",{\"1\":{\"13\":1}}],[\"或者证据下界\",{\"1\":{\"12\":1}}],[\"并对比其他嵌入方法突出其优越性\",{\"1\":{\"176\":1}}],[\"并仔细分析其算法优点\",{\"1\":{\"176\":1}}],[\"并巧妙地移除了可能非必要的中心化\",{\"1\":{\"173\":1}}],[\"并用其卓越的性能证明了这种简化归一化的有效性\",{\"1\":{\"173\":1}}],[\"并将数据中心化到0\",{\"1\":{\"167\":1}}],[\"并将其应用在规模更大\",{\"1\":{\"65\":1}}],[\"并已被广泛应用于诸如llama\",{\"1\":{\"164\":1}}],[\"并行计算\",{\"1\":{\"141\":1}}],[\"并行地捕捉序列内部长距离依赖关系的机制\",{\"1\":{\"135\":1}}],[\"并行性\",{\"1\":{\"127\":1}}],[\"并行训练\",{\"0\":{\"112\":1}}],[\"并即时决定如何与之交互\",{\"1\":{\"113\":1}}],[\"并基于当前的记忆产生输出\",{\"1\":{\"110\":1}}],[\"并不断地重新评估周围的环境\",{\"1\":{\"108\":1}}],[\"并像处理单词一样处理它们\",{\"1\":{\"99\":1}}],[\"并使用一个在空间上分布足够广泛的上下文块来进行预测\",{\"1\":{\"98\":1}}],[\"并使用这些权重来执行前向传播\",{\"1\":{\"18\":1}}],[\"并结合目标块的位置信息\",{\"1\":{\"97\":1}}],[\"并结合位置编码\",{\"1\":{\"92\":1}}],[\"并让目标表示更加平滑\",{\"1\":{\"97\":1}}],[\"并生成其对应的特征表示\",{\"1\":{\"97\":1}}],[\"并非简单等同于vision\",{\"1\":{\"95\":1}}],[\"并非对传统\",{\"1\":{\"31\":1}}],[\"并提升了对小目标的检测性能\",{\"1\":{\"93\":1}}],[\"并得到多少奖励r\",{\"1\":{\"86\":1}}],[\"并向演员提供反馈\",{\"1\":{\"84\":1}}],[\"并且其非对称性在计算和实践中提供了关键优势\",{\"1\":{\"48\":1}}],[\"并且其数学形式\",{\"1\":{\"47\":1,\"48\":1}}],[\"并取负号\",{\"1\":{\"46\":1}}],[\"并取得了与顶级卷积神经网络\",{\"1\":{\"28\":1}}],[\"并最终输出一个包含整张图像丰富信息的综合表示\",{\"1\":{\"29\":1}}],[\"并剖析其处理图像数据的每一步\",{\"1\":{\"28\":1}}],[\"并需要选择一个合适的逼近分布\",{\"1\":{\"13\":1}}],[\"−i\",{\"1\":{\"111\":1}}],[\"−1\",{\"1\":{\"111\":1}}],[\"−ez∼q​\",{\"1\":{\"46\":5}}],[\"−l\",{\"1\":{\"18\":4}}],[\"−logq\",{\"1\":{\"13\":1}}],[\"−kl\",{\"1\":{\"18\":1}}],[\"其数学优雅\",{\"1\":{\"184\":1}}],[\"其他方法如正弦位置编码在长序列上可能出现频率衰减问题\",{\"1\":{\"182\":1}}],[\"其主要思想是在每个样本中\",{\"1\":{\"174\":1}}],[\"其主要创新点在于\",{\"1\":{\"91\":1}}],[\"其原理是通过减去均值并除以标准差来做归一化\",{\"1\":{\"174\":1}}],[\"其输入包含编码器的输出信息\",{\"1\":{\"162\":1}}],[\"其当前输出只能基于已输出的部分\",{\"1\":{\"162\":1}}],[\"其元素为\",{\"1\":{\"157\":1}}],[\"其和为1\",{\"1\":{\"139\":1}}],[\"其最伟大的地方在于\",{\"1\":{\"136\":1}}],[\"其mcts树搜索部分就扮演了模型的角色\",{\"1\":{\"86\":1}}],[\"其核心在于它巧妙地解决了\",{\"1\":{\"31\":1}}],[\"其对应的输出向量被视作整个序列的聚合表示\",{\"1\":{\"29\":1}}],[\"其参数\",{\"1\":{\"18\":1}}],[\"其均值向量μ和协方差矩阵σ也需要初始化\",{\"1\":{\"18\":1}}],[\"其中旋转角度依赖于位置\",{\"1\":{\"177\":1}}],[\"其中元素\",{\"1\":{\"145\":1}}],[\"其中包含了query\",{\"1\":{\"137\":1}}],[\"其中一种常见的方法就是使用正态分布或者均匀分布来进行初始化\",{\"1\":{\"20\":1}}],[\"其中⊙表示元素级的乘法\",{\"1\":{\"18\":1}}],[\"其中t是总的迭代次数\",{\"1\":{\"15\":1}}],[\"其中p\",{\"1\":{\"15\":2}}],[\"其中d是训练数据\",{\"1\":{\"15\":1,\"19\":1}}],[\"其中先验分布可以是权重的高斯分布\",{\"1\":{\"15\":1}}],[\"其中ϕ是变分参数\",{\"1\":{\"13\":1}}],[\"其中θ是参数\",{\"1\":{\"13\":1}}],[\"其中\",{\"1\":{\"12\":1,\"18\":2,\"22\":2,\"45\":1,\"51\":1,\"110\":1,\"111\":2,\"112\":1,\"118\":1,\"120\":1,\"139\":1,\"143\":1,\"151\":1,\"157\":1,\"167\":2,\"174\":3,\"178\":1}}],[\"σ表示求和\",{\"1\":{\"174\":1}}],[\"σ²\",{\"1\":{\"174\":2}}],[\"σ2\",{\"1\":{\"171\":1}}],[\"σ2为方差的高斯分布\",{\"1\":{\"19\":1}}],[\"σ₃\",{\"1\":{\"72\":1}}],[\"σ₂\",{\"1\":{\"72\":1}}],[\"σ₁\",{\"1\":{\"72\":2,\"75\":1}}],[\"σϕ​\",{\"1\":{\"22\":2,\"24\":2}}],[\"σ∣x\",{\"1\":{\"18\":1}}],[\"σ←σ−α∂σ∂\",{\"1\":{\"18\":1}}],[\"σ\",{\"1\":{\"18\":5,\"19\":1,\"22\":1,\"72\":3,\"74\":1,\"167\":1,\"169\":1,\"170\":1,\"172\":2,\"174\":2}}],[\"μϕ​\",{\"1\":{\"22\":2,\"24\":1}}],[\"μ←μ−α∂μ∂\",{\"1\":{\"18\":1}}],[\"μ\",{\"1\":{\"18\":6,\"22\":1,\"167\":2,\"169\":1,\"170\":1,\"171\":1,\"172\":1,\"174\":3}}],[\"即均方根值\",{\"1\":{\"167\":1}}],[\"即每个子层的输出为\",{\"1\":{\"161\":1}}],[\"即高优势\",{\"1\":{\"106\":1}}],[\"即使是几个百分点\",{\"1\":{\"173\":1}}],[\"即使他发现了一条看似很好的路\",{\"1\":{\"108\":1}}],[\"即使一个动作的优势非常差\",{\"1\":{\"106\":1}}],[\"即使某个动作的\",{\"1\":{\"106\":1}}],[\"即使两个用户没有对任何同一部电影评分\",{\"1\":{\"73\":1}}],[\"即过早收敛到局部最优\",{\"1\":{\"105\":1}}],[\"即在某个状态下\",{\"1\":{\"103\":1}}],[\"即预测\",{\"1\":{\"86\":1}}],[\"即让很多权重值直接变为零\",{\"1\":{\"68\":1}}],[\"即模型过于复杂\",{\"1\":{\"65\":1}}],[\"即它会自动进行\",{\"1\":{\"62\":1}}],[\"即所有\",{\"1\":{\"60\":1}}],[\"即一个形状为\",{\"1\":{\"30\":1}}],[\"即可完成图像分类\",{\"1\":{\"29\":1}}],[\"即输入一张图像\",{\"1\":{\"29\":1}}],[\"即权重应该接近零但可以在两个方向上变化\",{\"1\":{\"18\":1}}],[\"即负elbo\",{\"1\":{\"18\":1}}],[\"即引入一个无参数的噪声向量ϵ\",{\"1\":{\"18\":1}}],[\"即\",{\"1\":{\"15\":1,\"47\":2,\"51\":1,\"92\":1,\"154\":1}}],[\"即wt+1​=wt​\",{\"1\":{\"15\":1}}],[\"即wt+1​=w\",{\"1\":{\"15\":1}}],[\"03167\",{\"1\":{\"175\":1}}],[\"094​\",{\"1\":{\"152\":1}}],[\"01\",{\"1\":{\"186\":1}}],[\"0122\",{\"1\":{\"152\":1}}],[\"01​42​\",{\"1\":{\"152\":1}}],[\"047⋅4+0\",{\"1\":{\"152\":1}}],[\"047⋅0+0\",{\"1\":{\"152\":1}}],[\"047​0\",{\"1\":{\"152\":1}}],[\"06450\",{\"1\":{\"175\":1}}],[\"06​​e2\",{\"1\":{\"152\":1}}],[\"06​7\",{\"1\":{\"152\":1}}],[\"06+e3\",{\"1\":{\"152\":2}}],[\"0−0\",{\"1\":{\"126\":1}}],[\"02​\",{\"1\":{\"126\":1}}],[\"050\",{\"1\":{\"126\":1}}],[\"006⋅4+0\",{\"1\":{\"152\":1}}],[\"006⋅0+0\",{\"1\":{\"152\":1}}],[\"0060\",{\"1\":{\"152\":1}}],[\"00\",{\"1\":{\"126\":1}}],[\"005\",{\"1\":{\"61\":1}}],[\"0\",{\"1\":{\"15\":1,\"22\":1,\"53\":3,\"54\":34,\"59\":2,\"61\":6,\"62\":2,\"68\":1,\"72\":1,\"79\":2,\"110\":1,\"126\":1,\"152\":10,\"178\":1,\"186\":5}}],[\"⋅wowhereheadi​=softmax\",{\"1\":{\"123\":1}}],[\"⋅wo\",{\"1\":{\"122\":1}}],[\"⋅δb\",{\"1\":{\"111\":1}}],[\"⋅\",{\"1\":{\"47\":2}}],[\"⋅ϵ\",{\"1\":{\"22\":1}}],[\"⋅∣∣⋅\",{\"1\":{\"18\":1}}],[\"⋅q\",{\"1\":{\"15\":2}}],[\"⋅p\",{\"1\":{\"15\":1}}],[\"假设一个mini\",{\"1\":{\"174\":1}}],[\"假设\",{\"1\":{\"157\":1}}],[\"假设输入序列\",{\"1\":{\"152\":1}}],[\"假设输入序列有\",{\"1\":{\"143\":1}}],[\"假设用户2没有对电影c评分\",{\"1\":{\"75\":1}}],[\"假设有两个离散分布\",{\"1\":{\"54\":1}}],[\"假设我们的变分后验\",{\"1\":{\"22\":1}}],[\"假设我们的神经网络的权重是\",{\"1\":{\"12\":1}}],[\"假设我们有一个小小的用户\",{\"1\":{\"75\":1}}],[\"假设我们有一个向量\",{\"1\":{\"57\":1}}],[\"假设我们有一个从正态分布抽取的数据集x\",{\"1\":{\"18\":1}}],[\"假设我们有一个统计模型\",{\"1\":{\"18\":1}}],[\"假设当前权重值为wt​\",{\"1\":{\"15\":1}}],[\"中表现优异\",{\"1\":{\"182\":1}}],[\"中心化可能不是必须的\",{\"0\":{\"170\":1}}],[\"中心化\",{\"1\":{\"169\":1}}],[\"中提出\",{\"1\":{\"164\":1}}],[\"中动态的注意力机制形成对比\",{\"1\":{\"112\":1}}],[\"中对应的值来预测m\",{\"1\":{\"73\":1}}],[\"中是球体\",{\"1\":{\"60\":1}}],[\"中间分布\",{\"1\":{\"53\":1}}],[\"中间人\",{\"1\":{\"51\":1}}],[\"中的缺失值\",{\"1\":{\"73\":1}}],[\"中的前\",{\"1\":{\"73\":1}}],[\"中的一个主要模式\",{\"1\":{\"47\":1}}],[\"中的自注意力机制在第一层就能够建立图像中任意两个块之间的关系\",{\"1\":{\"31\":1}}],[\"中进行规划和学习\",{\"1\":{\"86\":1}}],[\"中进行随机采样得到\",{\"1\":{\"24\":1}}],[\"中进行抽样\",{\"1\":{\"15\":1}}],[\"中采样一个噪声\",{\"1\":{\"22\":1}}],[\"中采样一个噪声变量\",{\"1\":{\"22\":1}}],[\"中采样\",{\"1\":{\"22\":2}}],[\"中采样权重w\",{\"1\":{\"18\":1}}],[\"中\",{\"1\":{\"21\":1,\"29\":1,\"45\":1,\"66\":1,\"103\":1,\"104\":1,\"144\":1,\"185\":1}}],[\"中抽样一个新的权重值w\",{\"1\":{\"15\":1}}],[\"中就介绍了一种基于laplace\",{\"1\":{\"14\":1}}],[\"神经网络的权重不再是固定的值\",{\"1\":{\"15\":1}}],[\"请注意\",{\"1\":{\"15\":1}}],[\"不增加额外的计算开销\",{\"1\":{\"183\":1}}],[\"不受batch\",{\"1\":{\"174\":1}}],[\"不仅会影响它对应的注意力权重\",{\"1\":{\"157\":1}}],[\"不仅适用于深度学习\",{\"1\":{\"64\":1}}],[\"不再是单纯的\",{\"1\":{\"140\":1}}],[\"不要过早地让策略变得太确定\",{\"1\":{\"104\":1}}],[\"不要说\",{\"1\":{\"70\":1}}],[\"不稳定\",{\"1\":{\"88\":1}}],[\"不一定非得是当前策略\",{\"1\":{\"87\":1}}],[\"不一定等于\",{\"1\":{\"30\":1}}],[\"不过\",{\"1\":{\"64\":1}}],[\"不是\",{\"1\":{\"53\":1}}],[\"不是完全无处重叠\",{\"1\":{\"51\":1}}],[\"不同头关注不同模式\",{\"1\":{\"128\":1}}],[\"不同\",{\"1\":{\"31\":1,\"141\":1}}],[\"不重叠的方块\",{\"1\":{\"29\":1}}],[\"不可导的\",{\"1\":{\"22\":1}}],[\"不能通过收集更多的数据来减小\",{\"1\":{\"19\":1}}],[\"不确定性通常分为两种类型\",{\"1\":{\"19\":1}}],[\"不依赖于过去的状态\",{\"1\":{\"15\":1}}],[\"不需要选择逼近分布\",{\"1\":{\"13\":1}}],[\"抽样\",{\"1\":{\"15\":2}}],[\"首先计算中间分布\",{\"1\":{\"54\":1}}],[\"首先从一个建议分布中抽样一个新的权重值\",{\"1\":{\"15\":1}}],[\"首先\",{\"1\":{\"15\":1,\"18\":1,\"30\":1,\"174\":1}}],[\"尾部的信息对于整个分布的影响是非常重要的\",{\"1\":{\"14\":1}}],[\"而非仅拟合原始数据标签\",{\"1\":{\"186\":1}}],[\"而非中心化操作\",{\"1\":{\"170\":1}}],[\"而rope是确定性的\",{\"1\":{\"181\":1}}],[\"而rmsnorm则将其简化为\",{\"1\":{\"167\":1}}],[\"而ln由于其独立于batch\",{\"1\":{\"174\":1}}],[\"而layer\",{\"1\":{\"174\":1}}],[\"而又不损失性能的方法都极具吸引力\",{\"1\":{\"173\":1}}],[\"而无需任何人工标注的注意力指导\",{\"1\":{\"159\":1}}],[\"而几乎忽略\",{\"1\":{\"131\":1}}],[\"而critic也会因为只看到这种单一的行为模式\",{\"1\":{\"103\":1}}],[\"而这种稳定性又间接地保留了策略的探索性\",{\"1\":{\"108\":1}}],[\"而这些部分均基于vision\",{\"1\":{\"97\":1}}],[\"而这正是我们无法计算的\",{\"1\":{\"47\":1}}],[\"而这正是因为它天然地\",{\"1\":{\"44\":1}}],[\"而不会损害模型的表达能力\",{\"1\":{\"171\":1}}],[\"而不像layernorm那样先进行中心化\",{\"1\":{\"166\":1}}],[\"而不能关注到i+1及之后的词\",{\"1\":{\"134\":1}}],[\"而不仅仅是记忆像素细节\",{\"1\":{\"95\":1}}],[\"而不是简单地固化任何能带来正奖励的行为\",{\"1\":{\"107\":1}}],[\"而不是它们的具体像素\",{\"1\":{\"96\":1}}],[\"而不是极度依赖少数特征\",{\"1\":{\"67\":1}}],[\"而不是依赖少数几个巨大的权重\",{\"1\":{\"65\":1}}],[\"而不是\",{\"0\":{\"47\":1},\"1\":{\"106\":1,\"131\":1,\"159\":1}}],[\"而不是像素总数\",{\"1\":{\"31\":1}}],[\"而不是通过采样\",{\"1\":{\"13\":1}}],[\"而电影c是\",{\"1\":{\"75\":1}}],[\"而舍弃了那些不重要的\",{\"1\":{\"73\":1}}],[\"而较小的奇异值可能对应一些次要趋势或噪声\",{\"1\":{\"72\":1}}],[\"而应该说\",{\"1\":{\"70\":1}}],[\"而且是其不可或缺的核心技术之一\",{\"1\":{\"64\":1}}],[\"而忽略其他模式\",{\"1\":{\"47\":1}}],[\"而是直接使用均方根值来对数据进行缩放\",{\"1\":{\"167\":1}}],[\"而是直接学习一个\",{\"1\":{\"83\":1}}],[\"而是\",{\"1\":{\"107\":1}}],[\"而是使用优势函数\",{\"1\":{\"107\":1}}],[\"而是要从更广阔的上下文中理解图像的整体结构和内容\",{\"1\":{\"98\":1}}],[\"而是一个全新的向量\",{\"1\":{\"140\":1}}],[\"而是一个分布\",{\"1\":{\"15\":1}}],[\"而是一种巧妙运用vit作为其核心构建模块的自监督学习算法\",{\"1\":{\"95\":1}}],[\"而是学习一个\",{\"1\":{\"82\":1}}],[\"而是以更一般化的\",{\"1\":{\"64\":1}}],[\"而是在数学推导最小化后验近似误差的过程中\",{\"1\":{\"48\":1}}],[\"而是从解决vi核心问题的数学推导中\",{\"1\":{\"44\":1}}],[\"而是通过与序列中所有其他词的\",{\"1\":{\"141\":1}}],[\"而是通过一种更根本的方式\",{\"1\":{\"105\":1}}],[\"而是通过\",{\"1\":{\"31\":1}}],[\"而是将图像分割成一系列固定大小的\",{\"1\":{\"29\":1}}],[\"而图像是二维的像素网格\",{\"1\":{\"29\":1}}],[\"而\",{\"1\":{\"22\":1,\"31\":1,\"53\":1,\"61\":1}}],[\"而采样操作本身是随机的\",{\"1\":{\"22\":1}}],[\"而似然函数则是描述在给定观测值的情况下\",{\"1\":{\"18\":1}}],[\"而map点就是高斯近似的均值\",{\"1\":{\"14\":1}}],[\"点积sij​=qi​⋅kjt​=∥qi​∥∥kj​∥cosθij​\",{\"1\":{\"151\":1}}],[\"点放在\",{\"1\":{\"61\":1}}],[\"点放在原点\",{\"1\":{\"61\":1}}],[\"点变成一个异常值\",{\"1\":{\"61\":1}}],[\"点附近可以被高斯分布很好地近似的情况\",{\"1\":{\"14\":1}}],[\"点附近是高斯的\",{\"1\":{\"14\":1}}],[\"点\",{\"1\":{\"14\":1}}],[\"拉普拉斯近似是一种简单且计算效率高的方法\",{\"1\":{\"14\":1}}],[\"每组对应一个旋转角度\",{\"1\":{\"178\":1}}],[\"每行和为\",{\"1\":{\"148\":1}}],[\"每次都先迈左腿\",{\"1\":{\"103\":1}}],[\"每次迭代都会使得我们的权重分布更接近真实的后验分布\",{\"1\":{\"12\":1}}],[\"每部电影属于各种类型的比例是多少\",{\"1\":{\"72\":1}}],[\"每种类型有多重要\",{\"1\":{\"72\":1}}],[\"每种方法都有其优点和缺点\",{\"1\":{\"13\":1}}],[\"每一层包括三个子层\",{\"1\":{\"162\":1}}],[\"每一层包括两个子层\",{\"1\":{\"161\":1}}],[\"每一层主要包含两个子模块\",{\"1\":{\"30\":1}}],[\"每一列是一部电影\",{\"1\":{\"72\":1}}],[\"每一行是一个用户\",{\"1\":{\"72\":1}}],[\"每一步的指引都充满了随机干扰\",{\"1\":{\"25\":1}}],[\"每一步都是确定性的\",{\"1\":{\"24\":1}}],[\"每个块是一个2x2旋转矩阵\",{\"1\":{\"178\":1}}],[\"每个子层后同样加入了\",{\"1\":{\"162\":1}}],[\"每个注意力权重应该变大还是变小\",{\"1\":{\"156\":1}}],[\"每个词不再是孤立的\",{\"1\":{\"141\":1}}],[\"每个输入词向量\",{\"1\":{\"137\":1}}],[\"每个头学习不同的投影\",{\"1\":{\"125\":1}}],[\"每个头的维度\",{\"1\":{\"119\":1}}],[\"每个对象查询负责预测一个特定的物体\",{\"1\":{\"92\":1}}],[\"每个位置的输出都会经过一个简单的全连接前馈网络\",{\"1\":{\"30\":1}}],[\"每个图像块的表示\",{\"1\":{\"30\":1}}],[\"每个\",{\"1\":{\"30\":1,\"82\":1,\"143\":1}}],[\"ϕ→μϕ​\",{\"1\":{\"24\":1}}],[\"ϕ→qϕ​\",{\"1\":{\"24\":1}}],[\"ϕ\",{\"1\":{\"13\":1,\"22\":4,\"24\":4,\"25\":1}}],[\"xl需要修改注意力计算公式\",{\"1\":{\"183\":1}}],[\"xl\",{\"1\":{\"180\":1}}],[\"x−μ\",{\"1\":{\"172\":1}}],[\"x​⋅γ\",{\"1\":{\"167\":1}}],[\"x+\",{\"1\":{\"161\":1}}],[\"x+multihead\",{\"1\":{\"128\":1}}],[\"xn​\",{\"1\":{\"160\":1}}],[\"xi​∈r1×dmodel​\",{\"1\":{\"143\":1}}],[\"xi​−μ\",{\"1\":{\"18\":1}}],[\"x2​\",{\"1\":{\"137\":1,\"138\":1,\"140\":1}}],[\"x1​x2​​\",{\"1\":{\"152\":1}}],[\"x1​\",{\"1\":{\"137\":2,\"138\":1,\"139\":1,\"140\":2,\"160\":1}}],[\"x=\",{\"1\":{\"126\":1,\"152\":1}}],[\"xwiv​\",{\"1\":{\"123\":1}}],[\"xwik​\",{\"1\":{\"123\":1}}],[\"xwiq​\",{\"1\":{\"123\":1}}],[\"x∈rn×dmodel​=​x1​x2​⋮xn​​​\",{\"1\":{\"143\":1}}],[\"x∈rn×dmodel​\",{\"1\":{\"118\":1}}],[\"xk​\",{\"1\":{\"114\":1}}],[\"xk​=axk−1​+buk​yk​=cxk​\",{\"1\":{\"111\":1}}],[\"xk−1​\",{\"1\":{\"111\":1}}],[\"x∣z\",{\"1\":{\"45\":2}}],[\"x∣θ\",{\"1\":{\"13\":2}}],[\"x\",{\"1\":{\"19\":3,\"22\":9,\"24\":1,\"25\":1,\"34\":12,\"36\":2,\"37\":8,\"45\":4,\"46\":16,\"48\":1,\"79\":7,\"110\":4,\"123\":1,\"124\":4,\"128\":1,\"132\":4,\"157\":2,\"161\":1,\"167\":7,\"172\":2,\"174\":7,\"178\":3}}],[\"x是观测到的数据\",{\"1\":{\"13\":1}}],[\"∝p\",{\"1\":{\"13\":1}}],[\"​wq∈rdmodel​×dk​wk∈rdmodel​×dk​wv∈rdmodel​×dv​​\",{\"1\":{\"144\":1}}],[\"​wiq​∈rdmodel​×dk​wik​∈rdmodel​×dk​wiv​∈rdmodel​×dv​​\",{\"1\":{\"119\":1}}],[\"​​\",{\"1\":{\"126\":1}}],[\"​​head2\",{\"1\":{\"126\":2}}],[\"​​head1\",{\"1\":{\"126\":1}}],[\"​multihead\",{\"1\":{\"123\":1}}],[\"​=ez∼q​\",{\"1\":{\"46\":1}}],[\"​和∂σ∂\",{\"1\":{\"18\":1}}],[\"​\",{\"1\":{\"13\":2,\"15\":2,\"18\":4,\"22\":1,\"24\":2,\"45\":1,\"46\":2,\"148\":1,\"152\":2}}],[\"∣d\",{\"1\":{\"15\":2}}],[\"∣wt​\",{\"1\":{\"15\":3}}],[\"∣∣p\",{\"1\":{\"13\":1,\"18\":1}}],[\"∣x\",{\"1\":{\"13\":1}}],[\"∣θ\",{\"1\":{\"13\":2}}],[\"1502\",{\"1\":{\"175\":1}}],[\"15−1\",{\"1\":{\"126\":1}}],[\"12​30\",{\"1\":{\"152\":1}}],[\"12⋅v2​\",{\"1\":{\"140\":1}}],[\"12\",{\"1\":{\"138\":1,\"139\":3}}],[\"11⋅v1​+权重\",{\"1\":{\"140\":1}}],[\"11\",{\"1\":{\"138\":1,\"139\":3}}],[\"1​​\",{\"1\":{\"126\":1}}],[\"1−0\",{\"1\":{\"126\":5}}],[\"1或0\",{\"1\":{\"106\":1}}],[\"1716402475924\",{\"1\":{\"79\":1}}],[\"1c9bddb9bdb0\",{\"1\":{\"79\":1}}],[\"1e\",{\"1\":{\"68\":1}}],[\"1²\",{\"1\":{\"61\":1}}],[\"1|\",{\"1\":{\"61\":4}}],[\"1∗log\",{\"1\":{\"54\":2}}],[\"1029\",{\"1\":{\"77\":1}}],[\"101\",{\"1\":{\"61\":1}}],[\"100²\",{\"1\":{\"61\":1}}],[\"100\",{\"1\":{\"61\":5}}],[\"10000^\",{\"1\":{\"178\":1}}],[\"1000\",{\"1\":{\"39\":1}}],[\"10\",{\"1\":{\"54\":2,\"61\":2,\"77\":1}}],[\"196\",{\"1\":{\"30\":1}}],[\"14\",{\"1\":{\"30\":2}}],[\"1607\",{\"1\":{\"175\":1}}],[\"16\",{\"1\":{\"30\":4,\"58\":1,\"61\":1}}],[\"16x16x3\",{\"1\":{\"30\":1}}],[\"16x16\",{\"1\":{\"30\":1}}],[\"1\",{\"0\":{\"24\":1,\"33\":1,\"36\":1,\"45\":1,\"51\":1,\"56\":1,\"65\":1,\"82\":1,\"102\":1,\"106\":1,\"110\":1,\"166\":1,\"169\":1,\"177\":1,\"180\":1},\"1\":{\"13\":1,\"15\":2,\"22\":1,\"34\":1,\"41\":2,\"53\":1,\"54\":7,\"60\":3,\"61\":13,\"75\":4,\"79\":3,\"106\":8,\"115\":1,\"119\":1,\"124\":1,\"126\":2,\"148\":1,\"151\":1,\"152\":5,\"157\":1,\"174\":2,\"175\":1,\"178\":2,\"186\":1,\"187\":3}}],[\"来弥补归一化过程中损失的模型表达能力\",{\"1\":{\"174\":1}}],[\"来推导\",{\"1\":{\"154\":1}}],[\"来说\",{\"1\":{\"138\":1}}],[\"来自序列b\",{\"1\":{\"135\":1}}],[\"来自序列a\",{\"1\":{\"135\":1}}],[\"来源\",{\"1\":{\"135\":1}}],[\"来捕捉输入的不同表示子空间\",{\"1\":{\"117\":1}}],[\"来最小化\",{\"1\":{\"114\":1}}],[\"来完成的\",{\"1\":{\"111\":1}}],[\"来指导策略更新\",{\"1\":{\"107\":1}}],[\"来避免策略过早地变得狭隘和确定\",{\"1\":{\"105\":1}}],[\"来预测目标块的表示\",{\"1\":{\"97\":1}}],[\"来提取图像的紧凑特征表示\",{\"1\":{\"92\":1}}],[\"来提出一个新的状态θ\",{\"1\":{\"13\":1}}],[\"来寻找预测框与真实框之间的最佳二分匹配\",{\"1\":{\"91\":1}}],[\"来评估在某个状态下执行某个动作有多好\",{\"1\":{\"82\":1}}],[\"来计算\",{\"1\":{\"74\":1}}],[\"来计算损失函数关于参数\",{\"1\":{\"24\":1}}],[\"来近似它\",{\"1\":{\"45\":1}}],[\"来近似后验分布的形状\",{\"1\":{\"14\":1}}],[\"来理解输入序列\",{\"1\":{\"29\":1}}],[\"来生成输出序列\",{\"1\":{\"29\":1}}],[\"来生成\",{\"1\":{\"22\":1}}],[\"来逼近真实的后验分布\",{\"1\":{\"22\":1}}],[\"来更新深度学习中神经网络权重的过程涉及到几个步骤\",{\"1\":{\"18\":1}}],[\"来更新θ\",{\"1\":{\"12\":1}}],[\"来得到更准确的结果\",{\"1\":{\"14\":1}}],[\"来找到最佳的ϕ\",{\"1\":{\"13\":1}}],[\"来决定是否接受这个提议\",{\"1\":{\"13\":1}}],[\"使小模型在保持较高性能的同时显著降低计算成本和存储需求\",{\"1\":{\"185\":1}}],[\"使梯度更加稳定\",{\"1\":{\"169\":1}}],[\"使模型关注不同方面的信息\",{\"1\":{\"125\":1}}],[\"使其以更小的体量达到接近教师的性能\",{\"1\":{\"192\":1}}],[\"使其在许多自然语言处理任务中成为首选\",{\"1\":{\"184\":1}}],[\"使其重新调整到一个稳定的尺度上\",{\"1\":{\"167\":1}}],[\"使其更适应图像分类等视觉任务\",{\"1\":{\"29\":1}}],[\"使其的平稳分布就是我们要求的后验分布p\",{\"1\":{\"13\":1}}],[\"使用上一步计算得到的均值和方差\",{\"1\":{\"174\":1}}],[\"使用独立的权重矩阵进行投影\",{\"1\":{\"119\":1}}],[\"使用高效的并行扫描算法\",{\"1\":{\"114\":1}}],[\"使用静态参数\",{\"1\":{\"113\":1}}],[\"使用多个模型来避免单一模型的崩溃\",{\"1\":{\"104\":1}}],[\"使用能提供更平滑\",{\"1\":{\"104\":1}}],[\"使用l2正则化\",{\"1\":{\"67\":1}}],[\"使用起来非常方便\",{\"1\":{\"66\":1}}],[\"使用\",{\"1\":{\"47\":2,\"114\":2,\"160\":1}}],[\"使用带有动量项的优化算法\",{\"1\":{\"20\":1}}],[\"使用随机变分推断\",{\"1\":{\"18\":1}}],[\"使得所有特征值都在相同的数值范围内\",{\"1\":{\"174\":1}}],[\"使得模型能够自动捕获序列中元素之间的相对距离\",{\"1\":{\"180\":1}}],[\"使得模型能够捕获相对位置关系\",{\"1\":{\"176\":1}}],[\"使得模型能够学习到更加语义化和高级别的特征\",{\"1\":{\"96\":1}}],[\"使得模型在训练过程中能够更好地学习和适应不同的数据分布\",{\"1\":{\"174\":1}}],[\"使得模型训练更加稳定\",{\"1\":{\"174\":1}}],[\"使得网络对参数的初始化和学习率不那么敏感\",{\"1\":{\"171\":1}}],[\"使得策略在更长的时间内保持其随机性和探索性\",{\"1\":{\"106\":1}}],[\"使得训练和推理过程更加一体化\",{\"1\":{\"93\":1}}],[\"使得目标检测流程更加简洁直观\",{\"1\":{\"93\":1}}],[\"使得基于梯度的优化成为可能\",{\"1\":{\"48\":1}}],[\"使得输入序列的长度与图像分辨率的平方成正比\",{\"1\":{\"31\":1}}],[\"使得标准的反向传播算法得以应用\",{\"1\":{\"26\":1}}],[\"使得似然函数最大\",{\"1\":{\"18\":1}}],[\"使得它们能够在拟合数据\",{\"1\":{\"18\":1}}],[\"使得其最大化了elbo\",{\"1\":{\"18\":1}}],[\"使得w=μ+σ⊙ϵ\",{\"1\":{\"18\":1}}],[\"使得q\",{\"1\":{\"12\":1}}],[\"我将总结rotary\",{\"1\":{\"176\":1}}],[\"我将使用latex数学公式来解释mcmc和变分推断的一些关键概念和公式\",{\"1\":{\"13\":1}}],[\"我在状态s下执行动作a\",{\"1\":{\"86\":1}}],[\"我们指出bn并不适用于rnn等动态网络和batchsize较小的时候效果不好\",{\"1\":{\"174\":1}}],[\"我们计算这m个样本的均值和方差\",{\"1\":{\"174\":1}}],[\"我们计算∂μ∂\",{\"1\":{\"18\":1}}],[\"我们再次回顾前向传播的步骤\",{\"1\":{\"154\":1}}],[\"我们再夸张一点\",{\"1\":{\"61\":1}}],[\"我们想知道这个词序列中不同词之间的关联程度\",{\"1\":{\"138\":1}}],[\"我们想推断的\",{\"1\":{\"45\":1}}],[\"我们用归一化后的值乘以γ\",{\"1\":{\"174\":1}}],[\"我们用每个样本的值减去该神经元的均值\",{\"1\":{\"174\":1}}],[\"我们用一个非常简单的例子来说明\",{\"1\":{\"136\":1}}],[\"我们用计算出的梯度来更新变分分布的参数\",{\"1\":{\"18\":1}}],[\"我们得到了一个概率分布\",{\"1\":{\"139\":1}}],[\"我们得到适用于离散序列\",{\"1\":{\"111\":1}}],[\"我们得到的新矩阵\",{\"1\":{\"73\":1}}],[\"我们得到的不仅是一组能使得预测性能最优的权重\",{\"1\":{\"12\":1}}],[\"我们来详细拆解一下\",{\"1\":{\"136\":1}}],[\"我们来详细深入地讲解一下\",{\"1\":{\"109\":1}}],[\"我们来一步一步地理解\",{\"1\":{\"55\":1}}],[\"我们知道用户2是\",{\"1\":{\"75\":1}}],[\"我们对这个矩阵进行\",{\"1\":{\"75\":1}}],[\"我们一步一步来思考\",{\"1\":{\"71\":1}}],[\"我们首先定义一个中间分布\",{\"1\":{\"51\":1}}],[\"我们同时实现了两个目标\",{\"1\":{\"48\":1}}],[\"我们注意到最后一项\",{\"1\":{\"46\":1}}],[\"我们展开这个kl散度\",{\"1\":{\"46\":1}}],[\"我们观测到的数据\",{\"1\":{\"45\":1}}],[\"我们只关心序列第一个位置\",{\"1\":{\"30\":1}}],[\"我们称之为\",{\"1\":{\"30\":1}}],[\"我们就可以认为他们有相似偏好\",{\"1\":{\"73\":1}}],[\"我们就可以利用标准的自动微分工具\",{\"1\":{\"24\":1}}],[\"我们就从一个简单的分布族\",{\"1\":{\"45\":1}}],[\"我们就得到了新的权重分布q\",{\"1\":{\"12\":1}}],[\"我们将前两项重新排列\",{\"1\":{\"46\":1}}],[\"我们将\",{\"1\":{\"22\":1,\"178\":1}}],[\"我们也无法简单地假设所有的深度神经网络的权重都符合正态分布\",{\"1\":{\"20\":1}}],[\"我们通过计算\",{\"1\":{\"138\":1}}],[\"我们通过对马尔可夫链进行随机游走来生成后验样本\",{\"1\":{\"13\":1}}],[\"我们通常称之为模式坍塌\",{\"1\":{\"101\":1}}],[\"我们通常会对权重进行初始化\",{\"1\":{\"20\":1}}],[\"我们通常想要找到一组参数\",{\"1\":{\"18\":1}}],[\"我们可能会假设权重来自一个以零为中心的正态分布\",{\"1\":{\"18\":1}}],[\"我们可以对输入样本进行归一化\",{\"1\":{\"174\":1}}],[\"我们可以得到\",{\"1\":{\"156\":1}}],[\"我们可以直接写出\",{\"1\":{\"155\":1}}],[\"我们可以看到模型在处理一个词时\",{\"1\":{\"133\":1}}],[\"我们可以使用卷积模式并行处理整个序列\",{\"1\":{\"112\":1}}],[\"我们可以使用随机变分推断或mcmc等方法来近似这个后验分布\",{\"1\":{\"19\":1}}],[\"我们可以定义一个核\",{\"1\":{\"112\":1}}],[\"我们可以用一个比喻来理解\",{\"1\":{\"108\":1}}],[\"我们可以将其理解为对更著名的layernorm\",{\"1\":{\"164\":1}}],[\"我们可以将强化学习算法的结构主要分为三大类\",{\"1\":{\"80\":1}}],[\"我们可以将似然函数视为参数θ的函数\",{\"1\":{\"18\":1}}],[\"我们可以只保留前\",{\"1\":{\"73\":1}}],[\"我们可以通过最小化这个对数似然性来估计模型的参数和数据的不确定性\",{\"1\":{\"19\":1}}],[\"我们可以假设预测值y遵循一个以f\",{\"1\":{\"19\":1}}],[\"我们可以初始化权重向量w和变分分布的参数\",{\"1\":{\"18\":1}}],[\"我们可以在下一次前向传播时\",{\"1\":{\"12\":1}}],[\"我们从后往前\",{\"1\":{\"154\":1}}],[\"我们从变分分布q\",{\"1\":{\"18\":1}}],[\"我们从一个建议分布q\",{\"1\":{\"15\":1}}],[\"我们丢弃前n个抽样结果\",{\"1\":{\"15\":1}}],[\"我们保持当前的权重值\",{\"1\":{\"15\":1}}],[\"我们生成一个\",{\"1\":{\"15\":1}}],[\"我们使用重参数化技巧\",{\"1\":{\"18\":1}}],[\"我们使用mcmc抽样方法来抽样新的权重值\",{\"1\":{\"15\":1}}],[\"我们使用变分推理来近似这个后验分布\",{\"1\":{\"12\":1}}],[\"我们有三个不同的权重矩阵\",{\"1\":{\"137\":1}}],[\"我们有一个不完整的评分矩阵\",{\"1\":{\"73\":1}}],[\"我们有\",{\"1\":{\"15\":2,\"18\":1}}],[\"我们有p\",{\"1\":{\"13\":1}}],[\"我们假设神经网络的权重矩阵为w\",{\"1\":{\"15\":1}}],[\"我们选择一个参数化的逼近分布q\",{\"1\":{\"13\":1}}],[\"我们在每一步中都会根据一个提议分布q\",{\"1\":{\"13\":1}}],[\"我们构建一个马尔可夫链\",{\"1\":{\"13\":1}}],[\"我们需要计算其在所有\",{\"1\":{\"174\":1}}],[\"我们需要计算损失函数对于每个参数的梯度\",{\"1\":{\"174\":1}}],[\"我们需要计算elbo关于θ的梯度\",{\"1\":{\"12\":1}}],[\"我们需要对输入的mini\",{\"1\":{\"174\":1}}],[\"我们需要先看layernorm做了什么\",{\"1\":{\"168\":1}}],[\"我们需要先深入了解它在传统\",{\"1\":{\"28\":1}}],[\"我们需要看所有查询向量\",{\"1\":{\"158\":1}}],[\"我们需要看这个\",{\"1\":{\"158\":1}}],[\"我们需要将模型离散化\",{\"1\":{\"111\":1}}],[\"我们需要分层拆解它的核心组件和数学原理\",{\"1\":{\"109\":1}}],[\"我们需要一个衡量分布\",{\"1\":{\"46\":1}}],[\"我们需要最大化证据下界\",{\"1\":{\"22\":1}}],[\"我们需要初始化网络的权重\",{\"1\":{\"18\":1}}],[\"我们需要优化一个叫做变分下界\",{\"1\":{\"12\":1}}],[\"我们的最小化kl散度的原始目标\",{\"1\":{\"46\":1}}],[\"我们的目标是最小化这个差异\",{\"1\":{\"46\":1}}],[\"我们的目标是最大化证据下界\",{\"1\":{\"18\":1}}],[\"我们的目标是最大化这个elbo\",{\"1\":{\"12\":1}}],[\"我们的目标是找到一个简单的参数化分布族\",{\"1\":{\"22\":1}}],[\"我们的目标是找到最优的变分分布\",{\"1\":{\"18\":1}}],[\"我们的目标是找到最优的参数θ\",{\"1\":{\"12\":1}}],[\"我们的目标是从权重w的后验分布p\",{\"1\":{\"15\":1}}],[\"我们的目标是从后验分布中抽样\",{\"1\":{\"15\":1}}],[\"我们的数据是\",{\"1\":{\"12\":1}}],[\"我们引入一个变分分布\",{\"1\":{\"12\":1}}],[\"我们希望计算权重的后验分布\",{\"1\":{\"12\":1}}],[\"优势函数为策略更新提供了更精确的信号\",{\"1\":{\"107\":1}}],[\"优势函数\",{\"0\":{\"107\":1}}],[\"优势\",{\"1\":{\"93\":1,\"106\":1,\"112\":1}}],[\"优化算法\",{\"1\":{\"20\":1}}],[\"优化难度降低\",{\"1\":{\"188\":1}}],[\"优化难度\",{\"1\":{\"13\":1}}],[\"优点\",{\"1\":{\"13\":2,\"82\":1,\"83\":1,\"84\":1,\"86\":2,\"87\":2,\"88\":1,\"180\":1,\"181\":1,\"182\":1,\"183\":1}}],[\"选择输出的词语\",{\"1\":{\"163\":1}}],[\"选择性机制让模型可以像注意力一样\",{\"1\":{\"113\":1}}],[\"选择性\",{\"0\":{\"113\":1},\"1\":{\"113\":3,\"115\":1}}],[\"选择一个动作\",{\"1\":{\"83\":1}}],[\"选择一个合适的逼近分布可能会很困难\",{\"1\":{\"13\":1}}],[\"选择得分最高的动作执行\",{\"1\":{\"82\":1}}],[\"选择哪种方法取决于具体的任务和需求\",{\"1\":{\"13\":1}}],[\"选择逼近分布\",{\"1\":{\"13\":1}}],[\"逼近误差\",{\"1\":{\"13\":1}}],[\"如具体算法实现\",{\"1\":{\"192\":1}}],[\"如dafl\",{\"1\":{\"190\":1}}],[\"如deep\",{\"1\":{\"190\":1}}],[\"如fitnets\",{\"1\":{\"190\":1}}],[\"如蒸馏版resnet\",{\"1\":{\"189\":1}}],[\"如交叉熵损失\",{\"1\":{\"187\":1}}],[\"如kl散度损失\",{\"1\":{\"187\":1}}],[\"如类别间相似性\",{\"1\":{\"186\":1}}],[\"如分类任务中各类别的概率\",{\"1\":{\"186\":1}}],[\"如mobilenet\",{\"1\":{\"186\":1}}],[\"如mae\",{\"1\":{\"96\":1}}],[\"如bert\",{\"1\":{\"186\":1}}],[\"如bert中的学习位置嵌入\",{\"1\":{\"180\":1}}],[\"如群论和正交变换\",{\"1\":{\"181\":1}}],[\"如transformer\",{\"1\":{\"180\":1}}],[\"如tensorflow或pytorch中的反向传播算法\",{\"1\":{\"24\":1}}],[\"如绝对位置编码\",{\"1\":{\"179\":1}}],[\"如图1所示\",{\"1\":{\"174\":1}}],[\"如图像\",{\"1\":{\"69\":1}}],[\"如rkd\",{\"1\":{\"190\":1}}],[\"如rnn\",{\"1\":{\"174\":1}}],[\"如resnet\",{\"1\":{\"92\":1}}],[\"如无必要\",{\"1\":{\"173\":1}}],[\"如贪婪搜索或束搜索\",{\"1\":{\"163\":1}}],[\"如稀疏注意力\",{\"1\":{\"153\":1}}],[\"如翻译\",{\"1\":{\"135\":1}}],[\"如gpu\",{\"1\":{\"133\":1,\"183\":1}}],[\"如gibbs采样\",{\"1\":{\"13\":1}}],[\"如局部依赖\",{\"1\":{\"128\":1}}],[\"如语法\",{\"1\":{\"125\":1}}],[\"如语言建模或代码生成\",{\"1\":{\"182\":1}}],[\"如语言\",{\"1\":{\"109\":1}}],[\"如百万长度级别的上下文\",{\"1\":{\"115\":1}}],[\"如复制\",{\"1\":{\"113\":1}}],[\"如文本\",{\"1\":{\"111\":1}}],[\"如掩码自编码器\",{\"1\":{\"96\":1}}],[\"如非极大值抑制\",{\"1\":{\"90\":1}}],[\"如石头剪刀布\",{\"1\":{\"83\":1}}],[\"如q\",{\"1\":{\"82\":1}}],[\"如pytorch\",{\"1\":{\"66\":1}}],[\"如一个高斯分布\",{\"1\":{\"47\":1}}],[\"如何生成更好的\",{\"1\":{\"159\":1}}],[\"如何工作\",{\"1\":{\"131\":1}}],[\"如何保留或忘记信息\",{\"1\":{\"110\":1}}],[\"如何随时间演变\",{\"1\":{\"110\":1}}],[\"如何学\",{\"0\":{\"85\":1}}],[\"如何利用\",{\"1\":{\"74\":1}}],[\"如何做\",{\"1\":{\"73\":4}}],[\"如何使用\",{\"1\":{\"67\":1,\"68\":1}}],[\"如何找到这个\",{\"1\":{\"45\":1}}],[\"如何衡量\",{\"1\":{\"45\":1}}],[\"如何变化\",{\"1\":{\"25\":1}}],[\"如\",{\"1\":{\"29\":1,\"31\":1,\"67\":1,\"74\":1,\"113\":1,\"115\":1,\"118\":1,\"119\":2,\"186\":1,\"188\":1}}],[\"如单词序列\",{\"1\":{\"29\":1}}],[\"如机器翻译\",{\"1\":{\"29\":1}}],[\"如adam\",{\"1\":{\"20\":1}}],[\"如果预测错了\",{\"1\":{\"159\":1}}],[\"如果有一个向量输入\",{\"1\":{\"157\":1}}],[\"如果一批样本都差不多\",{\"1\":{\"104\":1}}],[\"如果算法过分强调\",{\"1\":{\"103\":1}}],[\"如果环境模型学得不准\",{\"1\":{\"86\":1}}],[\"如果你有明确的模型轻量化\",{\"1\":{\"70\":1}}],[\"如果\",{\"1\":{\"25\":1,\"30\":1,\"52\":2,\"106\":2}}],[\"如果网络的某一层主要学习的特征在数据中的分布是非对称的\",{\"1\":{\"20\":1}}],[\"如果u<α\",{\"1\":{\"15\":1}}],[\"如果使用metropolis\",{\"1\":{\"15\":1}}],[\"如果接受了新的权重值\",{\"1\":{\"15\":1}}],[\"如果这个比值大于1\",{\"1\":{\"15\":1}}],[\"如果需要进一步探讨某个细节\",{\"1\":{\"192\":1}}],[\"如果需要处理大数据集\",{\"1\":{\"13\":1}}],[\"如果需要更精确的结果\",{\"1\":{\"13\":1}}],[\"如随机梯度下降\",{\"1\":{\"13\":1}}],[\"可学习参数\",{\"1\":{\"172\":1}}],[\"可解释性\",{\"1\":{\"133\":1}}],[\"可见的\",{\"1\":{\"97\":1}}],[\"可视化\",{\"1\":{\"74\":1,\"141\":1}}],[\"可微的估计\",{\"1\":{\"47\":1}}],[\"可微分的计算\",{\"1\":{\"24\":1}}],[\"可导性\",{\"1\":{\"153\":1}}],[\"可导\",{\"1\":{\"25\":1}}],[\"可导的函数\",{\"1\":{\"22\":1}}],[\"可能引入过拟合风险\",{\"1\":{\"181\":1}}],[\"可能在一次更新后\",{\"1\":{\"106\":1}}],[\"可能发现\",{\"1\":{\"103\":1}}],[\"可能无关紧要的像素细节\",{\"1\":{\"96\":1}}],[\"可能无定义\",{\"1\":{\"53\":1}}],[\"可能是噪声的模式\",{\"1\":{\"73\":1}}],[\"可能为零的问题\",{\"1\":{\"51\":1}}],[\"可能需要采取其他的近似方法\",{\"1\":{\"14\":1}}],[\"可能会使权重的分布在训练过程中变得更加复杂\",{\"1\":{\"20\":1}}],[\"可能会忽略尾部的信息\",{\"1\":{\"14\":1}}],[\"可能会选择变分推断\",{\"1\":{\"13\":1}}],[\"可能会选择mcmc\",{\"1\":{\"13\":1}}],[\"可能会陷入局部最优\",{\"1\":{\"13\":1}}],[\"可以通过向量化操作高效实现\",{\"1\":{\"183\":1}}],[\"可以通过神经网络的损失函数来定义\",{\"1\":{\"15\":1}}],[\"可以是\",{\"1\":{\"178\":1}}],[\"可以同时进行\",{\"1\":{\"133\":1}}],[\"可以利用\",{\"1\":{\"112\":1}}],[\"可以展开为输入\",{\"1\":{\"112\":1}}],[\"可以像\",{\"1\":{\"112\":1}}],[\"可以从评论家\",{\"1\":{\"103\":1}}],[\"可以将vit看作是构建i\",{\"1\":{\"99\":1}}],[\"可以重复利用旧数据\",{\"1\":{\"87\":1}}],[\"可以把过去的所有经验\",{\"1\":{\"87\":1}}],[\"可以预测用户3会给电影a低分\",{\"1\":{\"75\":1}}],[\"可以帮助我们找到那个\",{\"1\":{\"73\":1}}],[\"可以做一件非常厉害的事情\",{\"1\":{\"72\":1}}],[\"可以尝试l1正则化\",{\"1\":{\"70\":1}}],[\"可以\",{\"1\":{\"68\":1}}],[\"可以轻松地扩展模型规模\",{\"1\":{\"31\":1}}],[\"可以有效防止梯度消失和爆炸问题\",{\"1\":{\"30\":1}}],[\"可以说\",{\"1\":{\"26\":1}}],[\"可以假设为高斯分布\",{\"1\":{\"15\":1}}],[\"可以使用随机初始化或预训练模型将初始权重设置为w0​\",{\"1\":{\"15\":1}}],[\"可以直接用于预测或决策\",{\"1\":{\"13\":1}}],[\"可以生成精确的后验样本\",{\"1\":{\"13\":1}}],[\"可扩展性\",{\"1\":{\"13\":1}}],[\"计算高效\",{\"1\":{\"184\":1}}],[\"计算输入样本的均值和方差\",{\"1\":{\"174\":1}}],[\"计算输出向量\",{\"0\":{\"140\":1}}],[\"计算输出\",{\"1\":{\"114\":1}}],[\"计算均值和方差\",{\"1\":{\"174\":1}}],[\"计算开销\",{\"1\":{\"172\":1}}],[\"计算量更小\",{\"1\":{\"171\":1}}],[\"计算出的分数受到了多大影响\",{\"1\":{\"158\":2}}],[\"计算关于\",{\"0\":{\"155\":1,\"156\":1,\"157\":1,\"158\":1}}],[\"计算注意力分数\",{\"0\":{\"138\":1}}],[\"计算过程可以分为以下几步\",{\"1\":{\"136\":1}}],[\"计算相关性\",{\"1\":{\"131\":1}}],[\"计算复杂度\",{\"1\":{\"127\":1,\"153\":1}}],[\"计算挑战与高效算法\",{\"0\":{\"114\":1}}],[\"计算机视觉\",{\"1\":{\"74\":1}}],[\"计算所有参数的l1范数\",{\"1\":{\"68\":1}}],[\"计算涉及平方和开方\",{\"1\":{\"61\":1}}],[\"计算简单\",{\"1\":{\"61\":1}}],[\"计算方式\",{\"1\":{\"61\":1}}],[\"计算示例\",{\"1\":{\"58\":1,\"59\":1}}],[\"计算公式\",{\"1\":{\"58\":1,\"59\":1}}],[\"计算js散度\",{\"1\":{\"54\":1}}],[\"计算kl散度\",{\"1\":{\"54\":1}}],[\"计算两个分布\",{\"1\":{\"50\":1}}],[\"计算可行性\",{\"1\":{\"47\":1}}],[\"计算它需要对所有可能的\",{\"1\":{\"45\":1}}],[\"计算后验分布\",{\"1\":{\"45\":1}}],[\"计算图的结构是\",{\"1\":{\"24\":1}}],[\"计算梯度\",{\"1\":{\"18\":1}}],[\"计算梯度的过程就是所谓的反向传播\",{\"1\":{\"12\":1}}],[\"计算\",{\"1\":{\"13\":1,\"63\":1,\"145\":1}}],[\"计算效率和简洁性\",{\"0\":{\"183\":1}}],[\"计算效率更高\",{\"1\":{\"171\":1}}],[\"计算效率\",{\"1\":{\"13\":1,\"61\":1,\"125\":1}}],[\"计算成本\",{\"1\":{\"13\":1}}],[\"通道数\",{\"1\":{\"30\":1}}],[\"通过教师模型的\",{\"1\":{\"192\":1}}],[\"通过旋转操作将相对位置信息嵌入到transformer模型中\",{\"1\":{\"184\":1}}],[\"通过上述步骤\",{\"1\":{\"174\":1}}],[\"通过上下文编码器\",{\"1\":{\"99\":1}}],[\"通过减少内部协变量偏移\",{\"1\":{\"174\":1}}],[\"通过无数次这样的调整\",{\"1\":{\"159\":1}}],[\"通过三个权重矩阵\",{\"1\":{\"144\":1}}],[\"通过可视化注意力权重\",{\"1\":{\"133\":1}}],[\"通过计算\",{\"1\":{\"131\":1}}],[\"通过计算对数后验概率的hessian矩阵\",{\"1\":{\"14\":1}}],[\"通过设计一种硬件感知\",{\"1\":{\"114\":1}}],[\"通过设计混合编码器和实现iou感知的查询选择\",{\"1\":{\"93\":1}}],[\"通过一个隐藏状态\",{\"1\":{\"110\":1}}],[\"通过关注\",{\"1\":{\"107\":1}}],[\"通过将图像分割成块\",{\"1\":{\"99\":1}}],[\"通过让模型重建被遮挡\",{\"1\":{\"96\":1}}],[\"通过理解世界的基本模式来学习\",{\"1\":{\"95\":1}}],[\"通过改进训练方法和引入对比去噪训练等技巧\",{\"1\":{\"93\":1}}],[\"通过引入可变形注意力模块\",{\"1\":{\"93\":1}}],[\"通过多头自注意力机制来学习图像的全局上下文信息\",{\"1\":{\"92\":1}}],[\"通过限制每次策略更新的幅度来提高稳定性\",{\"1\":{\"83\":1}}],[\"通过丢弃它们\",{\"1\":{\"73\":1}}],[\"通过各种方式增加模型鲁棒性\",{\"1\":{\"70\":1}}],[\"通过l1正则化\",{\"1\":{\"68\":1}}],[\"通过在损失函数中增加模型权重\",{\"1\":{\"65\":2}}],[\"通过最大化elbo\",{\"1\":{\"48\":1}}],[\"通过最小化对数似然性来估计\",{\"1\":{\"19\":1}}],[\"通过调整模型深度\",{\"1\":{\"31\":1}}],[\"通过堆叠卷积层来逐步扩大感受野\",{\"1\":{\"31\":1}}],[\"通过自注意力机制\",{\"1\":{\"29\":1}}],[\"通过\",{\"1\":{\"22\":1,\"141\":1}}],[\"通过这种方式\",{\"1\":{\"22\":1}}],[\"通过以下确定性变换来得到\",{\"1\":{\"22\":1}}],[\"通常省略了偏移参数\",{\"1\":{\"167\":1}}],[\"通常设\",{\"1\":{\"144\":1}}],[\"通常可以被可视化\",{\"1\":{\"141\":1}}],[\"通常可忽略或设为\",{\"1\":{\"110\":1}}],[\"通常是庞大\",{\"1\":{\"186\":1}}],[\"通常是0\",{\"1\":{\"106\":1}}],[\"通常是一个简单的多层感知机\",{\"1\":{\"30\":1}}],[\"通常是一个全连接层\",{\"1\":{\"29\":1}}],[\"通常被称为\",{\"1\":{\"103\":1}}],[\"通常更窄\",{\"1\":{\"97\":1}}],[\"通常使用一个更简单的近似\",{\"1\":{\"111\":1}}],[\"通常使用一个预训练的卷积神经网络\",{\"1\":{\"92\":1}}],[\"通常使用l2距离来衡量\",{\"1\":{\"97\":1}}],[\"通常使用随机梯度下降\",{\"1\":{\"18\":1}}],[\"通常需要海量的样本才能学好\",{\"1\":{\"86\":1}}],[\"通常需要手动将其添加到损失函数中\",{\"1\":{\"68\":1}}],[\"通常难以处理连续的动作空间\",{\"1\":{\"82\":1}}],[\"通常的初始化方法有很多种\",{\"1\":{\"20\":1}}],[\"通常用于归一化\",{\"1\":{\"15\":1}}],[\"通常用于估计复杂模型的后验分布\",{\"1\":{\"14\":1}}],[\"通常我们假设这个分布是高斯分布\",{\"1\":{\"12\":1}}],[\"通用性\",{\"1\":{\"13\":1}}],[\"从上游传回来的梯度\",{\"1\":{\"157\":1}}],[\"从输入向量创建query\",{\"0\":{\"137\":1}}],[\"从连续时间到离散序列\",{\"0\":{\"111\":1}}],[\"从本质上讲\",{\"1\":{\"95\":1}}],[\"从像素玩游戏\",{\"1\":{\"82\":1}}],[\"从resnet到bert\",{\"1\":{\"67\":1}}],[\"从下图可以清晰地看到两者的几何差异\",{\"1\":{\"60\":1}}],[\"从点到点的路径\",{\"0\":{\"56\":1}}],[\"从直观理解开始\",{\"0\":{\"56\":1}}],[\"从\",{\"1\":{\"53\":2}}],[\"从定义就可以直接看出\",{\"1\":{\"52\":1}}],[\"从文本到图像的革命性跨越\",{\"0\":{\"28\":1}}],[\"从参数\",{\"1\":{\"24\":1}}],[\"从标准正态分布\",{\"1\":{\"22\":1}}],[\"从而更好地适应长序列\",{\"1\":{\"182\":1}}],[\"从而更紧地逼近真实的\",{\"1\":{\"46\":1}}],[\"从而提升性能\",{\"1\":{\"176\":1}}],[\"从而提高泛化能力\",{\"1\":{\"67\":1}}],[\"从而让神经网络更好地学习\",{\"1\":{\"174\":1}}],[\"从而带动了整个业界的采纳\",{\"1\":{\"173\":1}}],[\"从而稳定了层的输入分布\",{\"1\":{\"171\":1}}],[\"从而缓解训练过程中的内部协变量偏移\",{\"1\":{\"169\":1}}],[\"从而生成三组向量\",{\"1\":{\"137\":1}}],[\"从而计算出一种新的\",{\"1\":{\"130\":1}}],[\"从而在保持甚至提升模型表现的同时\",{\"1\":{\"173\":1}}],[\"从而在保持递归本质的同时实现了高效的并行化\",{\"1\":{\"114\":1}}],[\"从而在计算上是可行的\",{\"1\":{\"31\":1}}],[\"从而成为了对抗策略\",{\"1\":{\"108\":1}}],[\"从而欺骗或满足了当时的\",{\"1\":{\"104\":1}}],[\"从而强化对这个局部最优策略的评估\",{\"1\":{\"103\":1}}],[\"从而捕捉图像中的长距离依赖关系\",{\"1\":{\"99\":1}}],[\"从而降低了计算复杂度\",{\"1\":{\"93\":1}}],[\"从而实现了相对位置编码\",{\"1\":{\"177\":1}}],[\"从而实现了高效的计算和强大的性能\",{\"1\":{\"109\":1}}],[\"从而实现一对一的分配\",{\"1\":{\"91\":1}}],[\"从而实现特征选择\",{\"1\":{\"65\":1}}],[\"从而摒弃了传统目标检测方法中许多需要手动设计的组件\",{\"1\":{\"90\":1}}],[\"从而得到一个原始矩阵\",{\"1\":{\"73\":1}}],[\"从而得到精确的后验分布\",{\"1\":{\"13\":1}}],[\"从而减轻过拟合\",{\"1\":{\"65\":1}}],[\"从而拥有\",{\"1\":{\"31\":1}}],[\"从而将对参数的依赖从随机采样操作中解耦出来\",{\"1\":{\"26\":1}}],[\"从而能够训练更复杂\",{\"1\":{\"25\":1}}],[\"从而可以通过蒙特卡洛采样来近似这个梯度\",{\"1\":{\"24\":1}}],[\"从而使得整个模型可以进行端到端的梯度优化\",{\"1\":{\"21\":1}}],[\"从而引入了权重的分布\",{\"1\":{\"19\":1}}],[\"从这个新的分布中采样权重\",{\"1\":{\"12\":1}}],[\"精确性\",{\"1\":{\"13\":1}}],[\"精度和计算速度等方面存在一些差异\",{\"1\":{\"13\":1}}],[\"以下从几个关键点进行分析\",{\"1\":{\"179\":1}}],[\"以下将详细介绍rope的数学形式\",{\"1\":{\"176\":1}}],[\"以下是完整的数学推导\",{\"1\":{\"117\":1}}],[\"以下是详细的步骤\",{\"1\":{\"18\":1}}],[\"以下是使用mcmc进行深度神经网络权重更新的详细步骤\",{\"1\":{\"15\":1}}],[\"以下是这两种方法的优点和缺点的对比\",{\"1\":{\"13\":1}}],[\"以便能够学习更复杂的模式\",{\"1\":{\"174\":1}}],[\"以便用梯度下降来更新生成\",{\"1\":{\"154\":1}}],[\"以便扮演不同的角色\",{\"1\":{\"132\":1}}],[\"以稳定梯度\",{\"1\":{\"139\":1}}],[\"以循环方式计算整个序列的隐藏状态\",{\"1\":{\"114\":1}}],[\"以最大化短期回报\",{\"1\":{\"103\":1}}],[\"以权重衰减的形式集成\",{\"1\":{\"70\":1}}],[\"以2为底的对数时是\",{\"1\":{\"53\":1}}],[\"以及出色的长序列处理能力\",{\"1\":{\"184\":1}}],[\"以及在小batch\",{\"1\":{\"174\":1}}],[\"以及在面对新的\",{\"1\":{\"12\":1}}],[\"以及rmsnorm如何对其优化\",{\"1\":{\"168\":1}}],[\"以及注意力分数的计算和加权求和\",{\"1\":{\"137\":1}}],[\"以及如何构建和训练这个模型\",{\"1\":{\"99\":1}}],[\"以及对应的\",{\"1\":{\"73\":1}}],[\"以及\",{\"1\":{\"31\":1,\"159\":1}}],[\"以及一个解码器\",{\"1\":{\"29\":1}}],[\"以及权重的分布对网络性能的影响\",{\"1\":{\"20\":1}}],[\"以上就是在深度学习中使用svi来更新神经网络权重的详细步骤和相关的数学公式\",{\"1\":{\"18\":1}}],[\"但有时我们可能不希望归一化的数据严格位于0均值和单位标准差\",{\"1\":{\"174\":1}}],[\"但有时我们仍想解释哪些输入特征更重要\",{\"1\":{\"68\":1}}],[\"但非常重要\",{\"1\":{\"157\":1}}],[\"但非常有效地应对了策略上的\",{\"1\":{\"105\":1}}],[\"但因为它所有输入都来自同一序列\",{\"1\":{\"136\":1}}],[\"但q\",{\"1\":{\"132\":1}}],[\"但对模型来说是个挑战\",{\"1\":{\"131\":1}}],[\"但对异常值敏感\",{\"1\":{\"62\":1}}],[\"但单头维度\",{\"1\":{\"125\":1}}],[\"但简单的\",{\"1\":{\"114\":1}}],[\"但研究者发现它也可以被重新表述为一个卷积操作\",{\"1\":{\"112\":1}}],[\"但存在一个完全等价的问题\",{\"1\":{\"103\":1}}],[\"但如果在潜在空间中他们的向量很接近\",{\"1\":{\"73\":1}}],[\"但通过引入训练和测试时的不一致性\",{\"1\":{\"69\":1}}],[\"但不如rope直接和有效\",{\"1\":{\"180\":1}}],[\"但不如l2普遍\",{\"1\":{\"68\":1}}],[\"但不擅长推理\",{\"1\":{\"115\":1}}],[\"但不会让任何一个权重直接变为\",{\"1\":{\"62\":1}}],[\"但结果是合理的\",{\"1\":{\"54\":1}}],[\"但已经得到数学上的证明\",{\"1\":{\"52\":1}}],[\"但可能导致在任何一个模式上都拟合不佳\",{\"1\":{\"47\":1}}],[\"但可能会引入一些逼近误差\",{\"1\":{\"13\":1}}],[\"但它的数学性质使其可以作为一个真正的距离度量\",{\"1\":{\"52\":1}}],[\"但它会与其他图像块的表示进行交互\",{\"1\":{\"29\":1}}],[\"但它们大多可以根据几个关键的维度进行划分和理解\",{\"1\":{\"80\":1}}],[\"但它们都属于\",{\"1\":{\"30\":1}}],[\"但它们的角色和含义是不同的\",{\"1\":{\"18\":1}}],[\"但它们的含义和作用是不同的\",{\"1\":{\"18\":1}}],[\"但它们在处理复杂度\",{\"1\":{\"13\":1}}],[\"但其根本原因和现象是高度相似的\",{\"1\":{\"101\":1}}],[\"但其后续的众多改进模型已经证明了这一方向的巨大潜力和价值\",{\"1\":{\"93\":1}}],[\"但其原始版本也存在一些问题\",{\"1\":{\"93\":1}}],[\"但其梯度估计的方差通常非常高\",{\"1\":{\"25\":1}}],[\"但其仅适用于后验分布在最大后验\",{\"1\":{\"14\":1}}],[\"但我们的数据\",{\"1\":{\"111\":1}}],[\"但我们可以利用重参数技巧\",{\"1\":{\"22\":1}}],[\"但我会尽力将其分解为详细的步骤\",{\"1\":{\"12\":1}}],[\"但在潜在空间中\",{\"1\":{\"75\":1}}],[\"但在训练过程中\",{\"1\":{\"20\":1}}],[\"但在mcmc中我们不需要计算它\",{\"1\":{\"15\":1}}],[\"但是对于长尾分布\",{\"1\":{\"14\":1}}],[\"但计算成本高且收敛慢\",{\"1\":{\"13\":1}}],[\"和维度相关的频率\",{\"1\":{\"177\":1}}],[\"和键向量\",{\"1\":{\"178\":1}}],[\"和键\",{\"1\":{\"177\":1}}],[\"和transformer等模型中\",{\"1\":{\"174\":1}}],[\"和方差\",{\"1\":{\"171\":1}}],[\"和矩阵\",{\"1\":{\"162\":1}}],[\"和向量输出\",{\"1\":{\"157\":1}}],[\"和根据其与\",{\"1\":{\"140\":1}}],[\"和半监督评估\",{\"1\":{\"100\":1}}],[\"和锚框\",{\"1\":{\"90\":1}}],[\"和一个价值网络\",{\"1\":{\"84\":1}}],[\"和lasso回归一样\",{\"1\":{\"68\":1}}],[\"和lasso回归\",{\"1\":{\"64\":1}}],[\"和蒙特卡洛采样进行高效\",{\"1\":{\"47\":1}}],[\"和层归一化\",{\"1\":{\"30\":1}}],[\"和\",{\"0\":{\"158\":1},\"1\":{\"22\":2,\"46\":1,\"50\":1,\"51\":5,\"52\":2,\"53\":1,\"55\":2,\"60\":1,\"61\":1,\"62\":1,\"63\":1,\"73\":2,\"102\":1,\"114\":1,\"131\":2,\"138\":1,\"145\":1,\"155\":1,\"167\":1,\"172\":1,\"174\":1}}],[\"和数据不确定性\",{\"1\":{\"19\":1}}],[\"和遵循先验\",{\"1\":{\"18\":1}}],[\"和q\",{\"1\":{\"15\":1}}],[\"和p\",{\"1\":{\"15\":1}}],[\"和小批量\",{\"1\":{\"13\":1}}],[\"和变分推断\",{\"1\":{\"13\":1}}],[\"和先验分布p\",{\"1\":{\"12\":1}}],[\"最新论文方向\",{\"1\":{\"192\":1}}],[\"最核心的内容感知能力\",{\"1\":{\"115\":1}}],[\"最多将概率降低到旧策略的\",{\"1\":{\"106\":1}}],[\"最多只能提升到旧策略的\",{\"1\":{\"106\":1}}],[\"最基础的策略梯度算法\",{\"1\":{\"83\":1}}],[\"最应该存在的低秩矩阵\",{\"1\":{\"73\":1}}],[\"最有信号的\",{\"1\":{\"73\":1}}],[\"最主流的观影偏好\",{\"1\":{\"72\":1}}],[\"最显著的变化趋势\",{\"1\":{\"72\":1}}],[\"最大的奇异值\",{\"1\":{\"72\":1}}],[\"最大化elbo等价于最小化\",{\"1\":{\"48\":1}}],[\"最大化数据的似然\",{\"1\":{\"18\":1}}],[\"最广泛的应用\",{\"1\":{\"67\":1}}],[\"最常用\",{\"1\":{\"62\":1}}],[\"最短距离\",{\"1\":{\"56\":1}}],[\"最小化kl散度就是最大化elbo\",{\"1\":{\"46\":1}}],[\"最小化与先验分布的kl散度\",{\"1\":{\"18\":1}}],[\"最直接的选择之一就是kl散度\",{\"1\":{\"46\":1}}],[\"最接近的分布\",{\"1\":{\"45\":1}}],[\"最接近真实的后验分布p\",{\"1\":{\"12\":1}}],[\"最具开创性的改变\",{\"1\":{\"29\":1}}],[\"最优的变分分布将在kl散度意义下尽可能接近真实的后验分布\",{\"1\":{\"18\":1}}],[\"最后运用适当策略\",{\"1\":{\"163\":1}}],[\"最后通过nms来消除冗余的检测框\",{\"1\":{\"91\":1}}],[\"最后\",{\"1\":{\"18\":1,\"24\":1,\"157\":1}}],[\"最终每个词都带着对整句话的理解形成一个新的\",{\"1\":{\"131\":1}}],[\"最终输出维度\",{\"1\":{\"122\":1}}],[\"最终输出对各个类别的预测概率\",{\"1\":{\"30\":1}}],[\"最终预测出每个物体的类别和边界框坐标\",{\"1\":{\"92\":1}}],[\"最终只需要取出这个\",{\"1\":{\"29\":1}}],[\"最终\",{\"1\":{\"12\":1,\"54\":1,\"82\":1}}],[\"在手机\",{\"1\":{\"189\":1}}],[\"在长序列任务\",{\"1\":{\"182\":1}}],[\"在长序列上表现好\",{\"1\":{\"115\":1}}],[\"在transformer的注意力机制中\",{\"1\":{\"178\":1}}],[\"在transformer中\",{\"1\":{\"134\":1}}],[\"在图1中n表示样本轴\",{\"1\":{\"174\":1}}],[\"在文章的最后\",{\"1\":{\"174\":1}}],[\"在很多情况下可以被省略\",{\"1\":{\"171\":1}}],[\"在大规模模型中\",{\"1\":{\"171\":1}}],[\"在哪里关注\",{\"1\":{\"159\":1}}],[\"在通过\",{\"1\":{\"157\":1}}],[\"在标准\",{\"1\":{\"144\":1}}],[\"在扮演不同角色时\",{\"1\":{\"137\":1}}],[\"在生成\",{\"1\":{\"131\":1}}],[\"在语言建模\",{\"1\":{\"115\":1}}],[\"在历史中\",{\"1\":{\"113\":1}}],[\"在需要上下文依赖推理的任务\",{\"1\":{\"113\":1}}],[\"在状态s下\",{\"1\":{\"107\":1}}],[\"在状态s下执行动作a\",{\"1\":{\"82\":1}}],[\"在状态\",{\"1\":{\"106\":1}}],[\"在不让新策略偏离旧策略太远的前提下\",{\"1\":{\"106\":1}}],[\"在不同的场景下各有千秋\",{\"1\":{\"62\":1}}],[\"在优化目标里\",{\"1\":{\"104\":1}}],[\"在优化elbo时\",{\"1\":{\"47\":1}}],[\"在gan中\",{\"1\":{\"104\":1}}],[\"在gan的语境中\",{\"1\":{\"102\":1}}],[\"在actor选择动作时\",{\"1\":{\"104\":1}}],[\"在actor\",{\"1\":{\"103\":1,\"104\":1}}],[\"在抽象空间中预测被遮挡部分的表示\",{\"1\":{\"99\":1}}],[\"在抽象空间中进行预测\",{\"0\":{\"96\":1}}],[\"在线策略\",{\"0\":{\"87\":1},\"1\":{\"87\":1}}],[\"在学习的早期阶段通常更稳定\",{\"1\":{\"82\":1}}],[\"在当前状态下\",{\"1\":{\"82\":1}}],[\"在降维后的新空间\",{\"1\":{\"73\":1}}],[\"在原始数据中有多重要\",{\"1\":{\"72\":1}}],[\"在我们的例子里\",{\"1\":{\"72\":2}}],[\"在机器学习中扮演着\",{\"1\":{\"76\":1}}],[\"在机器学习中扮演的角色\",{\"1\":{\"71\":1}}],[\"在机器学习中的具体应用场景\",{\"0\":{\"74\":1}}],[\"在机器学习和数据科学中的应用\",{\"0\":{\"62\":1}}],[\"在训练集上训练一个高性能的复杂模型\",{\"1\":{\"187\":1}}],[\"在训练过程中持续监控验证集性能\",{\"1\":{\"69\":1}}],[\"在训练过程中\",{\"1\":{\"69\":1,\"91\":1,\"174\":1}}],[\"在训练神经网络时\",{\"1\":{\"18\":1}}],[\"在损失函数中增加模型权重的绝对值和作为惩罚项\",{\"1\":{\"68\":1}}],[\"在损失函数中加入模型权重的\",{\"1\":{\"62\":2}}],[\"在pytorch中\",{\"1\":{\"67\":1}}],[\"在定义优化器\",{\"1\":{\"67\":1}}],[\"在深度学习中的角色\",{\"1\":{\"70\":1}}],[\"在深度学习中的具体应用形式\",{\"0\":{\"66\":1}}],[\"在深度学习中\",{\"1\":{\"67\":1}}],[\"在深度学习框架\",{\"1\":{\"66\":1}}],[\"在深度神经网络中\",{\"1\":{\"15\":1,\"19\":1}}],[\"在城市地图上\",{\"1\":{\"56\":1}}],[\"在事件1\",{\"1\":{\"54\":1}}],[\"在事件3\",{\"1\":{\"54\":2}}],[\"在事件2\",{\"1\":{\"54\":2}}],[\"在公式中的位置交换了\",{\"1\":{\"52\":1}}],[\"在数学上称为度量函数\",{\"1\":{\"52\":1}}],[\"在计算上是可行的\",{\"1\":{\"47\":1}}],[\"在计算了梯度并更新了θ后\",{\"1\":{\"12\":1}}],[\"在处理高分辨率图像时\",{\"1\":{\"31\":1}}],[\"在每次参数更新时\",{\"1\":{\"67\":1}}],[\"在每一层中\",{\"1\":{\"30\":1}}],[\"在每个子层后\",{\"1\":{\"161\":1}}],[\"在每个子模块\",{\"1\":{\"30\":1}}],[\"在每个时间步都不同\",{\"1\":{\"114\":1}}],[\"在每个训练步骤中\",{\"1\":{\"18\":1}}],[\"在自注意力层之后\",{\"1\":{\"30\":1}}],[\"在上面的例子中\",{\"1\":{\"30\":1}}],[\"在经过所有\",{\"1\":{\"29\":1}}],[\"在\",{\"1\":{\"29\":1,\"30\":1,\"60\":1,\"160\":1}}],[\"在设计上进行了大胆的简化和调整\",{\"1\":{\"29\":1}}],[\"在未使用重参数技巧时\",{\"1\":{\"24\":1}}],[\"在贝叶斯变分推断\",{\"1\":{\"21\":1}}],[\"在贝叶斯推断中\",{\"1\":{\"18\":2}}],[\"在神经网络的训练开始时\",{\"1\":{\"20\":1}}],[\"在这个空间里\",{\"1\":{\"73\":1}}],[\"在这个新的计算图中\",{\"1\":{\"24\":1}}],[\"在这个流程中\",{\"1\":{\"24\":1}}],[\"在这个公式中\",{\"1\":{\"19\":2}}],[\"在这种情况下\",{\"1\":{\"18\":1}}],[\"在这里\",{\"1\":{\"13\":1}}],[\"在许多情况下\",{\"1\":{\"18\":1}}],[\"在随机变分推断中\",{\"1\":{\"18\":1}}],[\"在实际应用中\",{\"1\":{\"14\":1,\"108\":1,\"178\":1}}],[\"在变分推断中\",{\"1\":{\"13\":1,\"18\":1,\"22\":1,\"25\":1}}],[\"在变分推理中\",{\"1\":{\"12\":1}}],[\"在map点附近做二阶泰勒展开\",{\"1\":{\"14\":1}}],[\"在metropolis\",{\"1\":{\"13\":1,\"15\":1}}],[\"在mcmc中\",{\"1\":{\"13\":1,\"15\":2}}],[\"在足够多的迭代次数下\",{\"1\":{\"13\":1}}],[\"然后除以其标准差\",{\"1\":{\"174\":1}}],[\"然后再引入两个可训练的参数\",{\"1\":{\"174\":1}}],[\"然后再从每个元素中减去它\",{\"1\":{\"170\":1}}],[\"然后解码器再基于该表达生成输出序列\",{\"1\":{\"160\":1}}],[\"然后求和\",{\"1\":{\"140\":1}}],[\"然后小碎步前进\",{\"1\":{\"103\":1}}],[\"然后就疯狂利用这个盲点\",{\"1\":{\"102\":1}}],[\"然后要求模型预测图片中其他被遮挡部分\",{\"1\":{\"96\":1}}],[\"然后对这些锚框进行分类和回归\",{\"1\":{\"91\":1}}],[\"然后对剩下的抽样结果求平均来得到权重的估计值\",{\"1\":{\"15\":1}}],[\"然后反复从中抽取数据来学习\",{\"1\":{\"87\":1}}],[\"然后用\",{\"1\":{\"73\":1}}],[\"然后计算\",{\"1\":{\"54\":1}}],[\"然后计算接受率α\",{\"1\":{\"15\":1}}],[\"然后将这些二维的方块展平\",{\"1\":{\"29\":1}}],[\"然后从\",{\"1\":{\"24\":1}}],[\"然后通过一个确定性的\",{\"1\":{\"22\":1}}],[\"然后通过优化kl散度kl\",{\"1\":{\"13\":1}}],[\"然后我们计算损失函数\",{\"1\":{\"18\":1}}],[\"然后可以通过对剩下的抽样结果求平均来得到权重的估计值\",{\"1\":{\"15\":1}}],[\"然后重复步骤3\",{\"1\":{\"15\":2}}],[\"然后根据接受率来决定是否接受这个新的权重值\",{\"1\":{\"15\":1}}],[\"然后以接受率α\",{\"1\":{\"13\":1}}],[\"然后\",{\"1\":{\"12\":1,\"13\":1,\"51\":1,\"86\":1,\"106\":1,\"139\":1}}],[\"然后使用一个优化算法\",{\"1\":{\"12\":1}}],[\"然而\",{\"1\":{\"12\":1,\"14\":2,\"29\":1,\"96\":1}}],[\"是平衡模型效率与效果的利器\",{\"1\":{\"192\":1}}],[\"是位置编码领域的重要进展\",{\"1\":{\"184\":1}}],[\"是特征维度的总数\",{\"1\":{\"174\":1}}],[\"是否中心化\",{\"1\":{\"172\":1}}],[\"是否是距离度量\",{\"1\":{\"53\":1}}],[\"是可学习的参数\",{\"1\":{\"174\":1}}],[\"是可学习的参数矩阵\",{\"1\":{\"132\":1}}],[\"是可学习的缩放参数\",{\"1\":{\"167\":1}}],[\"是可学习的缩放和偏移参数\",{\"1\":{\"167\":1}}],[\"是深度学习模型中一种重要的归一化技术\",{\"1\":{\"164\":1}}],[\"是全连接前馈网络\",{\"1\":{\"162\":1}}],[\"是克罗内克函数\",{\"1\":{\"157\":1}}],[\"是第\",{\"1\":{\"143\":1}}],[\"是按行操作的\",{\"1\":{\"139\":1}}],[\"是相互独立的\",{\"1\":{\"133\":1}}],[\"是注意力机制的一种特殊形式\",{\"1\":{\"129\":1}}],[\"是固定的\",{\"1\":{\"112\":1}}],[\"是固定的常数\",{\"1\":{\"46\":1}}],[\"是序列长度\",{\"1\":{\"112\":1}}],[\"是时间步索引\",{\"1\":{\"111\":1}}],[\"是离散的序列\",{\"1\":{\"111\":1}}],[\"是前馈矩阵\",{\"1\":{\"110\":1}}],[\"是输出矩阵\",{\"1\":{\"110\":1}}],[\"是输入矩阵\",{\"1\":{\"110\":1}}],[\"是状态矩阵\",{\"1\":{\"110\":1}}],[\"是标量输出\",{\"1\":{\"110\":1}}],[\"是标量输入\",{\"1\":{\"110\":1}}],[\"是旧策略\",{\"1\":{\"106\":1}}],[\"是新策略\",{\"1\":{\"106\":1}}],[\"是新的权重值和当前权重值在建议分布下的概率\",{\"1\":{\"15\":1}}],[\"是新的权重值和当前权重值在目标分布下的概率\",{\"1\":{\"15\":1}}],[\"是目前解决复杂rl问题最主流和最强大的框架\",{\"1\":{\"84\":1}}],[\"是什么\",{\"0\":{\"72\":1,\"165\":1},\"1\":{\"67\":1,\"68\":1}}],[\"是勾股定理在多维空间的扩展\",{\"1\":{\"58\":1}}],[\"是这把尺子上两种最常见的刻度\",{\"1\":{\"55\":1}}],[\"是由\",{\"1\":{\"51\":1}}],[\"是之前介绍的kl散度\",{\"1\":{\"51\":1}}],[\"是\",{\"1\":{\"51\":1,\"53\":1,\"58\":1,\"59\":1,\"110\":1,\"131\":1,\"139\":1,\"155\":1,\"161\":1,\"162\":2,\"167\":2,\"172\":1}}],[\"是经过深思熟虑的\",{\"1\":{\"47\":1}}],[\"是常数\",{\"1\":{\"46\":1}}],[\"是难以计算\",{\"1\":{\"45\":1}}],[\"是的\",{\"1\":{\"44\":1,\"101\":1}}],[\"是成功训练变分自编码器\",{\"1\":{\"26\":1}}],[\"是变分参数\",{\"1\":{\"22\":1}}],[\"是变分分布q\",{\"1\":{\"12\":1}}],[\"是观测数据\",{\"1\":{\"22\":1}}],[\"是隐变量\",{\"1\":{\"22\":1}}],[\"是在每一层的激活函数之前\",{\"1\":{\"174\":1}}],[\"是在给定输入x\",{\"1\":{\"19\":1}}],[\"是在给定权重w的情况下数据d的似然性\",{\"1\":{\"19\":1}}],[\"是在变分分布q\",{\"1\":{\"12\":1}}],[\"是权重w的先验分布\",{\"1\":{\"19\":1}}],[\"是权重w的后验分布\",{\"1\":{\"19\":1}}],[\"是权重的先验分布\",{\"1\":{\"15\":1,\"18\":1}}],[\"是对参数的函数\",{\"1\":{\"18\":1}}],[\"是对观测值的函数\",{\"1\":{\"18\":1}}],[\"是一种模型压缩技术\",{\"1\":{\"185\":1}}],[\"是一种创新且高效的位置编码方法\",{\"1\":{\"184\":1}}],[\"是一种新型的状态空间模型\",{\"1\":{\"109\":1}}],[\"是一种基于transformer架构的端到端目标检测算法\",{\"1\":{\"90\":1}}],[\"是一种统计方法\",{\"1\":{\"15\":1}}],[\"是一个块对角矩阵\",{\"1\":{\"178\":1}}],[\"是一个很小的数\",{\"1\":{\"174\":1}}],[\"是一个简单的全连接前馈网络\",{\"1\":{\"161\":1}}],[\"是一个对称矩阵\",{\"1\":{\"157\":1}}],[\"是一个向量序列\",{\"1\":{\"141\":1}}],[\"是一个非常著名且棘手的问题\",{\"1\":{\"102\":1}}],[\"是一个数学工具\",{\"1\":{\"55\":1}}],[\"是一个有界值\",{\"1\":{\"54\":1}}],[\"是一个双峰分布\",{\"1\":{\"47\":1}}],[\"是一个与\",{\"1\":{\"46\":1}}],[\"是一个黑箱操作\",{\"1\":{\"24\":1}}],[\"是一个均值为\",{\"1\":{\"22\":1}}],[\"是一个关于θ的函数\",{\"1\":{\"18\":1}}],[\"是一项核心技术\",{\"1\":{\"21\":1}}],[\"是kl散度\",{\"1\":{\"18\":1}}],[\"是数据的证据\",{\"1\":{\"19\":1}}],[\"是数据的似然函数\",{\"1\":{\"18\":1}}],[\"是数据的边缘概率\",{\"1\":{\"15\":1}}],[\"是似然函数\",{\"1\":{\"15\":1}}],[\"下图给出了贪婪搜索策略下的运行流程\",{\"1\":{\"163\":1}}],[\"下图给出了编码器和解码器的内部结构\",{\"1\":{\"162\":1}}],[\"下图描绘了从输入词向量到最终输出的全过程\",{\"1\":{\"137\":1}}],[\"下面我将详细解释\",{\"1\":{\"142\":1}}],[\"下面我们来一步步详细解释\",{\"1\":{\"64\":1}}],[\"下面我们来一步步拆解这其中的逻辑和原因\",{\"1\":{\"44\":1}}],[\"下面我们以图像分类任务为例\",{\"1\":{\"30\":1}}],[\"下选择动作\",{\"1\":{\"106\":1}}],[\"下\",{\"1\":{\"12\":1}}],[\"=n1​∑i=1n​xi2​​\",{\"1\":{\"167\":1}}],[\"=rms\",{\"1\":{\"167\":1}}],[\"=rn×dmodel​\",{\"1\":{\"121\":1}}],[\"=σx−μ​⋅γ+β\",{\"1\":{\"167\":1}}],[\"=x⋅wv\",{\"1\":{\"144\":1}}],[\"=x⋅wk\",{\"1\":{\"144\":1}}],[\"=x⋅wiv​\",{\"1\":{\"119\":1}}],[\"=x⋅wik​\",{\"1\":{\"119\":1}}],[\"=同一个输入序列x的线性变换\",{\"1\":{\"132\":1}}],[\"=o\",{\"1\":{\"125\":1}}],[\"=concat\",{\"1\":{\"121\":1,\"123\":1}}],[\"=cx\",{\"1\":{\"110\":1}}],[\"=softmax\",{\"1\":{\"120\":1,\"157\":1}}],[\"=a⋅v​​\",{\"1\":{\"150\":1}}],[\"=ax\",{\"1\":{\"110\":1}}],[\"=argq∈qmin​dkl​\",{\"1\":{\"46\":1}}],[\"=21​\",{\"1\":{\"54\":1}}],[\"=21​dkl​\",{\"1\":{\"51\":1}}],[\"=0∗log\",{\"1\":{\"54\":1}}],[\"=0\",{\"1\":{\"52\":1,\"53\":1,\"54\":2}}],[\"=0当且仅当p=q\",{\"1\":{\"52\":1}}],[\"=djs​\",{\"1\":{\"52\":1,\"53\":1}}],[\"=−elbo\",{\"1\":{\"46\":1}}],[\"=−\",{\"1\":{\"46\":1}}],[\"=elbo\",{\"1\":{\"46\":1}}],[\"=ez∼q​\",{\"1\":{\"46\":4}}],[\"=eq\",{\"1\":{\"13\":1,\"18\":1}}],[\"=∫p\",{\"1\":{\"45\":1}}],[\"=i=1∏n​σ2π​1​e−2σ2\",{\"1\":{\"18\":1}}],[\"=p\",{\"1\":{\"15\":1,\"45\":1,\"46\":1}}],[\"=min\",{\"1\":{\"13\":1}}],[\"=\",{\"1\":{\"12\":1,\"19\":2,\"30\":3,\"33\":2,\"34\":1,\"54\":1,\"57\":1,\"58\":5,\"59\":4,\"61\":9,\"67\":1,\"68\":3,\"72\":1,\"106\":1,\"107\":1,\"124\":5,\"132\":9,\"138\":4,\"139\":2,\"151\":2,\"152\":1,\"157\":2,\"174\":4,\"178\":8,\"187\":3}}],[\"θ|x\",{\"1\":{\"18\":1}}],[\"θ∣ϕ\",{\"1\":{\"13\":4}}],[\"θ∣θ\",{\"1\":{\"13\":1}}],[\"θ∣x\",{\"1\":{\"13\":4}}],[\"θ\",{\"1\":{\"12\":1,\"13\":7,\"106\":5}}],[\"的知识\",{\"1\":{\"185\":1}}],[\"的维度为\",{\"1\":{\"178\":1}}],[\"的提出有效的解决bn的这两个问题\",{\"1\":{\"174\":1}}],[\"的步骤\",{\"1\":{\"171\":1}}],[\"的标准差\",{\"1\":{\"167\":1}}],[\"的均值\",{\"1\":{\"167\":1}}],[\"的生成\",{\"1\":{\"159\":1}}],[\"的调整\",{\"1\":{\"159\":1}}],[\"的转置相乘\",{\"1\":{\"156\":1}}],[\"的转置乘以上游传回来的梯度\",{\"1\":{\"155\":1}}],[\"的权重参数\",{\"1\":{\"154\":1,\"159\":1}}],[\"的依赖强度\",{\"1\":{\"153\":1}}],[\"的作用\",{\"1\":{\"151\":1}}],[\"的相似度\",{\"1\":{\"145\":1}}],[\"的嵌入向量\",{\"1\":{\"143\":1}}],[\"的嵌入向量维度为\",{\"1\":{\"143\":1}}],[\"的数学机制\",{\"1\":{\"142\":1}}],[\"的魔力所在\",{\"1\":{\"140\":1}}],[\"的分数行\",{\"1\":{\"139\":1}}],[\"的分数\",{\"1\":{\"138\":1}}],[\"的身份\",{\"1\":{\"137\":2}}],[\"的词表示\",{\"1\":{\"134\":1}}],[\"的信息\",{\"1\":{\"131\":2,\"140\":1}}],[\"的新表示\",{\"1\":{\"131\":1}}],[\"的关联度加权后的\",{\"1\":{\"140\":1}}],[\"的关联度\",{\"1\":{\"131\":1,\"138\":2}}],[\"的关键突破是引入了选择性\",{\"1\":{\"113\":1}}],[\"的意义在于\",{\"1\":{\"115\":1}}],[\"的工作流\",{\"1\":{\"114\":1}}],[\"的工作流程\",{\"1\":{\"30\":1}}],[\"的并行内存层次\",{\"1\":{\"114\":1}}],[\"的高效长序列处理能力\",{\"1\":{\"115\":1}}],[\"的高效并行扫描算法解决了这个问题\",{\"1\":{\"114\":1}}],[\"的高斯分布\",{\"1\":{\"22\":1}}],[\"的痛点\",{\"1\":{\"113\":1}}],[\"的函数\",{\"1\":{\"113\":1}}],[\"的参数\",{\"1\":{\"113\":1}}],[\"的卷积\",{\"1\":{\"112\":1}}],[\"的无限冲激响应\",{\"1\":{\"112\":1}}],[\"的输出向量为\",{\"1\":{\"149\":1}}],[\"的输出\",{\"1\":{\"112\":1,\"140\":3,\"141\":1}}],[\"的输入\",{\"1\":{\"30\":1}}],[\"的输入要求\",{\"1\":{\"29\":1}}],[\"的递归方程\",{\"1\":{\"111\":1}}],[\"的计算公式为\",{\"1\":{\"174\":2}}],[\"的计算时间\",{\"1\":{\"171\":1}}],[\"的计算过程\",{\"1\":{\"136\":1}}],[\"的计算会被简化\",{\"1\":{\"111\":1}}],[\"的计算方式完全一样\",{\"1\":{\"52\":1}}],[\"的黑盒子\",{\"1\":{\"110\":1}}],[\"的统治地位\",{\"1\":{\"109\":1}}],[\"的真正路径\",{\"1\":{\"108\":1}}],[\"的真实表示\",{\"1\":{\"97\":1}}],[\"的使用\",{\"0\":{\"107\":1}}],[\"的动作\",{\"1\":{\"106\":1}}],[\"的先进技术来对抗这一共同的挑战\",{\"1\":{\"104\":1}}],[\"的风险\",{\"1\":{\"104\":1,\"106\":1}}],[\"的有效指导\",{\"1\":{\"102\":1}}],[\"的样本\",{\"1\":{\"102\":1}}],[\"的双网络博弈结构\",{\"1\":{\"101\":1}}],[\"的表示\",{\"1\":{\"96\":1}}],[\"的集合的映射问题\",{\"1\":{\"91\":1}}],[\"的后果\",{\"1\":{\"86\":1}}],[\"的算法世界看似纷繁复杂\",{\"1\":{\"80\":1}}],[\"的角色\",{\"1\":{\"76\":1}}],[\"的第一行显示电影a和b有很高的正值\",{\"1\":{\"75\":1}}],[\"的第一列显示用户1和用户2在这个模式上有很高的正值\",{\"1\":{\"75\":1}}],[\"的最佳低秩近似矩阵\",{\"1\":{\"73\":1}}],[\"的每一行代表了一种\",{\"1\":{\"72\":1}}],[\"的每一列代表了一种\",{\"1\":{\"72\":1}}],[\"的对角矩阵\",{\"1\":{\"72\":1}}],[\"的正交矩阵\",{\"1\":{\"72\":2}}],[\"的训练配置中\",{\"1\":{\"67\":1}}],[\"的l1范数\",{\"1\":{\"65\":1}}],[\"的l2范数\",{\"1\":{\"65\":1}}],[\"的形式出现\",{\"1\":{\"64\":1}}],[\"的基本思想\",{\"1\":{\"64\":1}}],[\"的区别\",{\"1\":{\"63\":1}}],[\"的区别是看它们的单位圆\",{\"1\":{\"60\":1}}],[\"的贡献是线性叠加的\",{\"1\":{\"61\":1}}],[\"的点积\",{\"1\":{\"145\":1}}],[\"的点积来衡量相关性\",{\"1\":{\"138\":1}}],[\"的点\",{\"1\":{\"60\":2}}],[\"的点构成的图形\",{\"1\":{\"60\":1}}],[\"的尺子\",{\"1\":{\"55\":1}}],[\"的原始表示\",{\"1\":{\"140\":1}}],[\"的原始损失函数\",{\"1\":{\"53\":1}}],[\"的原生设计是处理一维的序列数据\",{\"1\":{\"29\":1}}],[\"的概率\",{\"1\":{\"106\":2}}],[\"的概率选择同一个动作\",{\"1\":{\"103\":1}}],[\"的概率在事件1\",{\"1\":{\"54\":1}}],[\"的概率都大于零\",{\"1\":{\"51\":1}}],[\"的概率质量\",{\"1\":{\"47\":1}}],[\"的平均差异\",{\"1\":{\"51\":1}}],[\"的简单算术平均\",{\"1\":{\"51\":1}}],[\"的简单照搬\",{\"1\":{\"31\":1}}],[\"的要求\",{\"1\":{\"47\":1}}],[\"的所有模式\",{\"1\":{\"47\":1}}],[\"的一个简化且高效的改进版本\",{\"1\":{\"164\":1}}],[\"的一个下界\",{\"1\":{\"46\":1}}],[\"的一道坚固防线\",{\"1\":{\"108\":1}}],[\"的一部分\",{\"1\":{\"44\":1}}],[\"的结构更具灵活性\",{\"1\":{\"31\":1}}],[\"的成功并非偶然\",{\"1\":{\"31\":1}}],[\"的周围\",{\"1\":{\"30\":1}}],[\"的灵魂\",{\"1\":{\"30\":1}}],[\"的自注意力机制本身是无法感知序列顺序的\",{\"1\":{\"30\":1}}],[\"的矩阵乘法结果\",{\"1\":{\"155\":1}}],[\"的矩阵\",{\"1\":{\"30\":1}}],[\"的向量\",{\"1\":{\"30\":1}}],[\"的图像块的原始像素来学习\",{\"1\":{\"96\":1}}],[\"的图像块会被展平成一个长度为\",{\"1\":{\"30\":1}}],[\"的图像\",{\"1\":{\"30\":1}}],[\"的小方块\",{\"1\":{\"30\":1}}],[\"的策略上\",{\"1\":{\"103\":1}}],[\"的策略\",{\"1\":{\"29\":1,\"104\":1,\"159\":1}}],[\"的\",{\"1\":{\"29\":2,\"30\":1,\"45\":1,\"75\":1,\"99\":1,\"102\":1,\"159\":1}}],[\"的问题\",{\"1\":{\"29\":1}}],[\"的核心机制\",{\"1\":{\"117\":1}}],[\"的核心创新\",{\"0\":{\"113\":1}}],[\"的核心问题是什么\",{\"0\":{\"45\":1}}],[\"的核心\",{\"1\":{\"44\":1,\"65\":2,\"73\":1}}],[\"的核心任务是图像分类\",{\"1\":{\"29\":1}}],[\"的核心是使用变分推理来近似权重的后验分布\",{\"1\":{\"12\":1}}],[\"的三大核心改变\",{\"0\":{\"29\":1}}],[\"的精髓\",{\"1\":{\"28\":1}}],[\"的出现\",{\"1\":{\"28\":1}}],[\"的方法\",{\"1\":{\"26\":1}}],[\"的值本身波动很大\",{\"1\":{\"25\":1}}],[\"的波动会相对较小\",{\"1\":{\"25\":1}}],[\"的梯度又进一步传回给\",{\"1\":{\"159\":1}}],[\"的梯度计算会考虑所有输出元素之间的相互作用\",{\"1\":{\"157\":1}}],[\"的梯度就是注意力权重矩阵\",{\"1\":{\"155\":1}}],[\"的梯度信息\",{\"1\":{\"25\":1}}],[\"的梯度\",{\"0\":{\"155\":1,\"156\":1,\"157\":1,\"158\":1},\"1\":{\"24\":1,\"154\":1,\"156\":1,\"158\":5}}],[\"的梯度流\",{\"1\":{\"24\":1}}],[\"的确定性函数\",{\"1\":{\"22\":1}}],[\"的过程\",{\"1\":{\"22\":1}}],[\"的量\",{\"1\":{\"12\":1}}],[\"q^t\",{\"1\":{\"178\":2}}],[\"qkvssscaled​aoutput​=x⋅wq=x⋅wk=x⋅wv=q⋅kt=dk​​s​=softmax\",{\"1\":{\"150\":1}}],[\"qkv​=x⋅wq\",{\"1\":{\"144\":1}}],[\"qk^t\",{\"1\":{\"132\":1,\"133\":1}}],[\"q2​⋅k2t​\",{\"1\":{\"138\":1}}],[\"q2​⋅k1t​\",{\"1\":{\"138\":1}}],[\"q2​=x2​⋅wq\",{\"1\":{\"137\":1}}],[\"q2​=xw2q​=\",{\"1\":{\"126\":1}}],[\"q1​⋅k2t​\",{\"1\":{\"138\":1}}],[\"q1​⋅k1t​\",{\"1\":{\"138\":1}}],[\"q1​\",{\"1\":{\"137\":1}}],[\"q1​=x1​⋅wq\",{\"1\":{\"137\":1}}],[\"q1​=xw1q​=\",{\"1\":{\"126\":1}}],[\"q1\",{\"1\":{\"124\":1}}],[\"qi​\",{\"1\":{\"119\":1,\"120\":1}}],[\"qi​ki​vi​​=x⋅wiq​\",{\"1\":{\"119\":1}}],[\"q值\",{\"1\":{\"88\":1}}],[\"q=\",{\"1\":{\"54\":1,\"152\":1}}],[\"q∥m\",{\"1\":{\"51\":1,\"54\":1}}],[\"q∥p\",{\"1\":{\"47\":3,\"48\":2,\"52\":1,\"53\":2,\"54\":1}}],[\"qmax​elbo\",{\"1\":{\"46\":1}}],[\"qmin​dkl​\",{\"1\":{\"46\":1}}],[\"q∗\",{\"1\":{\"45\":2,\"46\":1}}],[\"qϕ​\",{\"1\":{\"22\":4,\"24\":2}}],[\"query矩阵\",{\"1\":{\"144\":1}}],[\"query\",{\"1\":{\"129\":1,\"131\":1,\"137\":1,\"138\":1,\"144\":1,\"145\":1,\"151\":1,\"159\":1,\"177\":1}}],[\"query来自一个序列\",{\"1\":{\"129\":1}}],[\"queries\",{\"1\":{\"92\":1}}],[\"quering\",{\"1\":{\"4\":1}}],[\"quality\",{\"1\":{\"17\":1,\"43\":1}}],[\"quantification\",{\"1\":{\"17\":1}}],[\"q\",{\"1\":{\"12\":5,\"13\":3,\"14\":1,\"15\":1,\"18\":1,\"20\":2,\"45\":1,\"46\":15,\"47\":11,\"48\":1,\"50\":1,\"51\":7,\"52\":7,\"53\":2,\"82\":2,\"87\":1,\"88\":1,\"107\":1,\"121\":1,\"132\":6,\"135\":3,\"144\":1,\"154\":2,\"158\":4,\"159\":6,\"178\":6,\"187\":1}}],[\"因权重\",{\"1\":{\"152\":2}}],[\"因为图像在batch内的统计特性较为一致\",{\"1\":{\"174\":1}}],[\"因为序列中所有元素对的相似度计算\",{\"1\":{\"133\":1}}],[\"因为每次策略更新后\",{\"1\":{\"87\":1}}],[\"因为奖励信号可能很稀疏或有噪声\",{\"1\":{\"83\":1}}],[\"因为主流优化器没有像\",{\"1\":{\"68\":1}}],[\"因为计算了平方\",{\"1\":{\"61\":1}}],[\"因为\",{\"1\":{\"46\":1,\"155\":1,\"157\":1}}],[\"因为p\",{\"1\":{\"46\":1}}],[\"因为这会忽略了网络结构\",{\"1\":{\"20\":1}}],[\"因为这需要计算一个涉及到所有可能权重值的积分\",{\"1\":{\"12\":1}}],[\"因为它太累了\",{\"1\":{\"131\":1}}],[\"因为它无法忽略无关信息\",{\"1\":{\"113\":1}}],[\"因为它不必纠结于生成所有微小的\",{\"1\":{\"96\":1}}],[\"因为它不对称且不满足三角不等式\",{\"1\":{\"53\":1}}],[\"因为它可以在虚拟环境中无限次尝试\",{\"1\":{\"86\":1}}],[\"因为它可以利用随机优化算法\",{\"1\":{\"13\":1}}],[\"因为它的模式寻求\",{\"1\":{\"48\":1}}],[\"因为它与似然函数相结合\",{\"1\":{\"18\":1}}],[\"因为它是通过优化问题来逼近后验分布的\",{\"1\":{\"13\":1}}],[\"因此无法并行训练\",{\"1\":{\"111\":1}}],[\"因此我们可以预测用户2会给电影c一个低分\",{\"1\":{\"75\":1}}],[\"因此在训练和测试阶段的行为是一致的\",{\"1\":{\"174\":1}}],[\"因此在使用mcmc时\",{\"1\":{\"15\":1}}],[\"因此在尾部有相对较高的概率密度\",{\"1\":{\"14\":1}}],[\"因此通常需要抛弃前几次迭代的结果\",{\"1\":{\"15\":1}}],[\"因此对于长尾分布\",{\"1\":{\"14\":2}}],[\"因此\",{\"1\":{\"12\":1,\"14\":1,\"20\":1,\"24\":1,\"29\":1,\"30\":1,\"46\":1,\"104\":1,\"108\":1,\"111\":1,\"131\":1,\"174\":1,\"178\":1}}],[\"这增强了模型对序列长度变化的泛化能力\",{\"1\":{\"181\":1}}],[\"这消除了需要显式设计相对位置偏置的需求\",{\"1\":{\"180\":1}}],[\"这证明了rope天然编码了相对位置信息\",{\"1\":{\"178\":1}}],[\"这表明\",{\"1\":{\"171\":1}}],[\"这点微小的效率提升累积起来是非常可观的\",{\"1\":{\"171\":1}}],[\"这带来了稳定的梯度和更快的训练速度\",{\"1\":{\"171\":1}}],[\"这和我们熟悉的全连接层的反向传播规则一致\",{\"1\":{\"155\":1}}],[\"这也是transformer比rnn快得多的原因之一\",{\"1\":{\"141\":1}}],[\"这也使得i\",{\"1\":{\"96\":1}}],[\"这为了解模型的工作机制提供了宝贵的视角\",{\"1\":{\"133\":1}}],[\"这使得它在大规模硬件\",{\"1\":{\"133\":1}}],[\"这使得它非常擅长捕捉长距离依赖关系\",{\"1\":{\"133\":1}}],[\"这使得模型能够捕捉图像块之间的长距离依赖关系\",{\"1\":{\"30\":1}}],[\"这使得模型的训练过程更稳定\",{\"1\":{\"26\":1}}],[\"这样做的目的是消除不同特征值范围对梯度下降优化算法的影响\",{\"1\":{\"174\":1}}],[\"这样设计的目的是解码器是按顺序解码的\",{\"1\":{\"162\":1}}],[\"这样\",{\"1\":{\"131\":1}}],[\"这样一来\",{\"1\":{\"20\":1,\"24\":1,\"29\":1}}],[\"这与\",{\"1\":{\"112\":1}}],[\"这有助于引导actor进行更有效的探索\",{\"1\":{\"107\":1}}],[\"这提供了一个更稳定\",{\"1\":{\"107\":1}}],[\"这会将数据的尺度\",{\"1\":{\"169\":1}}],[\"这会将数据分布的中心移动到0\",{\"1\":{\"169\":1}}],[\"这会惩罚那些\",{\"1\":{\"104\":1}}],[\"这会让模型变得更简单\",{\"1\":{\"65\":1}}],[\"这对人类来说很简单\",{\"1\":{\"131\":1}}],[\"这对应了gan中生成器找到并利用判别器的盲点\",{\"1\":{\"103\":1}}],[\"这对于处理高维数据非常有用\",{\"1\":{\"62\":1}}],[\"这对于理解物体间的关系和减少误检有很大帮助\",{\"1\":{\"93\":1}}],[\"这对于理解需要长距离上下文信息的复杂场景至关重要\",{\"1\":{\"31\":1}}],[\"这对于理解模型的不确定性\",{\"1\":{\"12\":1}}],[\"这对于训练深度\",{\"1\":{\"30\":1}}],[\"这源于强化学习中一个经典的核心矛盾\",{\"1\":{\"103\":1}}],[\"这源于对抗博弈的纳什均衡点难以找到\",{\"1\":{\"102\":1}}],[\"这同样是一个vit\",{\"1\":{\"97\":1}}],[\"这类算法不学习每个动作的分数\",{\"1\":{\"83\":1}}],[\"这类算法不直接学习要执行哪个动作\",{\"1\":{\"82\":1}}],[\"这三大结构是理解所有rl算法的基石\",{\"1\":{\"81\":1}}],[\"这三类回答了\",{\"1\":{\"80\":1}}],[\"这说明数据中只有一个主导模式\",{\"1\":{\"75\":1}}],[\"这解决了稀疏性和同义词问题\",{\"1\":{\"73\":1}}],[\"这解决了kl散度中分母\",{\"1\":{\"51\":1}}],[\"这被称为\",{\"1\":{\"73\":1}}],[\"这极大地减少了存储量和计算量\",{\"1\":{\"73\":1}}],[\"这极大地缓解了过拟合\",{\"1\":{\"67\":1}}],[\"这迫使网络不能过度依赖任何单个神经元\",{\"1\":{\"69\":1}}],[\"这在ddpg等算法中很常见\",{\"1\":{\"104\":1}}],[\"这在某些环境中\",{\"1\":{\"83\":1}}],[\"这在某些场景下非常有用\",{\"1\":{\"68\":1}}],[\"这在实际中是不可行的\",{\"1\":{\"12\":1}}],[\"这形成了一个菱形\",{\"1\":{\"60\":1}}],[\"这形成了一个我们熟悉的\",{\"1\":{\"60\":1}}],[\"这正好是一个\",{\"1\":{\"58\":1}}],[\"这两个参数的维度与神经元的数量相同\",{\"1\":{\"174\":1}}],[\"这两个维度可以与上述三大结构任意组合\",{\"1\":{\"85\":1}}],[\"这两个非常重要且基础的概念\",{\"1\":{\"55\":1}}],[\"这两者不是简单的\",{\"1\":{\"44\":1}}],[\"这里\",{\"1\":{\"157\":1}}],[\"这里忽略一些严格定义\",{\"1\":{\"54\":1}}],[\"这里的\",{\"1\":{\"131\":1}}],[\"这里的σ2可以看作是数据不确定性的度量\",{\"1\":{\"19\":1}}],[\"这里的状态是权重的值\",{\"1\":{\"15\":1}}],[\"这里的后验分布是给定数据后\",{\"1\":{\"15\":1}}],[\"这一机器学习问题\",{\"1\":{\"176\":1}}],[\"这一步是反向传播通过\",{\"1\":{\"157\":1}}],[\"这一点证明起来较为复杂\",{\"1\":{\"52\":1}}],[\"这一过程涉及到一些复杂的数学\",{\"1\":{\"12\":1}}],[\"这完美解决了kl散度的非对称性问题\",{\"1\":{\"52\":1}}],[\"这些梯度会继续反向传播回去\",{\"1\":{\"159\":1}}],[\"这些矩阵是模型通过训练学习到的\",{\"1\":{\"137\":1}}],[\"这些模型通常用于控制系统\",{\"1\":{\"110\":1}}],[\"这些技术与l1\",{\"1\":{\"69\":1}}],[\"这些经过投射后的向量被称为\",{\"1\":{\"30\":1}}],[\"这些参数在训练过程中会被优化\",{\"1\":{\"18\":1}}],[\"这就好比\",{\"1\":{\"137\":1}}],[\"这就引出了两个问题\",{\"1\":{\"45\":1}}],[\"这就是它如此强大和高效的原因\",{\"1\":{\"159\":1}}],[\"这就是所谓的模型可解释性\",{\"1\":{\"141\":1}}],[\"这就是\",{\"1\":{\"140\":1}}],[\"这就是模式坍塌\",{\"1\":{\"106\":1}}],[\"这就是一种行为模式上的坍塌\",{\"1\":{\"103\":1}}],[\"这就是在网格上从\",{\"1\":{\"59\":1}}],[\"这就是连接两点的直线长度\",{\"1\":{\"56\":1}}],[\"这就是vi要解决的问题\",{\"1\":{\"45\":1}}],[\"这就是最大似然估计\",{\"1\":{\"18\":1}}],[\"这就像在一个非常嘈杂的环境中试图找到正确的方向\",{\"1\":{\"25\":1}}],[\"这就像试图通过询问一个随机数生成器的输出来调整它的内部参数一样\",{\"1\":{\"24\":1}}],[\"这导致梯度无法直接通过采样步骤进行反向传播\",{\"1\":{\"22\":1}}],[\"这意味着我们可以通过标准的反向传播算法来学习其中的参数\",{\"1\":{\"154\":1}}],[\"这意味着我们需要找到一组权重\",{\"1\":{\"18\":1}}],[\"这意味着在计算位置i的词的注意力时\",{\"1\":{\"134\":1}}],[\"这意味着在训练时\",{\"1\":{\"112\":1}}],[\"这意味着模型以相同的方式处理序列中的所有信息\",{\"1\":{\"113\":1}}],[\"这意味着\",{\"1\":{\"106\":2,\"114\":1,\"157\":1}}],[\"这意味着它会同时遮挡掉图像中多个\",{\"1\":{\"98\":1}}],[\"这意味着网络的输出也将是不确定的\",{\"1\":{\"15\":1}}],[\"这反映了我们的假设\",{\"1\":{\"18\":1}}],[\"这是归一化技术起效的关键\",{\"1\":{\"171\":1}}],[\"这是通过一个固定的步长\",{\"1\":{\"111\":1}}],[\"这是ppo的灵魂\",{\"1\":{\"106\":1}}],[\"这是最核心的一步\",{\"0\":{\"157\":1}}],[\"这是最核心也最优雅的解决方案\",{\"1\":{\"104\":1}}],[\"这是最简单的一步\",{\"1\":{\"155\":1}}],[\"这是最后一步\",{\"1\":{\"140\":1}}],[\"这是最关键的一步\",{\"1\":{\"137\":1}}],[\"这是最直接\",{\"1\":{\"67\":1}}],[\"这是detr的核心\",{\"1\":{\"92\":1}}],[\"这是前两类算法的结合体\",{\"1\":{\"84\":1}}],[\"这是整个分解的核心\",{\"1\":{\"72\":1}}],[\"这是计算机视觉领域最强大的正则化手段\",{\"1\":{\"69\":1}}],[\"这是深度学习中最标志性的正则化方法之一\",{\"1\":{\"69\":1}}],[\"这是两者最重要的应用之一\",{\"1\":{\"62\":1}}],[\"这是向量所有维度绝对值的总和\",{\"1\":{\"59\":1}}],[\"这是vae等工作的重要基础\",{\"1\":{\"48\":1}}],[\"这是vi和vae能够成功的关键\",{\"1\":{\"47\":1}}],[\"这是另一个关键点\",{\"1\":{\"47\":1}}],[\"这是\",{\"1\":{\"29\":1,\"30\":1,\"74\":1}}],[\"这是一个标准操作\",{\"1\":{\"157\":1}}],[\"这是一个标准的vit\",{\"1\":{\"97\":1}}],[\"这是一个核心问题\",{\"1\":{\"129\":1}}],[\"这是一个规模较小的vit\",{\"1\":{\"97\":1}}],[\"这是一个非常深入且重要的问题\",{\"1\":{\"164\":1}}],[\"这是一个非常核心的问题\",{\"1\":{\"80\":1}}],[\"这是一个非常棒的问题\",{\"1\":{\"64\":1}}],[\"这是一个非常自然的延伸问题\",{\"1\":{\"49\":1}}],[\"这是一个\",{\"1\":{\"29\":1}}],[\"这是因为\",{\"1\":{\"25\":1}}],[\"这是因为在长尾分布中\",{\"1\":{\"14\":1}}],[\"这是由于数据本身的噪声或测量误差导致的\",{\"1\":{\"19\":1}}],[\"这是由于模型参数的不确定性导致的\",{\"1\":{\"19\":1}}],[\"这是我们最熟悉的\",{\"1\":{\"58\":1}}],[\"这是我们所认为的\",{\"1\":{\"56\":1}}],[\"这是我们在变分推断中引入的一个新的分布\",{\"1\":{\"18\":1}}],[\"这是我们对权重在看到任何数据之前的知识或假设\",{\"1\":{\"18\":1}}],[\"这可以理解为\",{\"1\":{\"158\":1}}],[\"这可以通过重参数化技巧\",{\"1\":{\"47\":1}}],[\"这可以帮助我们更好地理解网络的训练过程\",{\"1\":{\"20\":1}}],[\"这可以用来衡量模型的不确定性\",{\"1\":{\"15\":1}}],[\"这可能会导致一些逼近误差\",{\"1\":{\"13\":1}}],[\"这种谨慎的策略让他不容易被困住\",{\"1\":{\"108\":1}}],[\"这种温和的更新方式\",{\"1\":{\"106\":1}}],[\"这种更优但早期探索时有风险的策略\",{\"1\":{\"103\":1}}],[\"这种\",{\"1\":{\"101\":1}}],[\"这种策略迫使模型不仅仅依赖于局部信息\",{\"1\":{\"98\":1}}],[\"这种设计有助于稳定训练过程\",{\"1\":{\"97\":1}}],[\"这种设计避免了在每个像素上计算注意力\",{\"1\":{\"31\":1}}],[\"这种在抽象空间中进行预测的方式\",{\"1\":{\"96\":1}}],[\"这种不确定性是固有的\",{\"1\":{\"19\":1}}],[\"这种不确定性可以通过收集更多的数据来减小\",{\"1\":{\"19\":1}}],[\"这种方法确保了注意力得分仅依赖于相对位置\",{\"1\":{\"177\":1}}],[\"这种方法可能无法很好地捕捉到尾部的信息\",{\"1\":{\"14\":1}}],[\"这种方法基于二次泰勒展开\",{\"1\":{\"14\":1}}],[\"这种分布的特点是尾部的概率密度下降的速度相对较慢\",{\"1\":{\"14\":1}}],[\"这通常会产生一个更紧的\",{\"1\":{\"47\":1}}],[\"这通常是通过贝叶斯定理来计算的\",{\"1\":{\"15\":1}}],[\"这通常是非常困难的\",{\"1\":{\"12\":1}}],[\"这通常可以通过随机初始化或使用预训练模型来进行\",{\"1\":{\"15\":1}}],[\"这通常通过梯度上升或牛顿法等优化方法来实现\",{\"1\":{\"14\":1}}],[\"这个上下文向量\",{\"1\":{\"159\":1}}],[\"这个梯度接着传回到注意力权重矩阵\",{\"1\":{\"159\":1}}],[\"这个梯度意味着\",{\"1\":{\"159\":1}}],[\"这个梯度告诉我们\",{\"1\":{\"156\":1}}],[\"这个计算确保了梯度能正确地分配给所有分数\",{\"1\":{\"157\":1}}],[\"这个分布代表了当编码\",{\"1\":{\"139\":1}}],[\"这个分布是由参数θ描述的\",{\"1\":{\"12\":1}}],[\"这个形式看起来就像一个线性循环神经网络\",{\"1\":{\"111\":1}}],[\"这个动作带来了比预期要好得多的奖励\",{\"1\":{\"106\":1}}],[\"这个值的计算方法是\",{\"1\":{\"106\":1}}],[\"这个策略能够避免摔倒并获得正向奖励\",{\"1\":{\"103\":1}}],[\"这个词的表示\",{\"1\":{\"131\":1}}],[\"这个词时\",{\"1\":{\"131\":1,\"139\":1}}],[\"这个词\",{\"1\":{\"103\":1}}],[\"这个指南明确指出在状态s下\",{\"1\":{\"83\":1}}],[\"这个函数直接告诉智能体在某个状态下应该采取什么动作\",{\"1\":{\"83\":1}}],[\"这个表告诉你\",{\"1\":{\"82\":1}}],[\"这个问题直击了ppo算法设计的精髓\",{\"1\":{\"105\":1}}],[\"这个问题\",{\"1\":{\"80\":1}}],[\"这个模式就是\",{\"1\":{\"75\":1}}],[\"这个空间通常被称为\",{\"1\":{\"73\":1}}],[\"这个稀疏模型可以大幅减少内存占用和计算量\",{\"1\":{\"68\":1}}],[\"这个概念\",{\"1\":{\"67\":1}}],[\"这个例子清晰地展示了js散度在处理零概率问题和不对称性上的巨大优势\",{\"1\":{\"54\":1}}],[\"这个等式是理解vi的关键\",{\"1\":{\"46\":1}}],[\"这个积分在高维空间\",{\"1\":{\"45\":1}}],[\"这个向量已经通过自注意力机制充分聚合了整张图像的全局信息\",{\"1\":{\"30\":1}}],[\"这个整体\",{\"1\":{\"30\":1}}],[\"这个位置编码通常也是一个可学习的\",{\"1\":{\"30\":1}}],[\"这个\",{\"1\":{\"29\":1,\"106\":1,\"159\":1}}],[\"这个令牌经过\",{\"1\":{\"29\":1}}],[\"这个优化过程通常涉及到对一个期望的梯度计算\",{\"1\":{\"22\":1}}],[\"这个后验分布可以通过贝叶斯定理计算\",{\"1\":{\"19\":1}}],[\"这个后验分布描述了在观测到数据之后\",{\"1\":{\"18\":1}}],[\"这个公式表示了在给定均值μ和标准差σ的情况下\",{\"1\":{\"18\":1}}],[\"这个变分分布通常是一个高斯分布q\",{\"1\":{\"18\":1}}],[\"这个过程如何让模型学习\",{\"1\":{\"159\":1}}],[\"这个过程就是\",{\"1\":{\"141\":1}}],[\"这个过程就像是序列中的每个词都在和其他所有词\",{\"1\":{\"131\":1}}],[\"这个过程由两个方程描述\",{\"1\":{\"110\":1}}],[\"这个过程叫做burn\",{\"1\":{\"15\":1}}],[\"这个过程会反复进行\",{\"1\":{\"12\":1}}],[\"953\",{\"1\":{\"152\":1}}],[\"953⋅2​\",{\"1\":{\"152\":1}}],[\"953⋅1​0\",{\"1\":{\"152\":1}}],[\"953​2\",{\"1\":{\"152\":1}}],[\"953​\",{\"1\":{\"152\":1}}],[\"9​−0\",{\"1\":{\"126\":1}}],[\"994\",{\"1\":{\"152\":1}}],[\"994⋅20\",{\"1\":{\"152\":1}}],[\"994⋅10\",{\"1\":{\"152\":1}}],[\"9940\",{\"1\":{\"152\":2}}],[\"99\",{\"1\":{\"106\":1}}],[\"9+0\",{\"1\":{\"54\":1}}],[\"9=0\",{\"1\":{\"54\":1}}],[\"9∗1+0+0=0\",{\"1\":{\"54\":1}}],[\"9∗log\",{\"1\":{\"54\":4}}],[\"90\",{\"1\":{\"54\":2}}],[\"900+\",{\"1\":{\"4\":1}}],[\"9\",{\"1\":{\"54\":10,\"58\":1,\"61\":3}}],[\"9c20\",{\"1\":{\"9\":1}}],[\"n×dv​\",{\"1\":{\"144\":1}}],[\"n×dk​\",{\"1\":{\"144\":2}}],[\"n×n\",{\"1\":{\"72\":1}}],[\"n2dmodel​\",{\"1\":{\"125\":1,\"127\":2}}],[\"n2dk​\",{\"1\":{\"125\":1,\"153\":1}}],[\"n2hdmodel​​\",{\"1\":{\"125\":1}}],[\"nms\",{\"1\":{\"90\":1}}],[\"natural\",{\"1\":{\"43\":1}}],[\"naive\",{\"0\":{\"11\":1}}],[\"number\",{\"1\":{\"36\":1,\"39\":1,\"43\":1}}],[\"n+1\",{\"1\":{\"30\":1}}],[\"n\",{\"1\":{\"22\":2,\"30\":4,\"57\":1,\"110\":1,\"118\":1,\"124\":10,\"143\":1,\"178\":9}}],[\"norm\",{\"0\":{\"55\":1,\"58\":1,\"59\":1},\"1\":{\"55\":2,\"56\":2,\"58\":1,\"59\":1,\"61\":2,\"62\":2,\"63\":4,\"68\":1}}],[\"normalize\",{\"1\":{\"34\":1}}],[\"normalization能够在单个样本级别上进行归一化\",{\"1\":{\"174\":1}}],[\"normalization是一种常见的深度学习模型中的归一化方法\",{\"1\":{\"174\":1}}],[\"normalization的主要作用是在训练深度神经网络时\",{\"1\":{\"174\":1}}],[\"normalization的反向传播比较复杂\",{\"1\":{\"174\":1}}],[\"normalization的最终输出\",{\"1\":{\"174\":1}}],[\"normalization的过程可以分为以下几个步骤\",{\"1\":{\"174\":1}}],[\"normalization引入了两个可学习的参数\",{\"1\":{\"174\":1}}],[\"normalization都是深度学习中的一种技术\",{\"1\":{\"174\":1}}],[\"normalization和layer\",{\"1\":{\"174\":1}}],[\"normalization\",{\"0\":{\"174\":2},\"1\":{\"30\":1,\"69\":1,\"164\":2,\"174\":5,\"175\":2}}],[\"now\",{\"1\":{\"41\":1,\"42\":1}}],[\"noisy\",{\"1\":{\"40\":2,\"41\":2,\"42\":3,\"43\":2}}],[\"noise\",{\"1\":{\"39\":8,\"40\":5,\"41\":7,\"42\":7,\"43\":6}}],[\"non\",{\"1\":{\"17\":1,\"52\":1}}],[\"no\",{\"1\":{\"17\":1}}],[\"notoriously\",{\"1\":{\"17\":1}}],[\"not\",{\"1\":{\"17\":2,\"20\":1,\"36\":1,\"40\":1,\"43\":2,\"193\":1}}],[\"n是burn\",{\"1\":{\"15\":1}}],[\"nlp\",{\"1\":{\"8\":1,\"28\":1,\"29\":1,\"74\":1}}],[\"neox等新锐模型\",{\"1\":{\"172\":1}}],[\"neox等众多现代大语言模型中\",{\"1\":{\"164\":1}}],[\"negativity\",{\"1\":{\"52\":1}}],[\"negative\",{\"1\":{\"34\":1,\"37\":3}}],[\"negatively\",{\"1\":{\"17\":1}}],[\"new\",{\"0\":{\"41\":1},\"1\":{\"38\":2,\"41\":3,\"42\":1}}],[\"newcastle\",{\"1\":{\"6\":1}}],[\"needs\",{\"1\":{\"43\":1}}],[\"needing\",{\"1\":{\"37\":1}}],[\"need\",{\"1\":{\"37\":1,\"40\":1}}],[\"neural\",{\"0\":{\"19\":1},\"1\":{\"17\":1,\"19\":1,\"36\":2,\"37\":1,\"40\":1,\"42\":1}}],[\"necessary\",{\"1\":{\"4\":1}}],[\"netflix\",{\"1\":{\"73\":1}}],[\"nets\",{\"1\":{\"43\":2}}],[\"network\",{\"1\":{\"30\":1,\"36\":3,\"37\":4,\"40\":4,\"42\":1,\"43\":7,\"82\":1,\"92\":1,\"175\":1}}],[\"networks\",{\"0\":{\"19\":1},\"1\":{\"17\":1,\"19\":1,\"43\":1}}],[\"net\",{\"0\":{\"42\":1,\"43\":1},\"1\":{\"4\":1,\"9\":1,\"40\":2,\"41\":5,\"42\":6,\"43\":11}}],[\"j\",{\"1\":{\"151\":3,\"153\":1,\"175\":4,\"187\":2}}],[\"j个\",{\"1\":{\"145\":1}}],[\"jepa是自监督学习领域的一项重要进展\",{\"1\":{\"100\":1}}],[\"jepa是一种训练方法或算法\",{\"1\":{\"99\":1}}],[\"jepa是一种自监督学习框架\",{\"1\":{\"99\":1}}],[\"jepa是一个学习框架\",{\"1\":{\"95\":1}}],[\"jepa能够在不使用或很少使用数据增强的情况下学习到高质量的语义表示\",{\"1\":{\"100\":1}}],[\"jepa预训练的模型在多种下游视觉任务中都表现出色\",{\"1\":{\"100\":1}}],[\"jepa巧妙地组织和运用了这些\",{\"1\":{\"99\":1}}],[\"jepa这座\",{\"1\":{\"99\":1}}],[\"jepa并非就是vit\",{\"1\":{\"99\":1}}],[\"jepa与vit的关系\",{\"0\":{\"99\":1}}],[\"jepa采用了一种\",{\"1\":{\"98\":1}}],[\"jepa的训练效率显著高于mae等方法\",{\"1\":{\"100\":1}}],[\"jepa的优势与意义\",{\"0\":{\"100\":1}}],[\"jepa的整体架构由三个关键部分组成\",{\"1\":{\"97\":1}}],[\"jepa的架构\",{\"0\":{\"97\":1}}],[\"jepa的核心思想\",{\"0\":{\"96\":1}}],[\"jepa在计算上比像素重建方法\",{\"1\":{\"96\":1}}],[\"jepa会向模型展示一张图片的一部分\",{\"1\":{\"96\":1}}],[\"jepa另辟蹊径\",{\"1\":{\"96\":1}}],[\"jepa\",{\"0\":{\"94\":1,\"95\":1,\"205\":1},\"1\":{\"95\":1}}],[\"jensen\",{\"1\":{\"49\":1,\"53\":1}}],[\"jenkins\",{\"1\":{\"8\":1}}],[\"just\",{\"1\":{\"43\":2}}],[\"joint\",{\"1\":{\"95\":1}}],[\"journey\",{\"1\":{\"39\":1}}],[\"job\",{\"1\":{\"33\":1,\"43\":1}}],[\"jft\",{\"1\":{\"31\":1}}],[\"javascript\",{\"1\":{\"8\":1}}],[\"js散度与kl散度的对比总结\",{\"0\":{\"53\":1}}],[\"js散度满足三角不等式\",{\"1\":{\"52\":1}}],[\"js散度衡量的是\",{\"1\":{\"51\":1}}],[\"js散度被定义为\",{\"1\":{\"51\":1}}],[\"js散度是kl散度的一种变体\",{\"1\":{\"50\":1}}],[\"js散度\",{\"0\":{\"50\":1},\"1\":{\"49\":1,\"53\":1}}],[\"js\",{\"0\":{\"49\":1},\"1\":{\"2\":1,\"4\":1}}],[\"😊\",{\"1\":{\"192\":1}}],[\"🔧\",{\"0\":{\"8\":1}}],[\"💼\",{\"0\":{\"3\":1},\"1\":{\"1\":1}}],[\"h=2\",{\"1\":{\"126\":1}}],[\"h⋅dv​\",{\"1\":{\"121\":1}}],[\"hbm\",{\"1\":{\"114\":1}}],[\"hmac\",{\"1\":{\"79\":1}}],[\"help\",{\"1\":{\"43\":1}}],[\"here\",{\"1\":{\"37\":1,\"39\":2,\"40\":1}}],[\"heart\",{\"1\":{\"33\":1,\"38\":1,\"43\":1}}],[\"head1\",{\"1\":{\"124\":1,\"126\":1}}],[\"head1​\",{\"1\":{\"121\":1,\"122\":1,\"123\":1,\"126\":1}}],[\"headh​\",{\"1\":{\"121\":1,\"122\":1,\"123\":1}}],[\"head2​\",{\"1\":{\"121\":1}}],[\"headi​学习独立的关系表示\",{\"1\":{\"128\":1}}],[\"headi​∈rn×dv​\",{\"1\":{\"120\":1}}],[\"headi​=attention\",{\"1\":{\"120\":1}}],[\"heads\",{\"1\":{\"117\":1}}],[\"head\",{\"0\":{\"116\":1,\"117\":1,\"142\":1},\"1\":{\"30\":2,\"124\":3,\"142\":1,\"161\":1,\"162\":2}}],[\"hessian矩阵的逆就是高斯近似的协方差矩阵\",{\"1\":{\"14\":1}}],[\"h\",{\"1\":{\"30\":1,\"119\":2,\"124\":2,\"125\":1,\"127\":1,\"174\":3}}],[\"hinton\",{\"1\":{\"175\":1}}],[\"hindering\",{\"1\":{\"17\":1}}],[\"higher\",{\"1\":{\"37\":1}}],[\"high\",{\"1\":{\"17\":2,\"33\":5,\"34\":1,\"37\":1,\"43\":6}}],[\"host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61\",{\"1\":{\"79\":1}}],[\"how\",{\"0\":{\"32\":1,\"35\":1,\"38\":1,\"79\":1,\"105\":1},\"1\":{\"34\":1,\"37\":1,\"38\":2,\"39\":1,\"40\":2,\"42\":1}}],[\"however\",{\"1\":{\"17\":1}}],[\"home\",{\"0\":{\"0\":1}}],[\"hungarian\",{\"1\":{\"91\":1}}],[\"hurdle\",{\"0\":{\"16\":1,\"17\":1},\"1\":{\"17\":2}}],[\"huggingface\",{\"1\":{\"9\":1}}],[\"https\",{\"1\":{\"9\":4,\"77\":1,\"78\":1,\"79\":1,\"93\":1,\"174\":1}}],[\"hard\",{\"1\":{\"186\":1}}],[\"hardware\",{\"1\":{\"114\":1}}],[\"hadamard积\",{\"1\":{\"157\":1}}],[\"hadoop\",{\"1\":{\"8\":1}}],[\"handle\",{\"1\":{\"43\":1}}],[\"happen\",{\"1\":{\"40\":1}}],[\"happens\",{\"1\":{\"39\":1}}],[\"having\",{\"1\":{\"36\":1}}],[\"have\",{\"1\":{\"17\":2,\"34\":2,\"41\":1,\"43\":1}}],[\"hash=a6cb9df501568760933d3f4e845acb3f9f728bfbbe52affe2a433a8ddd93ff4d\",{\"1\":{\"79\":1}}],[\"has\",{\"1\":{\"17\":3,\"37\":1}}],[\"hastings算法\",{\"1\":{\"15\":1}}],[\"hastings算法和hamiltonian\",{\"1\":{\"15\":1}}],[\"hastings算法中\",{\"1\":{\"13\":1,\"15\":1}}],[\"hastings\",{\"1\":{\"13\":1}}],[\"hamiltonian\",{\"1\":{\"13\":1}}],[\"hainan\",{\"1\":{\"7\":1}}],[\"kd\",{\"1\":{\"185\":1}}],[\"kiros\",{\"1\":{\"175\":1}}],[\"ki​\",{\"1\":{\"120\":1}}],[\"ki​∈rn×dk​\",{\"1\":{\"119\":1}}],[\"k2​=x2​⋅wk\",{\"1\":{\"137\":1}}],[\"k1​=x1​⋅wk\",{\"1\":{\"137\":1}}],[\"k1​\",{\"1\":{\"126\":1,\"137\":1}}],[\"k1\",{\"1\":{\"124\":1}}],[\"k=2\",{\"1\":{\"152\":1}}],[\"k=\",{\"1\":{\"112\":1,\"152\":1}}],[\"k∈rl\",{\"1\":{\"112\":1}}],[\"kernel\",{\"1\":{\"112\":1}}],[\"keys\",{\"1\":{\"159\":1}}],[\"key矩阵\",{\"1\":{\"144\":1}}],[\"key和value来自另一个序列\",{\"1\":{\"129\":1}}],[\"key\",{\"0\":{\"16\":1,\"17\":1,\"43\":1,\"137\":1},\"1\":{\"6\":1,\"33\":1,\"36\":1,\"39\":1,\"129\":1,\"131\":1,\"137\":2,\"138\":1,\"139\":1,\"144\":1,\"145\":1,\"151\":1,\"177\":1}}],[\"kca=eyjrzxkioii4azv0eml4nkrkwktza0x1a2xrl0lyaethwtnpznzxemhwmmj4mjnmq2kzckjjrvlkwurgrfjts0yrnvbjowixu3e4ouq3u1f0uudsz0p6zi9wdk1cvnu3btvzsmjidc9vvtrpv0xstxveowovmkz1wtd5ctuytthvl0vcztrobff1yli2zmxqwmhsmhjqvtraavrkm0lva05pu2jusvkyvfvnowuztzg2rxr6axdwtxbrpt0ilcjpdii6ijvhnty4ztezndfjymmxmdaxn2rkodvhodayy2exmjg0in0=\",{\"1\":{\"79\":1}}],[\"k\",{\"1\":{\"73\":10,\"74\":3,\"111\":1,\"112\":2,\"121\":1,\"124\":2,\"132\":8,\"135\":3,\"144\":1,\"154\":2,\"157\":1,\"158\":4,\"159\":6,\"162\":1,\"178\":8}}],[\"knowledge\",{\"1\":{\"185\":1}}],[\"know\",{\"1\":{\"39\":1,\"40\":2,\"43\":1}}],[\"known\",{\"1\":{\"39\":1,\"43\":1}}],[\"kullback\",{\"1\":{\"12\":1,\"53\":1}}],[\"kubernetes\",{\"1\":{\"2\":1,\"8\":1}}],[\"kl散度\",{\"1\":{\"53\":1}}],[\"kl散度本身是非负的\",{\"1\":{\"52\":1}}],[\"kl散度适合做变分推断\",{\"1\":{\"48\":1}}],[\"kl散度并非\",{\"1\":{\"48\":1}}],[\"kl散度就越小\",{\"1\":{\"46\":1}}],[\"kl散度如何自然地出现\",{\"0\":{\"46\":1}}],[\"kl散度自然而然地\",{\"1\":{\"44\":1}}],[\"kl散度非常适合用来做变分推断\",{\"1\":{\"44\":1}}],[\"kl\",{\"0\":{\"44\":1,\"49\":1},\"1\":{\"12\":2,\"18\":1,\"47\":2,\"187\":1}}],[\"kaggle\",{\"1\":{\"9\":1}}],[\"uk​\",{\"1\":{\"111\":1,\"112\":1,\"113\":2}}],[\"ua=120d5c5f540a060106570d\",{\"1\":{\"79\":1}}],[\"u\",{\"0\":{\"42\":1,\"43\":1},\"1\":{\"40\":2,\"41\":5,\"42\":6,\"43\":13,\"72\":4,\"73\":2,\"74\":1,\"75\":1,\"110\":2,\"112\":1}}],[\"usually\",{\"1\":{\"43\":1}}],[\"us\",{\"1\":{\"41\":1}}],[\"used\",{\"1\":{\"43\":1}}],[\"use\",{\"1\":{\"36\":1,\"37\":1,\"41\":1,\"43\":1}}],[\"using\",{\"1\":{\"4\":1,\"17\":1,\"37\":4,\"43\":2}}],[\"upsamples\",{\"1\":{\"43\":1}}],[\"update\",{\"1\":{\"37\":2}}],[\"up\",{\"1\":{\"34\":2,\"37\":3}}],[\"undo\",{\"1\":{\"40\":1}}],[\"understand\",{\"1\":{\"43\":1}}],[\"understanding\",{\"0\":{\"33\":1}}],[\"underscores\",{\"1\":{\"17\":1}}],[\"undermine\",{\"1\":{\"17\":1}}],[\"undetermined\",{\"1\":{\"17\":2}}],[\"until\",{\"1\":{\"37\":1,\"42\":1}}],[\"unstable\",{\"1\":{\"33\":1}}],[\"unlikely\",{\"1\":{\"33\":1}}],[\"unlike\",{\"1\":{\"33\":1}}],[\"unit\",{\"1\":{\"60\":1}}],[\"universally\",{\"1\":{\"17\":1}}],[\"university\",{\"1\":{\"6\":1,\"7\":1}}],[\"unintended\",{\"1\":{\"17\":1}}],[\"uninformative\",{\"1\":{\"17\":1}}],[\"unresolved\",{\"1\":{\"17\":1}}],[\"uncertainty\",{\"0\":{\"16\":1,\"17\":1,\"19\":1},\"1\":{\"17\":1,\"19\":2}}],[\"gpu\",{\"1\":{\"112\":1,\"114\":2}}],[\"gpt\",{\"0\":{\"12\":1},\"1\":{\"4\":1,\"164\":1,\"172\":1,\"174\":1}}],[\"gsm\",{\"1\":{\"78\":1}}],[\"global\",{\"0\":{\"78\":1}}],[\"glass\",{\"1\":{\"39\":1}}],[\"grained\",{\"1\":{\"43\":2}}],[\"grad\",{\"1\":{\"68\":1}}],[\"gradual\",{\"1\":{\"41\":1}}],[\"gradually\",{\"1\":{\"39\":1,\"40\":1}}],[\"graduated\",{\"1\":{\"6\":1,\"7\":1}}],[\"gradient\",{\"1\":{\"37\":2,\"43\":1}}],[\"guess\",{\"1\":{\"40\":1}}],[\"guided\",{\"1\":{\"42\":1}}],[\"guide\",{\"1\":{\"4\":1}}],[\"generally\",{\"0\":{\"89\":1}}],[\"generator\",{\"1\":{\"102\":1,\"104\":1}}],[\"generated\",{\"1\":{\"37\":3,\"43\":1}}],[\"generate\",{\"1\":{\"37\":1,\"40\":1,\"41\":3,\"42\":2}}],[\"generating\",{\"0\":{\"41\":1},\"1\":{\"38\":1,\"43\":2}}],[\"generative\",{\"1\":{\"8\":1,\"38\":1}}],[\"generation\",{\"1\":{\"6\":1}}],[\"getting\",{\"1\":{\"103\":1}}],[\"get\",{\"1\":{\"37\":2}}],[\"g\",{\"1\":{\"37\":1,\"39\":1,\"175\":1}}],[\"goal\",{\"1\":{\"37\":1,\"40\":2}}],[\"goes\",{\"1\":{\"34\":3}}],[\"good\",{\"1\":{\"33\":1,\"37\":1}}],[\"gives\",{\"1\":{\"41\":1}}],[\"give\",{\"1\":{\"40\":1}}],[\"given\",{\"1\":{\"34\":1,\"40\":1,\"42\":1}}],[\"gibbs\",{\"0\":{\"34\":1},\"1\":{\"34\":1}}],[\"github\",{\"1\":{\"1\":1,\"9\":1,\"78\":1}}],[\"gϕ​\",{\"1\":{\"22\":1,\"24\":1,\"25\":1}}],[\"gan中的模式坍塌\",{\"0\":{\"102\":1}}],[\"gan\",{\"1\":{\"53\":1,\"104\":1}}],[\"gans\",{\"1\":{\"6\":1}}],[\"gaussian\",{\"0\":{\"20\":1},\"1\":{\"17\":1,\"39\":2,\"41\":1}}],[\"r^\",{\"1\":{\"174\":1}}],[\"rms\",{\"1\":{\"166\":1,\"167\":1,\"172\":1}}],[\"rmsnorm比layernorm减少了约7\",{\"1\":{\"171\":1}}],[\"rmsnorm省去了计算均值\",{\"1\":{\"171\":1}}],[\"rmsnorm通过除以rms值\",{\"1\":{\"171\":1}}],[\"rmsnorm的有效性在于它抓住了归一化问题的关键\",{\"1\":{\"173\":1}}],[\"rmsnorm的有效性原理\",{\"0\":{\"171\":1}}],[\"rmsnorm的流行与大规模语言模型的发展紧密相关\",{\"1\":{\"173\":1}}],[\"rmsnorm的洞察\",{\"0\":{\"170\":1}}],[\"rmsnorm的核心思想非常直观\",{\"1\":{\"166\":1}}],[\"rmsnorm不再计算均值\",{\"1\":{\"167\":1}}],[\"rmsnorm\",{\"0\":{\"165\":1},\"1\":{\"164\":1,\"167\":1,\"172\":1}}],[\"rmsnorm为什么有效\",{\"0\":{\"164\":1}}],[\"rotary\",{\"0\":{\"177\":1},\"1\":{\"184\":1}}],[\"rope提供了更好的泛化性能和模型简洁性\",{\"1\":{\"184\":1}}],[\"rope的计算是并行的\",{\"1\":{\"183\":1}}],[\"rope的频率参数\",{\"1\":{\"182\":1}}],[\"rope的核心思想是使用旋转操作将位置信息直接嵌入到查询\",{\"1\":{\"177\":1}}],[\"rope基于复数旋转操作\",{\"1\":{\"181\":1}}],[\"rope直接通过旋转矩阵将相对位置信息融入注意力计算\",{\"1\":{\"180\":1}}],[\"rope相较于其他位置编码方法\",{\"1\":{\"179\":1}}],[\"rope编码函数为\",{\"1\":{\"178\":1}}],[\"rope应用一个旋转矩阵\",{\"1\":{\"177\":1}}],[\"rope是一种用于transformer模型的位置编码方法\",{\"1\":{\"176\":1}}],[\"rope\",{\"0\":{\"177\":1},\"1\":{\"176\":1,\"178\":1,\"184\":1}}],[\"root\",{\"1\":{\"164\":2,\"166\":1}}],[\"robust\",{\"1\":{\"17\":1}}],[\"right\",{\"1\":{\"157\":2}}],[\"ridge\",{\"1\":{\"62\":1}}],[\"rdmodel​→⊕i=1h​rdk​\",{\"1\":{\"128\":1}}],[\"rn×dmodel​\",{\"1\":{\"122\":1}}],[\"rnn的计算是顺序的\",{\"1\":{\"133\":1}}],[\"rnn\",{\"1\":{\"111\":1,\"112\":1}}],[\"rt\",{\"1\":{\"93\":1}}],[\"rl\",{\"1\":{\"80\":1,\"103\":1,\"104\":1}}],[\"rl的主要结构种类\",{\"0\":{\"80\":1}}],[\"rr=887edcc6ba568b3f\",{\"1\":{\"79\":1}}],[\"r\",{\"1\":{\"52\":2,\"93\":1,\"106\":6,\"175\":1,\"177\":1,\"178\":11}}],[\"rangle\",{\"1\":{\"178\":3}}],[\"range\",{\"1\":{\"43\":1}}],[\"random\",{\"1\":{\"33\":1,\"37\":1,\"39\":2,\"40\":1,\"41\":1,\"42\":2}}],[\"ratio\",{\"1\":{\"106\":1}}],[\"rather\",{\"1\":{\"33\":1}}],[\"rates\",{\"1\":{\"17\":1}}],[\"rag\",{\"1\":{\"4\":1}}],[\"read\",{\"0\":{\"202\":1}}],[\"realistic\",{\"1\":{\"38\":1,\"41\":1}}],[\"real\",{\"1\":{\"2\":1,\"37\":6,\"39\":1}}],[\"reference\",{\"0\":{\"175\":1}}],[\"refinement\",{\"1\":{\"41\":1}}],[\"reinforcement\",{\"0\":{\"203\":1}}],[\"reinforce的改进版\",{\"1\":{\"83\":1}}],[\"reinforce\",{\"1\":{\"83\":1,\"88\":1}}],[\"request\",{\"1\":{\"79\":1}}],[\"requirement\",{\"1\":{\"43\":1}}],[\"review\",{\"0\":{\"79\":1}}],[\"reversal\",{\"1\":{\"42\":1}}],[\"reverse\",{\"0\":{\"40\":1},\"1\":{\"38\":1,\"40\":1,\"42\":3,\"43\":1,\"47\":1}}],[\"reducing\",{\"1\":{\"175\":1}}],[\"reduction\",{\"1\":{\"73\":1}}],[\"reduces\",{\"1\":{\"43\":1}}],[\"region\",{\"1\":{\"83\":1}}],[\"regularization\",{\"0\":{\"64\":1},\"1\":{\"64\":1,\"104\":1}}],[\"regularizer\",{\"1\":{\"62\":1}}],[\"regression\",{\"1\":{\"62\":2}}],[\"reconstruct\",{\"1\":{\"43\":1}}],[\"recreate\",{\"0\":{\"40\":1}}],[\"return\",{\"1\":{\"43\":1}}],[\"retrieving\",{\"1\":{\"4\":1}}],[\"removing\",{\"1\":{\"41\":1}}],[\"removes\",{\"1\":{\"40\":1}}],[\"remains\",{\"1\":{\"17\":1}}],[\"relu会将负值置零\",{\"1\":{\"170\":1}}],[\"relevant\",{\"1\":{\"43\":1}}],[\"related\",{\"1\":{\"34\":1}}],[\"relationships\",{\"1\":{\"43\":1}}],[\"relationship\",{\"1\":{\"34\":1}}],[\"reliable\",{\"1\":{\"17\":1}}],[\"reliability\",{\"1\":{\"17\":1}}],[\"repeated\",{\"1\":{\"40\":1}}],[\"repeatedly\",{\"1\":{\"37\":1}}],[\"repeat\",{\"1\":{\"39\":1,\"41\":2}}],[\"reparameterization\",{\"1\":{\"21\":1,\"47\":1}}],[\"reparameter\",{\"0\":{\"21\":1}}],[\"representation\",{\"1\":{\"43\":3,\"96\":1,\"135\":1}}],[\"representing\",{\"1\":{\"36\":1}}],[\"represents\",{\"1\":{\"33\":1}}],[\"represent\",{\"1\":{\"17\":1}}],[\"resnet\",{\"1\":{\"186\":1}}],[\"respectively\",{\"1\":{\"178\":1}}],[\"resolutions\",{\"1\":{\"43\":1}}],[\"resolution\",{\"1\":{\"43\":3}}],[\"resources\",{\"0\":{\"9\":1}}],[\"resembling\",{\"1\":{\"39\":1}}],[\"research\",{\"1\":{\"17\":2,\"90\":1}}],[\"resulting\",{\"1\":{\"34\":1}}],[\"results\",{\"1\":{\"17\":1}}],[\"residual\",{\"1\":{\"30\":1,\"43\":3}}],[\"restrains\",{\"1\":{\"17\":1}}],[\"restraint\",{\"1\":{\"17\":1}}],[\"restrained\",{\"1\":{\"17\":1}}],[\"restful\",{\"1\":{\"4\":1}}],[\"dn\",{\"1\":{\"174\":1}}],[\"d2\",{\"1\":{\"174\":1}}],[\"d2865c0a9fee\",{\"1\":{\"9\":1}}],[\"d1\",{\"1\":{\"174\":1}}],[\"dk​=dv​\",{\"1\":{\"144\":1}}],[\"dk​=dv​=64\",{\"1\":{\"144\":1}}],[\"dk​=dv​=2\",{\"1\":{\"126\":1,\"152\":1}}],[\"dk​=dv​=hdmodel​​\",{\"1\":{\"119\":1}}],[\"dk​\",{\"1\":{\"125\":1,\"139\":1}}],[\"dk​​s​\",{\"1\":{\"154\":1}}],[\"dk​​\",{\"1\":{\"123\":1,\"139\":3,\"157\":3}}],[\"dk​​qi​kit​​∈rn×n\",{\"1\":{\"120\":1}}],[\"dk​​qi​kit​​\",{\"1\":{\"120\":1}}],[\"dkl​\",{\"1\":{\"46\":5,\"47\":5,\"48\":2,\"51\":1,\"52\":1,\"53\":2,\"54\":4}}],[\"dmodel​=512\",{\"1\":{\"144\":1}}],[\"dmodel​=4\",{\"1\":{\"126\":1}}],[\"dmodel​\",{\"1\":{\"118\":1,\"143\":1}}],[\"d∈r\",{\"1\":{\"110\":1}}],[\"dtd​x\",{\"1\":{\"110\":1}}],[\"ddpg\",{\"1\":{\"84\":1,\"87\":1}}],[\"dqn\",{\"1\":{\"82\":1,\"87\":1,\"88\":1}}],[\"date=20240522t182749z\",{\"1\":{\"79\":1}}],[\"dataset\",{\"1\":{\"17\":1,\"37\":1,\"39\":1}}],[\"data\",{\"1\":{\"17\":2,\"33\":5,\"34\":4,\"36\":3,\"37\":8,\"38\":3,\"69\":1}}],[\"databases\",{\"1\":{\"4\":1}}],[\"djs​​\",{\"1\":{\"52\":1}}],[\"djs​=0\",{\"1\":{\"52\":2}}],[\"djs​\",{\"1\":{\"51\":1,\"52\":6,\"53\":1,\"54\":1}}],[\"dz\",{\"1\":{\"45\":1}}],[\"dropout\",{\"1\":{\"69\":1,\"70\":1}}],[\"drops\",{\"1\":{\"39\":1}}],[\"drop\",{\"1\":{\"39\":1}}],[\"dynamics\",{\"1\":{\"37\":1}}],[\"dots\",{\"1\":{\"178\":1}}],[\"doi\",{\"1\":{\"77\":1}}],[\"done\",{\"1\":{\"39\":1}}],[\"doesn\",{\"1\":{\"39\":1}}],[\"does\",{\"0\":{\"38\":1,\"42\":1},\"1\":{\"40\":1}}],[\"do\",{\"0\":{\"42\":1},\"1\":{\"37\":1,\"40\":1}}],[\"dog\",{\"1\":{\"33\":1}}],[\"downsampling\",{\"1\":{\"43\":2}}],[\"down\",{\"1\":{\"32\":1,\"34\":3,\"37\":3,\"38\":1,\"41\":1}}],[\"domain\",{\"1\":{\"17\":1}}],[\"during\",{\"1\":{\"37\":1}}],[\"dual\",{\"1\":{\"17\":1}}],[\"due\",{\"1\":{\"17\":2}}],[\"dump\",{\"1\":{\"4\":1}}],[\"d∣w\",{\"1\":{\"15\":2,\"18\":2}}],[\"didn\",{\"1\":{\"131\":1}}],[\"dino\",{\"1\":{\"93\":1}}],[\"dingxiaoh\",{\"1\":{\"78\":1}}],[\"dimensions\",{\"1\":{\"43\":1}}],[\"dimensional\",{\"1\":{\"34\":1,\"43\":1}}],[\"dimensionality\",{\"1\":{\"17\":1,\"73\":1}}],[\"dictates\",{\"1\":{\"34\":1}}],[\"directly\",{\"1\":{\"33\":1,\"37\":1,\"40\":1,\"43\":2}}],[\"diffusers\",{\"0\":{\"200\":1}}],[\"diffusion\",{\"0\":{\"38\":1,\"42\":1,\"43\":1},\"1\":{\"38\":2,\"39\":1,\"40\":1,\"41\":1,\"42\":6,\"43\":6}}],[\"different\",{\"1\":{\"33\":1,\"43\":4}}],[\"difference\",{\"0\":{\"13\":1}}],[\"difficult\",{\"1\":{\"17\":1}}],[\"difficulty\",{\"1\":{\"17\":2}}],[\"divergence\",{\"0\":{\"44\":1,\"49\":1,\"50\":1},\"1\":{\"12\":1,\"37\":1,\"49\":1,\"52\":1,\"53\":2}}],[\"discrimination\",{\"1\":{\"104\":1}}],[\"discriminator\",{\"1\":{\"102\":1,\"104\":1}}],[\"distillation\",{\"0\":{\"185\":1},\"1\":{\"185\":1}}],[\"distinguish\",{\"1\":{\"33\":1}}],[\"distributions\",{\"0\":{\"20\":1},\"1\":{\"17\":3}}],[\"distribution\",{\"0\":{\"34\":1},\"1\":{\"14\":1,\"17\":2,\"34\":1,\"36\":1,\"37\":1,\"39\":1,\"45\":1}}],[\"distributed\",{\"1\":{\"6\":1}}],[\"dissertation\",{\"1\":{\"6\":1}}],[\"d|w\",{\"1\":{\"12\":2,\"19\":3}}],[\"d\",{\"1\":{\"12\":1,\"15\":2,\"19\":2,\"30\":6,\"124\":8,\"152\":1,\"157\":1,\"178\":5}}],[\"d0472a20\",{\"1\":{\"9\":1}}],[\"deformable\",{\"1\":{\"93\":1}}],[\"defining\",{\"1\":{\"17\":2}}],[\"detection\",{\"1\":{\"90\":1}}],[\"detr的提出为目标检测领域带来了一种全新的思路\",{\"1\":{\"93\":1}}],[\"detr的整体架构简洁明了\",{\"1\":{\"92\":1}}],[\"detr在小物体上的检测效果不佳\",{\"1\":{\"93\":1}}],[\"detr需要更长的训练时间才能收敛\",{\"1\":{\"93\":1}}],[\"detr能够更好地利用图像的全局上下文信息\",{\"1\":{\"93\":1}}],[\"detr摆脱了对锚框和nms等复杂手工组件的依赖\",{\"1\":{\"93\":1}}],[\"detr使用匈牙利算法\",{\"1\":{\"91\":1}}],[\"detr将目标检测任务直接建模为一个从图像到一组\",{\"1\":{\"91\":1}}],[\"detr则另辟蹊径\",{\"1\":{\"91\":1}}],[\"detr算法\",{\"0\":{\"90\":1}}],[\"detr\",{\"0\":{\"89\":1,\"204\":1},\"1\":{\"90\":1,\"93\":3}}],[\"detail\",{\"1\":{\"43\":1}}],[\"details\",{\"1\":{\"43\":3}}],[\"detailed\",{\"1\":{\"41\":1}}],[\"decay`\",{\"1\":{\"67\":1}}],[\"decay=1e\",{\"1\":{\"67\":1}}],[\"decay\",{\"0\":{\"67\":1},\"1\":{\"67\":1,\"68\":1}}],[\"decoder\",{\"0\":{\"29\":1},\"1\":{\"28\":1,\"29\":2,\"43\":6,\"92\":2,\"135\":1,\"160\":2}}],[\"delve\",{\"1\":{\"42\":1}}],[\"deliver\",{\"1\":{\"2\":1}}],[\"denoising\",{\"1\":{\"41\":2,\"42\":1,\"43\":1,\"73\":1,\"93\":1}}],[\"denoted\",{\"1\":{\"34\":1,\"39\":1}}],[\"design\",{\"1\":{\"43\":1}}],[\"designed\",{\"1\":{\"43\":1}}],[\"designing\",{\"0\":{\"36\":1},\"1\":{\"35\":1}}],[\"destruction\",{\"1\":{\"39\":1}}],[\"destroying\",{\"0\":{\"39\":1}}],[\"destroy\",{\"1\":{\"38\":1}}],[\"descent\",{\"1\":{\"37\":2}}],[\"dependencies\",{\"1\":{\"43\":1}}],[\"dependent\",{\"1\":{\"17\":1,\"113\":1}}],[\"deployment\",{\"1\":{\"4\":1}}],[\"demonstrated\",{\"1\":{\"17\":1}}],[\"deeplearning\",{\"1\":{\"9\":1}}],[\"deep\",{\"0\":{\"64\":1},\"1\":{\"8\":1,\"17\":3,\"36\":1,\"43\":1,\"82\":1,\"175\":1}}],[\"developer\",{\"1\":{\"93\":1}}],[\"developed\",{\"1\":{\"17\":1}}],[\"developing\",{\"1\":{\"17\":1}}],[\"development\",{\"1\":{\"4\":1}}],[\"devops\",{\"1\":{\"2\":1,\"4\":1}}],[\"t=1\",{\"1\":{\"187\":1}}],[\"t=0\",{\"1\":{\"41\":1}}],[\"t⋅q\",{\"1\":{\"158\":1}}],[\"t​\",{\"1\":{\"123\":1}}],[\"t强行\",{\"1\":{\"106\":1}}],[\"tsoh=d3d3lnnjawvuy2vkaxjly3quy29t\",{\"1\":{\"79\":1}}],[\"tinybert\",{\"1\":{\"186\":1}}],[\"tired\",{\"1\":{\"131\":3}}],[\"title\",{\"1\":{\"93\":1}}],[\"tid=spdf\",{\"1\":{\"79\":1}}],[\"time\",{\"1\":{\"41\":1,\"43\":2}}],[\"timestep\",{\"1\":{\"39\":2,\"40\":2,\"41\":5,\"42\":1,\"43\":4}}],[\"timesteps\",{\"1\":{\"39\":2,\"40\":1}}],[\"tf\",{\"1\":{\"74\":1}}],[\"temperature\",{\"1\":{\"187\":1}}],[\"teacher\",{\"1\":{\"186\":1}}],[\"tensorflow\",{\"1\":{\"66\":1}}],[\"text\",{\"1\":{\"43\":4,\"151\":1,\"157\":4}}],[\"terms\",{\"1\":{\"42\":1}}],[\"tell\",{\"1\":{\"41\":2}}],[\"television\",{\"1\":{\"39\":1}}],[\"technique\",{\"1\":{\"37\":1}}],[\"techniques\",{\"1\":{\"17\":1}}],[\"technical\",{\"0\":{\"8\":1,\"43\":1},\"1\":{\"4\":1,\"42\":1}}],[\"typically\",{\"1\":{\"36\":1,\"43\":1}}],[\"type=client\",{\"1\":{\"79\":1}}],[\"type\",{\"1\":{\"33\":1,\"39\":1}}],[\"t\",{\"1\":{\"33\":1,\"37\":2,\"39\":3,\"40\":2,\"41\":4,\"43\":1,\"106\":6,\"110\":14,\"131\":1,\"187\":5}}],[\"targets\",{\"1\":{\"186\":1}}],[\"target\",{\"1\":{\"97\":1}}],[\"taxicab\",{\"1\":{\"63\":1}}],[\"tabular\",{\"1\":{\"36\":1}}],[\"take\",{\"1\":{\"37\":1,\"41\":1}}],[\"takes\",{\"1\":{\"34\":1,\"36\":1,\"43\":1}}],[\"takeaway\",{\"1\":{\"33\":1}}],[\"task\",{\"1\":{\"17\":1,\"40\":1,\"42\":2,\"43\":2}}],[\"tail\",{\"1\":{\"14\":2}}],[\"two\",{\"1\":{\"17\":2,\"35\":1,\"37\":1,\"38\":1}}],[\"trpo\",{\"1\":{\"83\":1}}],[\"trust\",{\"1\":{\"83\":1}}],[\"truncated\",{\"1\":{\"73\":1}}],[\"true\",{\"1\":{\"17\":1}}],[\"transposed\",{\"1\":{\"43\":1}}],[\"transformers\",{\"0\":{\"209\":1}}],[\"transformer编码器\",{\"1\":{\"92\":1}}],[\"transformer\",{\"0\":{\"28\":1,\"29\":1,\"160\":1},\"1\":{\"28\":3,\"29\":7,\"30\":8,\"31\":3,\"90\":1,\"95\":1,\"97\":1,\"109\":1,\"112\":1,\"115\":3,\"117\":1,\"128\":1,\"141\":1,\"144\":1,\"160\":1,\"172\":1}}],[\"trains\",{\"1\":{\"37\":1}}],[\"train\",{\"1\":{\"34\":1,\"37\":2,\"40\":1}}],[\"training\",{\"0\":{\"37\":1},\"1\":{\"33\":2,\"35\":1,\"37\":3,\"39\":1,\"40\":1,\"43\":1,\"175\":1}}],[\"trained\",{\"1\":{\"33\":1,\"38\":1,\"40\":2,\"41\":4,\"42\":1}}],[\"tractable\",{\"1\":{\"17\":2}}],[\"triangle\",{\"1\":{\"52\":1}}],[\"tricks\",{\"0\":{\"210\":1}}],[\"tricky\",{\"1\":{\"37\":1}}],[\"trick\",{\"0\":{\"21\":1,\"208\":1},\"1\":{\"21\":1,\"47\":1}}],[\"trivial\",{\"1\":{\"17\":1}}],[\"troubleshooting\",{\"1\":{\"4\":4}}],[\"thinking\",{\"1\":{\"136\":1,\"137\":2,\"138\":4,\"139\":2,\"140\":3}}],[\"think\",{\"1\":{\"39\":1}}],[\"thinks\",{\"1\":{\"37\":1}}],[\"this\",{\"0\":{\"35\":1},\"1\":{\"17\":4,\"33\":4,\"34\":7,\"36\":1,\"37\":9,\"38\":1,\"39\":3,\"40\":5,\"41\":8,\"42\":5,\"43\":9}}],[\"through\",{\"1\":{\"36\":1,\"37\":2,\"39\":2}}],[\"that\",{\"1\":{\"17\":7,\"33\":5,\"34\":6,\"36\":2,\"37\":1,\"38\":1,\"39\":2,\"40\":3,\"41\":4,\"42\":1,\"43\":2}}],[\"theta\",{\"1\":{\"177\":2,\"178\":17,\"182\":1}}],[\"thetaij​\",{\"1\":{\"151\":1}}],[\"their\",{\"1\":{\"36\":1,\"43\":2}}],[\"theorem\",{\"0\":{\"34\":1},\"1\":{\"34\":1}}],[\"theoretical\",{\"1\":{\"17\":1}}],[\"them\",{\"1\":{\"33\":1,\"37\":2}}],[\"then\",{\"1\":{\"33\":1,\"35\":1,\"40\":1,\"41\":1,\"42\":2,\"43\":1}}],[\"these\",{\"1\":{\"17\":1,\"34\":1,\"37\":1,\"39\":1,\"41\":1,\"43\":4}}],[\"they\",{\"1\":{\"17\":1,\"20\":2,\"33\":2}}],[\"therefore\",{\"1\":{\"17\":1}}],[\"there\",{\"1\":{\"17\":1}}],[\"the\",{\"0\":{\"33\":1,\"34\":2,\"36\":1,\"37\":1,\"39\":1,\"40\":2,\"42\":1,\"43\":1,\"102\":1,\"103\":1},\"1\":{\"17\":36,\"33\":9,\"34\":16,\"35\":1,\"36\":10,\"37\":22,\"38\":6,\"39\":12,\"40\":19,\"41\":18,\"42\":18,\"43\":72,\"131\":2}}],[\"too\",{\"1\":{\"131\":1}}],[\"tools\",{\"1\":{\"8\":1}}],[\"tooling\",{\"1\":{\"4\":1}}],[\"torch\",{\"1\":{\"67\":1,\"68\":1}}],[\"total\",{\"1\":{\"34\":1,\"68\":2}}],[\"token=iqojb3jpz2lux2vjeor\",{\"1\":{\"79\":1}}],[\"token\",{\"1\":{\"29\":1,\"30\":1,\"143\":3,\"145\":2,\"149\":1,\"152\":1,\"153\":2}}],[\"to\",{\"0\":{\"32\":1,\"35\":1,\"40\":1,\"79\":1},\"1\":{\"2\":1,\"4\":1,\"17\":5,\"30\":1,\"33\":4,\"34\":5,\"36\":2,\"37\":13,\"38\":5,\"39\":3,\"40\":9,\"41\":5,\"42\":8,\"43\":19}}],[\"o=av\",{\"1\":{\"154\":1,\"155\":1,\"156\":1}}],[\"o\",{\"1\":{\"115\":2,\"125\":1,\"127\":2,\"153\":1,\"155\":1,\"159\":5}}],[\"objective\",{\"0\":{\"106\":1}}],[\"object\",{\"1\":{\"92\":1}}],[\"observing\",{\"1\":{\"17\":1}}],[\"obstacles\",{\"1\":{\"17\":1}}],[\"obstacle\",{\"1\":{\"17\":1}}],[\"our\",{\"1\":{\"41\":2}}],[\"outputi​=j=1∑n​aij​⋅vj​\",{\"1\":{\"149\":1}}],[\"output=a⋅v=\",{\"1\":{\"152\":1}}],[\"output=a⋅v∈rn×dv​\",{\"1\":{\"149\":1}}],[\"output=f\",{\"1\":{\"125\":1}}],[\"output=concat⋅wo\",{\"1\":{\"126\":1}}],[\"output=concat\",{\"1\":{\"122\":1}}],[\"outputs\",{\"1\":{\"34\":1,\"36\":1,\"42\":1}}],[\"outputting\",{\"1\":{\"33\":1}}],[\"output\",{\"1\":{\"33\":3,\"36\":2,\"43\":3,\"110\":1,\"124\":1,\"151\":1,\"154\":1}}],[\"outlook\",{\"1\":{\"1\":1}}],[\"old\",{\"1\":{\"39\":1,\"106\":2}}],[\"own\",{\"1\":{\"37\":1,\"43\":1}}],[\"overfitting\",{\"1\":{\"65\":1}}],[\"over\",{\"1\":{\"34\":1,\"39\":2}}],[\"overcoming\",{\"1\":{\"17\":1}}],[\"other\",{\"1\":{\"17\":1,\"36\":1}}],[\"onlinelibrary\",{\"1\":{\"77\":1}}],[\"once\",{\"1\":{\"41\":1}}],[\"one\",{\"1\":{\"17\":1,\"37\":1,\"41\":1}}],[\"on\",{\"0\":{\"35\":1,\"87\":1,\"105\":1},\"1\":{\"17\":2,\"33\":2,\"37\":1,\"38\":1,\"39\":1,\"41\":1,\"43\":3,\"87\":1}}],[\"original\",{\"1\":{\"39\":1,\"40\":1,\"43\":1,\"68\":1}}],[\"or\",{\"1\":{\"17\":1,\"33\":2,\"34\":1,\"37\":2,\"43\":1}}],[\"off\",{\"0\":{\"87\":1},\"1\":{\"87\":1}}],[\"often\",{\"1\":{\"17\":4,\"34\":1,\"39\":2,\"43\":3,\"45\":1}}],[\"of\",{\"0\":{\"33\":1,\"43\":1},\"1\":{\"9\":1,\"17\":25,\"32\":1,\"33\":7,\"34\":6,\"36\":2,\"37\":8,\"38\":1,\"39\":7,\"40\":1,\"41\":3,\"42\":5,\"43\":21,\"52\":1}}],[\"opaque\",{\"1\":{\"39\":1}}],[\"operating\",{\"1\":{\"43\":1}}],[\"operation\",{\"1\":{\"34\":1}}],[\"operates\",{\"1\":{\"42\":1}}],[\"open\",{\"1\":{\"4\":1}}],[\"optimum\",{\"1\":{\"103\":1}}],[\"optim\",{\"1\":{\"67\":1}}],[\"optimizer\",{\"1\":{\"67\":1,\"68\":2}}],[\"optimization\",{\"1\":{\"4\":1,\"83\":2}}],[\"optimal\",{\"1\":{\"17\":1}}],[\"m×m\",{\"1\":{\"72\":1}}],[\"m×n\",{\"1\":{\"72\":2}}],[\"mse\",{\"1\":{\"62\":1,\"63\":1}}],[\"m=p=q\",{\"1\":{\"52\":1}}],[\"m=21​\",{\"1\":{\"51\":1,\"54\":1}}],[\"m\",{\"1\":{\"50\":1,\"51\":5,\"53\":1,\"72\":3,\"73\":7,\"177\":3,\"178\":18}}],[\"mutual\",{\"1\":{\"190\":1}}],[\"must\",{\"1\":{\"43\":1}}],[\"much\",{\"1\":{\"39\":1,\"40\":1}}],[\"multihead\",{\"1\":{\"121\":1}}],[\"multi\",{\"0\":{\"116\":1,\"117\":1},\"1\":{\"30\":1,\"36\":1,\"98\":1,\"161\":1,\"162\":2}}],[\"mechanisms\",{\"1\":{\"43\":2}}],[\"mechanics\",{\"1\":{\"34\":1}}],[\"mean\",{\"1\":{\"47\":1,\"164\":2,\"166\":1}}],[\"means\",{\"1\":{\"38\":2}}],[\"meaning\",{\"1\":{\"34\":1,\"39\":1}}],[\"meaningful\",{\"1\":{\"17\":2}}],[\"merely\",{\"1\":{\"17\":1}}],[\"metric\",{\"1\":{\"52\":1}}],[\"metropolis\",{\"1\":{\"13\":1}}],[\"methodology\",{\"1\":{\"17\":1}}],[\"methods\",{\"0\":{\"10\":1},\"1\":{\"8\":1,\"17\":5}}],[\"mitigating\",{\"1\":{\"43\":1}}],[\"might\",{\"1\":{\"36\":1}}],[\"misspecified\",{\"1\":{\"17\":1}}],[\"millions\",{\"1\":{\"17\":1}}],[\"mini\",{\"1\":{\"13\":1}}],[\"microservices\",{\"1\":{\"8\":1}}],[\"microsoft\",{\"0\":{\"4\":1}}],[\"mcmc是一种处理不确定性的方法\",{\"1\":{\"15\":1}}],[\"mcmc可以用来估计网络权重的后验分布并进行更新\",{\"1\":{\"15\":1}}],[\"mcmc可以提供精确的后验样本\",{\"1\":{\"13\":1}}],[\"mcmc可以处理各种复杂的模型和分布\",{\"1\":{\"13\":1}}],[\"mcmc的收敛速度可能会很慢\",{\"1\":{\"13\":1}}],[\"mcmc通常需要大量的计算时间和资源\",{\"1\":{\"13\":1}}],[\"mcmc不需要选择一个逼近分布\",{\"1\":{\"13\":1}}],[\"mcmc方法\",{\"1\":{\"13\":1}}],[\"mcmc\",{\"0\":{\"13\":1,\"15\":1},\"1\":{\"13\":3,\"15\":1,\"17\":1,\"37\":1}}],[\"moving\",{\"1\":{\"97\":1}}],[\"momentum\",{\"0\":{\"78\":1}}],[\"most\",{\"1\":{\"40\":1,\"42\":1,\"43\":1}}],[\"modulate\",{\"1\":{\"43\":1}}],[\"module\",{\"1\":{\"20\":1}}],[\"mode\",{\"0\":{\"101\":1,\"105\":1},\"1\":{\"47\":1,\"48\":1,\"101\":1}}],[\"modern\",{\"1\":{\"36\":1,\"40\":1,\"43\":2}}],[\"model\",{\"0\":{\"35\":1,\"37\":1,\"38\":1,\"86\":2},\"1\":{\"17\":4,\"33\":6,\"34\":1,\"37\":5,\"38\":2,\"41\":3,\"42\":3,\"43\":1,\"67\":1,\"68\":2,\"86\":2,\"109\":1,\"124\":3}}],[\"models\",{\"0\":{\"33\":1,\"42\":1,\"43\":1},\"1\":{\"8\":1,\"17\":3,\"32\":1,\"34\":1,\"36\":1,\"38\":1,\"40\":1,\"41\":1,\"42\":1,\"43\":5}}],[\"more\",{\"1\":{\"17\":6,\"39\":1,\"40\":1,\"43\":1}}],[\"monte\",{\"0\":{\"15\":1},\"1\":{\"10\":1,\"13\":3,\"15\":1,\"17\":1,\"37\":1}}],[\"mamba\",{\"0\":{\"113\":1},\"1\":{\"109\":3,\"110\":1,\"113\":1,\"114\":2,\"115\":2}}],[\"mamba算法\",{\"0\":{\"109\":1}}],[\"masking\",{\"1\":{\"98\":1}}],[\"mask\",{\"1\":{\"96\":1}}],[\"masked\",{\"1\":{\"96\":1,\"134\":1,\"162\":1}}],[\"mastering\",{\"1\":{\"41\":1}}],[\"mathematics\",{\"0\":{\"201\":1}}],[\"mathematical\",{\"1\":{\"34\":2}}],[\"matrix\",{\"1\":{\"73\":1}}],[\"mae\",{\"1\":{\"62\":1,\"63\":1,\"96\":1}}],[\"marginal\",{\"1\":{\"45\":1}}],[\"markov\",{\"0\":{\"15\":1},\"1\":{\"10\":1,\"13\":2,\"17\":1,\"37\":1}}],[\"making\",{\"1\":{\"43\":1}}],[\"makes\",{\"1\":{\"43\":1}}],[\"make\",{\"1\":{\"17\":1,\"37\":2,\"40\":1,\"43\":1}}],[\"max\",{\"1\":{\"43\":1}}],[\"maximum\",{\"1\":{\"14\":1,\"18\":1,\"37\":1}}],[\"maximization\",{\"1\":{\"10\":1}}],[\"magic\",{\"1\":{\"40\":1}}],[\"magnified\",{\"1\":{\"17\":1}}],[\"many\",{\"1\":{\"39\":1,\"43\":1}}],[\"manner\",{\"1\":{\"17\":1,\"39\":1}}],[\"main\",{\"1\":{\"35\":1,\"38\":1,\"79\":1}}],[\"major\",{\"1\":{\"17\":1}}],[\"mackay在1992年的论文\",{\"1\":{\"14\":1}}],[\"machines\",{\"1\":{\"136\":1,\"137\":1,\"138\":2,\"140\":3}}],[\"machine\",{\"0\":{\"16\":1,\"17\":1,\"194\":1},\"1\":{\"6\":1,\"17\":4,\"33\":1,\"40\":1}}],[\"maps\",{\"1\":{\"43\":2}}],[\"mapping\",{\"1\":{\"17\":1}}],[\"map\",{\"1\":{\"14\":3}}],[\"mlp\",{\"1\":{\"30\":1,\"36\":1}}],[\"mle\",{\"1\":{\"18\":1}}],[\"mlapp\",{\"1\":{\"9\":1}}],[\"ml\",{\"1\":{\"2\":2,\"8\":1,\"9\":1}}],[\"v=\",{\"1\":{\"152\":1}}],[\"v2​\",{\"1\":{\"140\":1}}],[\"v2​=x2​⋅wv\",{\"1\":{\"137\":1}}],[\"v的来源变了\",{\"1\":{\"132\":1}}],[\"v1​=x1​⋅wv\",{\"1\":{\"137\":1}}],[\"v1​\",{\"1\":{\"126\":1,\"137\":1,\"140\":1}}],[\"v1\",{\"1\":{\"124\":1}}],[\"volcengine\",{\"1\":{\"93\":1}}],[\"vs\",{\"0\":{\"86\":1,\"87\":1},\"1\":{\"75\":1,\"101\":1,\"103\":1,\"114\":1,\"128\":1,\"186\":2}}],[\"vᵀ\",{\"1\":{\"72\":3,\"73\":1,\"75\":1}}],[\"vₙ²\",{\"1\":{\"58\":1}}],[\"vₙ\",{\"1\":{\"57\":1}}],[\"v₃²\",{\"1\":{\"58\":1}}],[\"v₃\",{\"1\":{\"57\":1}}],[\"v₂²\",{\"1\":{\"58\":1}}],[\"v₂\",{\"1\":{\"57\":1}}],[\"v₁²\",{\"1\":{\"58\":1}}],[\"v₁\",{\"1\":{\"57\":1}}],[\"v\",{\"1\":{\"57\":1,\"58\":1,\"59\":1,\"72\":1,\"73\":1,\"107\":1,\"121\":1,\"124\":3,\"132\":7,\"135\":3,\"144\":1,\"151\":1,\"152\":2,\"154\":2,\"155\":2,\"156\":1,\"159\":6,\"162\":1}}],[\"vector\",{\"1\":{\"43\":1}}],[\"version\",{\"1\":{\"42\":1}}],[\"very\",{\"1\":{\"17\":1,\"33\":1,\"37\":1,\"43\":1}}],[\"vanishing\",{\"1\":{\"43\":1}}],[\"various\",{\"1\":{\"43\":1}}],[\"variational\",{\"0\":{\"11\":1,\"18\":1,\"44\":1},\"1\":{\"10\":2,\"12\":1,\"13\":2,\"17\":1,\"18\":1,\"21\":1,\"44\":1}}],[\"valleys\",{\"1\":{\"37\":1}}],[\"value矩阵\",{\"1\":{\"144\":1}}],[\"value的生成\",{\"1\":{\"137\":1}}],[\"value全部来自于同一个输入序列\",{\"1\":{\"129\":1}}],[\"values\",{\"1\":{\"34\":1}}],[\"value\",{\"0\":{\"82\":1,\"137\":1},\"1\":{\"33\":1,\"34\":1,\"36\":1,\"137\":1,\"140\":1,\"144\":1,\"149\":1,\"151\":1}}],[\"vae\",{\"1\":{\"26\":1,\"53\":1}}],[\"vague\",{\"1\":{\"17\":1}}],[\"vi​\",{\"1\":{\"120\":2}}],[\"vi​∈rn×dv​\",{\"1\":{\"119\":1}}],[\"vi特意选择了\",{\"1\":{\"48\":1}}],[\"vi的度量\",{\"1\":{\"48\":1}}],[\"vi的思路是\",{\"1\":{\"45\":1}}],[\"vi通常选择更精确的模式寻求行为\",{\"1\":{\"47\":1}}],[\"vi所用\",{\"1\":{\"47\":1}}],[\"vi选择\",{\"1\":{\"47\":1}}],[\"vision\",{\"0\":{\"28\":1},\"1\":{\"28\":1}}],[\"vit是一种模型架构\",{\"1\":{\"99\":1}}],[\"vital\",{\"1\":{\"43\":1}}],[\"vitae\",{\"0\":{\"1\":1}}],[\"vit\",{\"0\":{\"27\":1,\"28\":1,\"29\":1,\"30\":1,\"31\":1,\"94\":1},\"1\":{\"28\":2,\"29\":6,\"30\":2,\"31\":6,\"95\":1,\"97\":1}}],[\"view\",{\"1\":{\"17\":1}}],[\"vi\",{\"0\":{\"13\":1,\"45\":1},\"1\":{\"44\":1}}],[\"vue\",{\"1\":{\"2\":1,\"4\":1}}],[\"f是每个通道的特征数量\",{\"1\":{\"174\":1}}],[\"ffn\",{\"1\":{\"92\":1}}],[\"fake\",{\"1\":{\"37\":2}}],[\"false\",{\"1\":{\"17\":1}}],[\"feature\",{\"1\":{\"43\":5}}],[\"features\",{\"1\":{\"36\":1,\"43\":2}}],[\"feeding\",{\"1\":{\"43\":1}}],[\"feed\",{\"1\":{\"30\":1,\"37\":2,\"41\":1,\"92\":1}}],[\"frac\",{\"1\":{\"157\":6}}],[\"framework\",{\"1\":{\"42\":1}}],[\"free的\",{\"1\":{\"86\":1}}],[\"free\",{\"0\":{\"86\":1},\"1\":{\"86\":1}}],[\"freely\",{\"1\":{\"36\":1}}],[\"from\",{\"1\":{\"17\":2,\"34\":1,\"37\":1,\"39\":3,\"40\":1,\"41\":3,\"42\":3,\"43\":5}}],[\"frontend\",{\"1\":{\"4\":1}}],[\"fit\",{\"1\":{\"43\":1}}],[\"fine\",{\"1\":{\"43\":2}}],[\"finer\",{\"1\":{\"42\":1}}],[\"final\",{\"1\":{\"36\":1,\"39\":1,\"40\":1,\"41\":1,\"43\":1}}],[\"fixed\",{\"1\":{\"39\":1}}],[\"first\",{\"1\":{\"38\":1,\"42\":1}}],[\"field\",{\"1\":{\"17\":1,\"42\":1}}],[\"flow\",{\"1\":{\"43\":1}}],[\"flexibility\",{\"1\":{\"36\":1}}],[\"flexible\",{\"1\":{\"17\":1}}],[\"flattening\",{\"1\":{\"30\":1}}],[\"flatten\",{\"1\":{\"29\":1}}],[\"f\",{\"1\":{\"22\":1,\"24\":1,\"25\":6,\"178\":7}}],[\"found\",{\"1\":{\"193\":1}}],[\"foundational\",{\"1\":{\"17\":1}}],[\"foundation\",{\"1\":{\"9\":1}}],[\"follows\",{\"1\":{\"39\":1}}],[\"following\",{\"1\":{\"34\":1}}],[\"folder\",{\"1\":{\"9\":1}}],[\"formula\",{\"1\":{\"34\":2}}],[\"formalizes\",{\"1\":{\"34\":1}}],[\"forward\",{\"0\":{\"39\":1},\"1\":{\"30\":1,\"38\":1,\"39\":2,\"40\":3,\"42\":1,\"47\":1,\"92\":1}}],[\"for\",{\"0\":{\"64\":1},\"1\":{\"2\":1,\"4\":3,\"6\":1,\"17\":7,\"34\":1,\"36\":2,\"37\":1,\"39\":2,\"40\":1,\"42\":1,\"43\":8,\"68\":1,\"114\":1,\"137\":2}}],[\"further\",{\"1\":{\"17\":1,\"43\":1}}],[\"furthermore\",{\"1\":{\"17\":1}}],[\"fundamental\",{\"1\":{\"17\":1}}],[\"fundamentally\",{\"1\":{\"17\":1,\"43\":1}}],[\"function\",{\"0\":{\"36\":1,\"107\":1},\"1\":{\"25\":1,\"33\":1,\"34\":2,\"35\":1,\"36\":4,\"37\":3,\"62\":1,\"68\":1}}],[\"functions\",{\"1\":{\"4\":1}}],[\"funcs\",{\"1\":{\"4\":1}}],[\"full\",{\"1\":{\"2\":1,\"4\":1,\"17\":1,\"77\":1}}],[\"wv\",{\"1\":{\"137\":1,\"141\":1,\"144\":1}}],[\"wk\",{\"1\":{\"137\":1,\"141\":1,\"144\":1}}],[\"wq\",{\"1\":{\"137\":1,\"141\":1,\"144\":1}}],[\"w2q​=​−0\",{\"1\":{\"126\":1}}],[\"w1q​=​0\",{\"1\":{\"126\":1}}],[\"w1^v\",{\"1\":{\"124\":1}}],[\"w1^k\",{\"1\":{\"124\":1}}],[\"w1^q\",{\"1\":{\"124\":1}}],[\"w^o\",{\"1\":{\"124\":1}}],[\"w^=t−n1​t=n+1∑t​wt​\",{\"1\":{\"15\":1}}],[\"write\",{\"0\":{\"79\":1}}],[\"water\",{\"1\":{\"39\":3}}],[\"way\",{\"1\":{\"37\":1,\"41\":1}}],[\"want\",{\"1\":{\"37\":2}}],[\"wasserstein\",{\"1\":{\"104\":1}}],[\"was\",{\"1\":{\"33\":1,\"38\":1,\"40\":3,\"41\":1,\"42\":1,\"131\":1}}],[\"wo=​0\",{\"1\":{\"126\":1}}],[\"wo∈rdmodel​×dmodel​\",{\"1\":{\"122\":1}}],[\"wo\",{\"1\":{\"122\":1,\"125\":1}}],[\"would\",{\"1\":{\"33\":1,\"34\":1}}],[\"work\",{\"0\":{\"38\":1},\"1\":{\"42\":1}}],[\"works\",{\"1\":{\"33\":1,\"37\":1}}],[\"workflow\",{\"1\":{\"4\":1}}],[\"world\",{\"1\":{\"2\":1}}],[\"weigh\",{\"1\":{\"43\":1}}],[\"weights\",{\"1\":{\"37\":4,\"154\":1}}],[\"weight\",{\"0\":{\"20\":1,\"67\":1},\"1\":{\"67\":2,\"68\":1}}],[\"well\",{\"1\":{\"41\":2,\"42\":1,\"43\":1}}],[\"we\",{\"0\":{\"20\":1},\"1\":{\"34\":1,\"37\":5,\"39\":1,\"40\":6,\"41\":10,\"43\":1}}],[\"websites\",{\"1\":{\"9\":1}}],[\"web\",{\"1\":{\"4\":2}}],[\"when\",{\"1\":{\"43\":1}}],[\"where\",{\"1\":{\"37\":1,\"40\":1}}],[\"which\",{\"1\":{\"33\":1,\"34\":2,\"38\":1,\"39\":1,\"43\":2}}],[\"while\",{\"1\":{\"17\":4,\"43\":2}}],[\"why\",{\"0\":{\"20\":1},\"1\":{\"42\":1}}],[\"what\",{\"0\":{\"19\":1,\"42\":1},\"1\":{\"39\":1,\"40\":1,\"41\":1,\"42\":1,\"43\":1}}],[\"wio​\",{\"1\":{\"125\":1}}],[\"wiv​相互独立\",{\"1\":{\"125\":1}}],[\"wik​\",{\"1\":{\"125\":1}}],[\"wik​∈rdmodel​×dk​wiv​∈rdmodel​×dv​wo∈rdmodel​×dmodel​​​​\",{\"1\":{\"123\":1}}],[\"wiq​\",{\"1\":{\"125\":1}}],[\"wiley\",{\"1\":{\"77\":1}}],[\"will\",{\"1\":{\"41\":1}}],[\"widespread\",{\"1\":{\"17\":1}}],[\"with⎩⎨⎧​dk​=dv​=hdmodel​​wiq​\",{\"1\":{\"123\":1}}],[\"within\",{\"1\":{\"41\":1,\"42\":1,\"43\":1}}],[\"without\",{\"1\":{\"37\":1}}],[\"with\",{\"1\":{\"2\":1,\"4\":1,\"33\":1,\"34\":1,\"37\":1,\"39\":1,\"40\":1,\"41\":1,\"43\":2,\"93\":1}}],[\"wt​是第t次迭代的权重值\",{\"1\":{\"15\":1}}],[\"wt​∣w\",{\"1\":{\"15\":2}}],[\"wt​∣d\",{\"1\":{\"15\":2}}],[\"w∣d\",{\"1\":{\"15\":2}}],[\"w|θ\",{\"1\":{\"12\":7}}],[\"w|d\",{\"1\":{\"12\":2,\"19\":3}}],[\"w\",{\"1\":{\"12\":4,\"15\":7,\"18\":6,\"19\":4,\"30\":1,\"132\":6}}],[\"www\",{\"1\":{\"9\":2}}],[\"i=j\",{\"1\":{\"157\":1}}],[\"i=1∑h​headi​⋅wio​\",{\"1\":{\"125\":1}}],[\"ij\",{\"1\":{\"151\":1,\"153\":1}}],[\"ij​\",{\"1\":{\"148\":1}}],[\"ik​\",{\"1\":{\"148\":1}}],[\"i∈\",{\"1\":{\"119\":1}}],[\"idf\",{\"1\":{\"74\":1}}],[\"identity\",{\"1\":{\"52\":1}}],[\"idea\",{\"1\":{\"33\":1,\"38\":1}}],[\"if\",{\"1\":{\"41\":1}}],[\"iclr\",{\"1\":{\"20\":1}}],[\"i的概率分布\",{\"1\":{\"19\":1}}],[\"i和权重w的情况下\",{\"1\":{\"19\":1}}],[\"i|f\",{\"1\":{\"19\":2}}],[\"i\",{\"0\":{\"94\":1,\"95\":1,\"96\":1,\"97\":1,\"99\":1,\"100\":1},\"1\":{\"19\":3,\"33\":2,\"51\":1,\"53\":2,\"95\":1,\"96\":2,\"97\":1,\"98\":1,\"99\":4,\"100\":3,\"120\":1,\"143\":1,\"145\":1,\"149\":1,\"151\":1,\"153\":1,\"178\":4,\"182\":1,\"187\":2}}],[\"iterative\",{\"1\":{\"41\":2,\"42\":1,\"43\":1}}],[\"iteratively\",{\"1\":{\"37\":1}}],[\"its\",{\"1\":{\"33\":2,\"34\":3,\"37\":4,\"38\":2,\"40\":3,\"42\":1,\"43\":6}}],[\"it\",{\"0\":{\"160\":1},\"1\":{\"17\":3,\"34\":3,\"35\":1,\"36\":1,\"37\":4,\"38\":2,\"39\":3,\"40\":6,\"41\":4,\"42\":3,\"43\":5,\"131\":7}}],[\"isn\",{\"1\":{\"33\":1}}],[\"issue\",{\"1\":{\"17\":3}}],[\"is\",{\"0\":{\"19\":1},\"1\":{\"17\":15,\"33\":5,\"34\":11,\"36\":4,\"37\":7,\"38\":4,\"39\":8,\"40\":7,\"41\":6,\"42\":5,\"43\":17}}],[\"invariance\",{\"1\":{\"171\":1}}],[\"involve\",{\"1\":{\"39\":1}}],[\"involves\",{\"1\":{\"35\":1}}],[\"inequality\",{\"1\":{\"52\":1}}],[\"indiscernibles\",{\"1\":{\"52\":1}}],[\"indistinguishable\",{\"1\":{\"39\":1}}],[\"inherent\",{\"1\":{\"43\":1}}],[\"injecting\",{\"1\":{\"43\":1}}],[\"information\",{\"1\":{\"43\":2}}],[\"inference\",{\"0\":{\"10\":1,\"11\":1,\"18\":1,\"44\":1},\"1\":{\"10\":2,\"13\":2,\"17\":5,\"18\":1,\"21\":1,\"44\":1}}],[\"ink\",{\"1\":{\"39\":1}}],[\"instead\",{\"1\":{\"37\":1,\"40\":1,\"43\":1}}],[\"inspired\",{\"1\":{\"33\":1}}],[\"inputs\",{\"1\":{\"34\":1}}],[\"input\",{\"1\":{\"33\":4,\"36\":2,\"40\":1,\"43\":4,\"113\":1}}],[\"incorporate\",{\"1\":{\"43\":1}}],[\"incorrect\",{\"1\":{\"33\":1}}],[\"incredibly\",{\"1\":{\"41\":1}}],[\"increasingly\",{\"1\":{\"39\":1}}],[\"increasing\",{\"1\":{\"17\":1,\"43\":1}}],[\"including\",{\"1\":{\"17\":1}}],[\"integrated\",{\"1\":{\"43\":1}}],[\"integration\",{\"1\":{\"43\":1}}],[\"integer\",{\"1\":{\"43\":1}}],[\"intertwined\",{\"1\":{\"17\":1}}],[\"interpolation\",{\"1\":{\"14\":1}}],[\"internal\",{\"1\":{\"4\":2,\"169\":1,\"175\":1}}],[\"intuitive\",{\"1\":{\"38\":1}}],[\"intuition\",{\"1\":{\"37\":1}}],[\"intricate\",{\"1\":{\"43\":1}}],[\"intractable\",{\"1\":{\"37\":1,\"45\":1}}],[\"introduce\",{\"1\":{\"17\":2}}],[\"into\",{\"1\":{\"17\":2,\"33\":1,\"37\":1,\"38\":1,\"41\":1,\"42\":1,\"43\":2}}],[\"innocuous\",{\"1\":{\"17\":1}}],[\"inaccurate\",{\"1\":{\"17\":1}}],[\"initially\",{\"1\":{\"39\":1}}],[\"initial\",{\"1\":{\"17\":1,\"42\":1}}],[\"in期的迭代次数\",{\"1\":{\"15\":1}}],[\"in\",{\"0\":{\"16\":1,\"17\":1,\"19\":1,\"42\":1,\"43\":1,\"44\":1},\"1\":{\"15\":1,\"17\":10,\"33\":1,\"36\":3,\"38\":1,\"39\":1,\"40\":3,\"41\":1,\"42\":3,\"43\":12,\"68\":1,\"103\":1}}],[\"iiya\",{\"1\":{\"9\":1}}],[\"imagine\",{\"1\":{\"33\":1}}],[\"images\",{\"1\":{\"33\":1,\"34\":1,\"36\":1,\"38\":1,\"40\":1,\"41\":3,\"43\":1}}],[\"image\",{\"0\":{\"39\":1,\"40\":1,\"41\":1},\"1\":{\"6\":1,\"29\":1,\"33\":3,\"34\":1,\"36\":1,\"37\":1,\"39\":5,\"40\":7,\"41\":10,\"42\":6,\"43\":11,\"95\":1,\"160\":1,\"163\":1,\"174\":1}}],[\"improved\",{\"0\":{\"105\":1},\"1\":{\"93\":1}}],[\"improving\",{\"1\":{\"43\":1}}],[\"importance\",{\"1\":{\"43\":1}}],[\"important\",{\"1\":{\"17\":1}}],[\"impossible\",{\"1\":{\"34\":1}}],[\"implausible\",{\"1\":{\"33\":1}}],[\"implemented\",{\"1\":{\"4\":1}}],[\"implement\",{\"1\":{\"2\":1}}],[\"impact\",{\"1\":{\"17\":1}}],[\"impacts\",{\"1\":{\"17\":1}}],[\"ioffe\",{\"1\":{\"175\":1}}],[\"io\",{\"1\":{\"1\":1,\"114\":1}}],[\"pmatrix\",{\"1\":{\"178\":2}}],[\"png\",{\"1\":{\"160\":1,\"163\":1,\"174\":1}}],[\"ppo的目标函数里通常还会显式地加入熵正则化项\",{\"1\":{\"108\":1}}],[\"ppo与模式坍塌的关系\",{\"0\":{\"108\":1}}],[\"ppo作为一种actor\",{\"1\":{\"107\":1}}],[\"ppo通过其核心的截断机制来维持训练的稳定性\",{\"1\":{\"108\":1}}],[\"ppo通过其核心机制间接地\",{\"1\":{\"105\":1}}],[\"ppo通过限制单次更新的步长\",{\"1\":{\"106\":1}}],[\"ppo也不允许新策略过度惩罚它\",{\"1\":{\"106\":1}}],[\"ppo也不允许新策略将选中该动作的概率无限拔高\",{\"1\":{\"106\":1}}],[\"ppo也是一种actor\",{\"1\":{\"84\":1}}],[\"ppo设置了一个超参数\",{\"1\":{\"106\":1}}],[\"ppo主要通过以下两个相互关联的机制来做到这一点\",{\"1\":{\"105\":1}}],[\"ppo\",{\"0\":{\"105\":1},\"1\":{\"83\":1,\"84\":1,\"86\":1,\"87\":1,\"88\":2,\"108\":1}}],[\"pdf\",{\"1\":{\"79\":2}}],[\"pca\",{\"1\":{\"74\":2}}],[\"pytorch中手动添加l1正则化的示例\",{\"1\":{\"68\":1}}],[\"python\",{\"1\":{\"2\":1,\"4\":1,\"8\":1}}],[\"p=1\",{\"1\":{\"68\":1}}],[\"p=\",{\"1\":{\"54\":1}}],[\"p=q\",{\"1\":{\"52\":2}}],[\"p∥m\",{\"1\":{\"51\":1,\"52\":1,\"54\":1}}],[\"p∥q\",{\"1\":{\"47\":2,\"51\":1,\"52\":1,\"53\":2,\"54\":2}}],[\"p+q\",{\"1\":{\"51\":1,\"54\":1}}],[\"phase\",{\"1\":{\"39\":1}}],[\"physical\",{\"1\":{\"33\":1}}],[\"physics\",{\"1\":{\"33\":1}}],[\"pii=s0148296319304564\",{\"1\":{\"79\":1}}],[\"picture\",{\"1\":{\"39\":1}}],[\"pictures\",{\"1\":{\"38\":1}}],[\"pick\",{\"1\":{\"37\":1}}],[\"pipeline\",{\"1\":{\"2\":1}}],[\"pipelines\",{\"1\":{\"2\":1,\"8\":1}}],[\"pure\",{\"1\":{\"39\":2,\"41\":2,\"42\":1}}],[\"purpose\",{\"1\":{\"34\":1,\"38\":1,\"43\":1}}],[\"pulling\",{\"1\":{\"37\":1}}],[\"pushing\",{\"1\":{\"37\":1}}],[\"push\",{\"1\":{\"37\":4}}],[\"perfectly\",{\"1\":{\"43\":1}}],[\"perform\",{\"1\":{\"42\":1}}],[\"performing\",{\"1\":{\"17\":1,\"37\":1}}],[\"performance\",{\"1\":{\"17\":2}}],[\"perceptron\",{\"1\":{\"36\":1}}],[\"plausible\",{\"1\":{\"33\":1}}],[\"p²\",{\"1\":{\"30\":3}}],[\"policy\",{\"0\":{\"83\":1,\"87\":2},\"1\":{\"83\":2,\"87\":2,\"103\":1}}],[\"policy类型\",{\"1\":{\"82\":1,\"83\":1}}],[\"pooling\",{\"1\":{\"43\":1}}],[\"poorly\",{\"1\":{\"17\":1}}],[\"powerful\",{\"1\":{\"40\":1,\"43\":1}}],[\"point\",{\"1\":{\"34\":3,\"37\":3,\"40\":1,\"41\":2}}],[\"points\",{\"1\":{\"33\":2,\"37\":2,\"42\":1,\"43\":1}}],[\"potential\",{\"1\":{\"17\":1}}],[\"potentially\",{\"1\":{\"17\":1}}],[\"posts\",{\"0\":{\"195\":1,\"196\":1}}],[\"posterior\",{\"1\":{\"17\":2,\"45\":1}}],[\"posteriori\",{\"1\":{\"14\":1}}],[\"possible\",{\"1\":{\"33\":1,\"34\":3,\"40\":1}}],[\"position\",{\"0\":{\"177\":1},\"1\":{\"176\":1,\"184\":1}}],[\"positional\",{\"1\":{\"30\":2}}],[\"positive\",{\"1\":{\"17\":1,\"34\":1,\"37\":1}}],[\"papers\",{\"0\":{\"202\":1}}],[\"pasted\",{\"1\":{\"160\":1,\"163\":1,\"174\":1}}],[\"passes\",{\"1\":{\"36\":1}}],[\"passing\",{\"1\":{\"30\":1}}],[\"pay\",{\"1\":{\"43\":1}}],[\"path\",{\"1\":{\"41\":1,\"43\":3}}],[\"patch\",{\"1\":{\"30\":2}}],[\"patches\",{\"1\":{\"29\":1,\"30\":1,\"99\":1}}],[\"patching\",{\"1\":{\"29\":1,\"30\":1}}],[\"param\",{\"1\":{\"68\":2}}],[\"parameter\",{\"1\":{\"17\":1}}],[\"parameters\",{\"1\":{\"17\":2,\"40\":1,\"67\":1,\"68\":1}}],[\"parity\",{\"1\":{\"43\":1}}],[\"parts\",{\"1\":{\"43\":3}}],[\"part\",{\"1\":{\"37\":3,\"43\":2}}],[\"partial\",{\"1\":{\"157\":10}}],[\"particular\",{\"1\":{\"40\":1,\"42\":1}}],[\"particularly\",{\"1\":{\"17\":1,\"43\":1}}],[\"partition\",{\"1\":{\"34\":1,\"37\":2}}],[\"paas\",{\"1\":{\"2\":1,\"4\":1,\"8\":1}}],[\"p\",{\"1\":{\"12\":3,\"13\":3,\"15\":6,\"18\":2,\"19\":11,\"22\":2,\"30\":4,\"34\":3,\"45\":9,\"46\":3,\"47\":8,\"50\":1,\"51\":6,\"52\":7,\"53\":2,\"174\":2}}],[\"preprint\",{\"1\":{\"175\":2}}],[\"prepending\",{\"1\":{\"30\":1}}],[\"premature\",{\"1\":{\"103\":1}}],[\"precise\",{\"1\":{\"43\":1}}],[\"predictor\",{\"1\":{\"97\":1}}],[\"predictive\",{\"1\":{\"95\":1}}],[\"predicting\",{\"1\":{\"41\":1}}],[\"prediction\",{\"1\":{\"40\":2,\"42\":1,\"43\":3}}],[\"predicted\",{\"1\":{\"41\":1,\"42\":1,\"43\":1}}],[\"predicts\",{\"1\":{\"41\":2}}],[\"predict\",{\"1\":{\"40\":2,\"42\":1}}],[\"predefined\",{\"1\":{\"39\":1}}],[\"preserve\",{\"1\":{\"43\":1}}],[\"preserving\",{\"1\":{\"43\":1}}],[\"present\",{\"1\":{\"4\":1}}],[\"pressing\",{\"1\":{\"17\":1}}],[\"prize\",{\"1\":{\"73\":1}}],[\"principle\",{\"0\":{\"35\":1},\"1\":{\"34\":1}}],[\"primary\",{\"1\":{\"17\":1,\"38\":1,\"42\":1}}],[\"priors\",{\"1\":{\"17\":7}}],[\"prior\",{\"0\":{\"16\":1,\"17\":1},\"1\":{\"17\":13}}],[\"practitioners\",{\"1\":{\"17\":1}}],[\"practical\",{\"1\":{\"17\":1}}],[\"practice\",{\"1\":{\"17\":1}}],[\"pragmatic\",{\"1\":{\"17\":1}}],[\"propagation\",{\"0\":{\"154\":1}}],[\"proximal\",{\"1\":{\"83\":1}}],[\"providing\",{\"1\":{\"43\":1}}],[\"proven\",{\"1\":{\"2\":1}}],[\"produce\",{\"1\":{\"42\":1}}],[\"products\",{\"1\":{\"2\":1}}],[\"procedure\",{\"1\":{\"39\":1}}],[\"process\",{\"0\":{\"39\":1,\"40\":1},\"1\":{\"17\":1,\"37\":1,\"38\":3,\"39\":4,\"40\":4,\"41\":2,\"42\":5,\"43\":3}}],[\"progressively\",{\"1\":{\"42\":1,\"43\":2}}],[\"progress\",{\"1\":{\"17\":1,\"39\":1}}],[\"prohibitively\",{\"1\":{\"17\":1}}],[\"probing\",{\"1\":{\"100\":1}}],[\"probability\",{\"1\":{\"17\":1,\"33\":4,\"34\":4,\"36\":1,\"106\":1}}],[\"problem\",{\"1\":{\"17\":2,\"43\":1}}],[\"prompt\",{\"1\":{\"4\":1,\"43\":2}}],[\"projection\",{\"1\":{\"30\":1}}],[\"project\",{\"1\":{\"4\":1}}],[\"professional\",{\"0\":{\"3\":1}}],[\"szegedy\",{\"1\":{\"175\":1}}],[\"square\",{\"1\":{\"164\":2,\"166\":1}}],[\"sqrt\",{\"1\":{\"157\":1,\"174\":1}}],[\"sql\",{\"1\":{\"8\":1}}],[\"sj​\",{\"1\":{\"157\":1}}],[\"s=qkt\",{\"1\":{\"154\":1,\"158\":1}}],[\"s=qkt=\",{\"1\":{\"152\":1}}],[\"s=q⋅kt∈rn×n\",{\"1\":{\"145\":1}}],[\"sscaled\",{\"1\":{\"148\":2}}],[\"sscaled​=s\",{\"1\":{\"157\":2}}],[\"sscaled​=2​s​=\",{\"1\":{\"152\":1}}],[\"sscaled​=dk​​s​\",{\"1\":{\"147\":1}}],[\"sscaled​\",{\"1\":{\"148\":1,\"150\":1,\"157\":2}}],[\"ssm\",{\"0\":{\"110\":1,\"113\":1},\"1\":{\"109\":1,\"110\":1,\"112\":2,\"113\":2,\"115\":3}}],[\"sram\",{\"1\":{\"114\":1}}],[\"sk​\",{\"1\":{\"113\":3}}],[\"sk​=linear\",{\"1\":{\"113\":1}}],[\"skip\",{\"1\":{\"43\":5}}],[\"s4\",{\"1\":{\"113\":1,\"115\":1}}],[\"s0148296319304564\",{\"1\":{\"79\":1}}],[\"s0148296319x00089\",{\"1\":{\"79\":1}}],[\"s2\",{\"1\":{\"79\":2}}],[\"svd\",{\"0\":{\"71\":1,\"72\":1,\"73\":1,\"74\":1},\"1\":{\"71\":1,\"72\":2,\"73\":3,\"74\":7,\"75\":2,\"76\":1}}],[\"svi\",{\"0\":{\"13\":1},\"1\":{\"18\":1}}],[\"symmetry\",{\"1\":{\"52\":1}}],[\"symmetric\",{\"1\":{\"43\":1}}],[\"systematically\",{\"0\":{\"39\":1}}],[\"systems\",{\"1\":{\"4\":1,\"6\":1}}],[\"system\",{\"1\":{\"4\":1,\"33\":1}}],[\"slightly\",{\"1\":{\"41\":2,\"42\":1}}],[\"slow\",{\"1\":{\"17\":1}}],[\"sarsa\",{\"1\":{\"87\":1}}],[\"sac\",{\"1\":{\"84\":1,\"86\":1,\"87\":1,\"88\":1}}],[\"same\",{\"1\":{\"43\":1}}],[\"sampling\",{\"1\":{\"17\":1,\"37\":1}}],[\"sample\",{\"1\":{\"4\":1,\"37\":5}}],[\"sauce\",{\"1\":{\"43\":1}}],[\"say\",{\"1\":{\"40\":1}}],[\"small\",{\"1\":{\"39\":1,\"41\":2}}],[\"size的情况下也能很好的工作\",{\"1\":{\"174\":1}}],[\"size的影响\",{\"1\":{\"174\":1}}],[\"size\",{\"1\":{\"174\":1}}],[\"sij​=qi​⋅kjt​\",{\"1\":{\"145\":1}}],[\"sid=dfe244208eb5c1425d1b401\",{\"1\":{\"79\":1}}],[\"signature=eda871ed11ea3537e664b33ba7b2b5a284ec92a24a315646a740db06b1fada96\",{\"1\":{\"79\":1}}],[\"signedheaders=host\",{\"1\":{\"79\":1}}],[\"significant\",{\"1\":{\"17\":2}}],[\"significantly\",{\"1\":{\"17\":1}}],[\"sin\",{\"1\":{\"178\":2}}],[\"since\",{\"1\":{\"37\":1}}],[\"single\",{\"0\":{\"142\":1},\"1\":{\"36\":2,\"142\":1}}],[\"simple\",{\"1\":{\"36\":1,\"42\":1}}],[\"similar\",{\"1\":{\"33\":1,\"38\":1}}],[\"sciencedirectassets\",{\"1\":{\"79\":1}}],[\"scratch\",{\"1\":{\"41\":1}}],[\"scores\",{\"1\":{\"154\":1}}],[\"score\",{\"1\":{\"25\":1,\"33\":3}}],[\"scale\",{\"1\":{\"171\":1}}],[\"scaled\",{\"1\":{\"157\":2}}],[\"scalar\",{\"1\":{\"33\":1,\"36\":1}}],[\"scalable\",{\"1\":{\"2\":1,\"17\":1}}],[\"scaling\",{\"1\":{\"4\":1,\"169\":1,\"173\":1}}],[\"sgd\",{\"0\":{\"78\":1},\"1\":{\"18\":1,\"25\":1,\"67\":1,\"78\":1}}],[\"shift\",{\"1\":{\"169\":1,\"175\":1}}],[\"should\",{\"1\":{\"38\":1}}],[\"shown\",{\"1\":{\"17\":1}}],[\"sha256\",{\"1\":{\"79\":1}}],[\"shannon\",{\"0\":{\"50\":1},\"1\":{\"49\":1,\"53\":1}}],[\"sharp\",{\"1\":{\"43\":1}}],[\"shares\",{\"1\":{\"41\":1}}],[\"sharing\",{\"1\":{\"9\":1}}],[\"shape\",{\"1\":{\"37\":1}}],[\"specification\",{\"1\":{\"17\":2}}],[\"specific\",{\"1\":{\"17\":1,\"36\":1,\"40\":1,\"42\":1}}],[\"specifying\",{\"1\":{\"17\":1}}],[\"sparse\",{\"0\":{\"78\":1}}],[\"spark\",{\"1\":{\"8\":1}}],[\"spatial\",{\"1\":{\"43\":5}}],[\"space\",{\"1\":{\"17\":1,\"109\":1}}],[\"street\",{\"1\":{\"131\":4}}],[\"structure\",{\"1\":{\"43\":2}}],[\"student\",{\"1\":{\"186\":1}}],[\"studies\",{\"1\":{\"17\":1}}],[\"stuck\",{\"1\":{\"103\":1}}],[\"stopping\",{\"1\":{\"69\":1}}],[\"stochastic\",{\"0\":{\"18\":1},\"1\":{\"10\":1,\"18\":1}}],[\"still\",{\"1\":{\"39\":1}}],[\"steps\",{\"1\":{\"37\":1}}],[\"step\",{\"0\":{\"33\":1,\"34\":1,\"35\":1},\"1\":{\"32\":2,\"37\":3,\"39\":3,\"40\":1,\"41\":3,\"42\":3,\"43\":2,\"68\":1}}],[\"stages\",{\"1\":{\"38\":1}}],[\"starting\",{\"1\":{\"41\":1}}],[\"starts\",{\"1\":{\"40\":1}}],[\"start\",{\"1\":{\"37\":1,\"39\":1,\"41\":1}}],[\"statistical\",{\"1\":{\"34\":1,\"39\":1}}],[\"static\",{\"1\":{\"4\":1,\"33\":1,\"39\":1}}],[\"states\",{\"1\":{\"34\":1}}],[\"state\",{\"1\":{\"33\":2,\"37\":1,\"43\":1,\"109\":1,\"110\":1}}],[\"stable\",{\"1\":{\"33\":1,\"43\":1}}],[\"stakes\",{\"1\":{\"17\":1}}],[\"standard\",{\"1\":{\"17\":1,\"43\":1}}],[\"stack\",{\"1\":{\"2\":1,\"4\":1}}],[\"s\",{\"1\":{\"9\":1,\"17\":2,\"32\":1,\"33\":1,\"34\":2,\"36\":1,\"37\":3,\"39\":2,\"40\":3,\"42\":1,\"43\":3,\"82\":1,\"106\":1,\"107\":4,\"157\":4,\"158\":1,\"175\":1}}],[\"semantic\",{\"1\":{\"73\":1}}],[\"semi\",{\"1\":{\"6\":1}}],[\"security\",{\"1\":{\"79\":1}}],[\"secret\",{\"1\":{\"43\":1}}],[\"second\",{\"1\":{\"39\":1}}],[\"series\",{\"1\":{\"43\":1}}],[\"serve\",{\"1\":{\"36\":1}}],[\"service\",{\"1\":{\"4\":2}}],[\"services\",{\"1\":{\"2\":1,\"8\":1}}],[\"settles\",{\"1\":{\"37\":1}}],[\"settings\",{\"1\":{\"17\":2}}],[\"several\",{\"1\":{\"36\":1,\"43\":1}}],[\"selectivity\",{\"1\":{\"113\":1}}],[\"selection\",{\"1\":{\"17\":4}}],[\"selfattention\",{\"1\":{\"132\":1}}],[\"self\",{\"0\":{\"129\":1,\"134\":1,\"136\":1,\"160\":1},\"1\":{\"29\":1,\"30\":1,\"43\":1,\"129\":2,\"130\":1,\"131\":2,\"132\":1,\"133\":2,\"134\":2,\"135\":2,\"136\":1,\"140\":2,\"141\":1,\"161\":1,\"162\":2}}],[\"seeking\",{\"1\":{\"47\":2,\"48\":1}}],[\"see\",{\"1\":{\"20\":1,\"39\":1,\"43\":1}}],[\"seemingly\",{\"1\":{\"17\":1}}],[\"search\",{\"1\":{\"4\":1}}],[\"softmax后的结果\",{\"1\":{\"141\":1}}],[\"softmax\",{\"1\":{\"132\":1,\"139\":4,\"148\":1,\"151\":1,\"152\":1,\"157\":5,\"163\":1}}],[\"soft\",{\"1\":{\"84\":1,\"104\":1,\"186\":1}}],[\"software\",{\"1\":{\"4\":1}}],[\"sophisticated\",{\"1\":{\"17\":1}}],[\"so\",{\"1\":{\"17\":1,\"34\":1,\"40\":1,\"42\":1}}],[\"sole\",{\"1\":{\"17\":1}}],[\"solutions\",{\"1\":{\"4\":1,\"8\":1}}],[\"source\",{\"1\":{\"4\":1}}],[\"surrogate\",{\"0\":{\"106\":1}}],[\"surprisingly\",{\"1\":{\"38\":1}}],[\"suited\",{\"1\":{\"42\":1,\"43\":1}}],[\"sumj=1n​aij​=1\",{\"1\":{\"148\":1}}],[\"sum\",{\"1\":{\"34\":1,\"151\":2,\"157\":2}}],[\"summary\",{\"0\":{\"2\":1},\"1\":{\"43\":1}}],[\"sublayer\",{\"1\":{\"161\":1}}],[\"subtracted\",{\"1\":{\"42\":1}}],[\"subtract\",{\"1\":{\"41\":2}}],[\"subjective\",{\"1\":{\"17\":1}}],[\"substantial\",{\"1\":{\"17\":1}}],[\"such\",{\"1\":{\"17\":3,\"41\":1}}],[\"successful\",{\"1\":{\"4\":1}}],[\"supervised\",{\"1\":{\"6\":1}}],[\"bk​\",{\"1\":{\"114\":4}}],[\"bk​=linearb​\",{\"1\":{\"113\":1}}],[\"b=\",{\"1\":{\"111\":1}}],[\"b∈rn×1\",{\"1\":{\"110\":1}}],[\"bn如右侧所示\",{\"1\":{\"174\":1}}],[\"bn更适合于图像类的任务\",{\"1\":{\"174\":1}}],[\"bn和ln的使用取决于数据和任务的特性\",{\"1\":{\"174\":1}}],[\"bn的主要目的是为了解决深度神经网络训练过程中的内部协变量偏移问题\",{\"1\":{\"174\":1}}],[\"bn\",{\"1\":{\"70\":1}}],[\"block\",{\"1\":{\"98\":1}}],[\"blocks\",{\"1\":{\"43\":5}}],[\"blurry\",{\"1\":{\"43\":1}}],[\"black\",{\"1\":{\"39\":1}}],[\"big\",{\"1\":{\"34\":1}}],[\"biases\",{\"1\":{\"17\":1}}],[\"but\",{\"1\":{\"33\":1,\"39\":1,\"41\":1,\"43\":1}}],[\"burden\",{\"1\":{\"17\":1}}],[\"built\",{\"1\":{\"4\":2}}],[\"building\",{\"1\":{\"35\":1}}],[\"build\",{\"0\":{\"32\":1,\"35\":1},\"1\":{\"2\":2}}],[\"begin\",{\"1\":{\"178\":1}}],[\"behavior\",{\"1\":{\"43\":1}}],[\"behind\",{\"1\":{\"38\":1}}],[\"become\",{\"1\":{\"178\":1}}],[\"becomes\",{\"1\":{\"39\":2,\"42\":1}}],[\"because\",{\"1\":{\"34\":1,\"40\":1,\"43\":1,\"131\":1}}],[\"bell\",{\"1\":{\"39\":1}}],[\"beliefs\",{\"1\":{\"17\":2}}],[\"beauty\",{\"1\":{\"36\":1}}],[\"bert等经典模型\",{\"1\":{\"172\":1}}],[\"bert\",{\"1\":{\"29\":1}}],[\"been\",{\"1\":{\"17\":1}}],[\"be\",{\"1\":{\"17\":2,\"33\":1,\"36\":2,\"37\":2,\"38\":1,\"39\":1}}],[\"being\",{\"1\":{\"17\":3,\"39\":1,\"43\":1}}],[\"best\",{\"1\":{\"17\":1,\"36\":1}}],[\"before\",{\"1\":{\"17\":1}}],[\"between\",{\"0\":{\"13\":1},\"1\":{\"33\":1,\"34\":1,\"43\":1}}],[\"b\",{\"0\":{\"87\":1,\"148\":1},\"1\":{\"14\":1,\"56\":1,\"61\":4,\"111\":3,\"112\":1,\"113\":4,\"115\":1}}],[\"boxes\",{\"1\":{\"93\":1}}],[\"bottleneck\",{\"1\":{\"43\":1}}],[\"both\",{\"1\":{\"17\":1,\"38\":1,\"43\":1}}],[\"boltzmann\",{\"0\":{\"34\":1},\"1\":{\"34\":1}}],[\"bound\",{\"1\":{\"12\":2,\"22\":1,\"46\":1}}],[\"books\",{\"1\":{\"9\":1}}],[\"boost\",{\"1\":{\"4\":1}}],[\"by\",{\"0\":{\"11\":1},\"1\":{\"12\":1,\"17\":2,\"32\":1,\"33\":2,\"34\":1,\"36\":1,\"37\":2,\"39\":2,\"41\":3,\"42\":3,\"43\":4,\"175\":1}}],[\"b145\",{\"1\":{\"9\":1}}],[\"ba\",{\"1\":{\"175\":1}}],[\"backward\",{\"1\":{\"68\":1}}],[\"backbone\",{\"1\":{\"43\":1,\"92\":1}}],[\"back\",{\"0\":{\"154\":1},\"1\":{\"41\":1}}],[\"backprop\",{\"0\":{\"11\":1},\"1\":{\"12\":1}}],[\"bad\",{\"1\":{\"33\":1}}],[\"barrier\",{\"1\":{\"17\":1}}],[\"batch包含m个样本\",{\"1\":{\"174\":1}}],[\"batch进行处理\",{\"1\":{\"174\":1}}],[\"batch\",{\"0\":{\"174\":1},\"1\":{\"13\":1,\"69\":1,\"174\":7,\"175\":1}}],[\"bayes\",{\"0\":{\"11\":1},\"1\":{\"10\":1,\"11\":1,\"12\":1,\"13\":1,\"14\":1,\"15\":1,\"18\":1,\"19\":1,\"20\":1}}],[\"bayesian\",{\"0\":{\"10\":1,\"16\":1,\"17\":1,\"77\":1,\"197\":1},\"1\":{\"6\":1,\"8\":1,\"14\":1,\"17\":10,\"19\":1,\"20\":1,\"21\":1}}],[\"based\",{\"0\":{\"33\":1,\"35\":1,\"82\":1,\"83\":1,\"86\":1},\"1\":{\"4\":1,\"32\":1,\"33\":1,\"37\":1,\"43\":1,\"86\":1,\"95\":1}}],[\"break\",{\"1\":{\"32\":1,\"34\":1}}],[\"broken\",{\"1\":{\"38\":1}}],[\"broader\",{\"1\":{\"17\":1}}],[\"browsing\",{\"1\":{\"4\":1}}],[\"brief\",{\"0\":{\"2\":1}}],[\"aj​\",{\"1\":{\"157\":1}}],[\"axis=1\",{\"1\":{\"157\":2}}],[\"a=\",{\"1\":{\"152\":1}}],[\"a=softmax\",{\"1\":{\"148\":1,\"154\":1,\"157\":2}}],[\"a=exp\",{\"1\":{\"111\":1}}],[\"aware\",{\"1\":{\"114\":1}}],[\"ak​\",{\"1\":{\"114\":2}}],[\"a∈rn×n\",{\"1\":{\"110\":1}}],[\"average\",{\"1\":{\"97\":1}}],[\"a3c\",{\"1\":{\"84\":1}}],[\"a2c\",{\"1\":{\"84\":1,\"87\":1,\"88\":1}}],[\"a|s\",{\"1\":{\"83\":1,\"106\":4}}],[\"amz\",{\"1\":{\"79\":7}}],[\"amount\",{\"1\":{\"39\":1,\"41\":1,\"43\":1}}],[\"agent\",{\"1\":{\"82\":1}}],[\"agupubs\",{\"1\":{\"77\":1}}],[\"again\",{\"1\":{\"37\":1,\"39\":1,\"41\":1}}],[\"augmentation\",{\"1\":{\"69\":1}}],[\"autoencoders\",{\"1\":{\"96\":1}}],[\"autogen\",{\"1\":{\"4\":1}}],[\"auto\",{\"1\":{\"4\":1}}],[\"after\",{\"1\":{\"39\":1,\"178\":1}}],[\"affect\",{\"1\":{\"17\":2}}],[\"at\",{\"1\":{\"33\":1,\"38\":1,\"39\":2,\"40\":4,\"41\":5,\"42\":3,\"43\":5}}],[\"attn1\",{\"1\":{\"124\":1}}],[\"attention详细计算方法\",{\"0\":{\"136\":1}}],[\"attention是一种让序列模型能够高效\",{\"1\":{\"135\":1}}],[\"attention会被屏蔽\",{\"1\":{\"134\":1}}],[\"attention有两种主要应用\",{\"1\":{\"134\":1}}],[\"attention在transformer中的角色\",{\"0\":{\"134\":1}}],[\"attention的计算可以完全并行化\",{\"1\":{\"133\":1}}],[\"attention一步到位\",{\"1\":{\"133\":1}}],[\"attention如此强大\",{\"0\":{\"133\":1}}],[\"attention\",{\"0\":{\"116\":1,\"117\":1,\"129\":1,\"142\":1,\"207\":1},\"1\":{\"20\":1,\"29\":1,\"30\":1,\"43\":5,\"117\":1,\"129\":2,\"130\":1,\"131\":2,\"132\":2,\"134\":2,\"135\":2,\"136\":1,\"140\":2,\"141\":1,\"142\":1,\"161\":1,\"162\":2}}],[\"attractive\",{\"1\":{\"17\":1}}],[\"arxiv\",{\"1\":{\"175\":4}}],[\"arbitrary\",{\"1\":{\"43\":1}}],[\"arguably\",{\"1\":{\"43\":1}}],[\"article\",{\"1\":{\"93\":1}}],[\"articles\",{\"1\":{\"93\":1}}],[\"art\",{\"1\":{\"41\":1,\"43\":1}}],[\"area\",{\"1\":{\"17\":1}}],[\"are\",{\"0\":{\"20\":2},\"1\":{\"17\":2,\"20\":2,\"33\":7,\"36\":1,\"41\":2,\"43\":2}}],[\"arc\",{\"1\":{\"9\":1}}],[\"architecture\",{\"1\":{\"6\":1,\"36\":1,\"40\":1,\"43\":3,\"95\":1}}],[\"accelerating\",{\"1\":{\"175\":1}}],[\"accurate\",{\"1\":{\"17\":1,\"40\":1,\"43\":1}}],[\"accuracy\",{\"1\":{\"4\":1}}],[\"achieved\",{\"1\":{\"43\":2}}],[\"actor就会迅速收敛到第一个它发现的\",{\"1\":{\"103\":1}}],[\"actor\",{\"0\":{\"84\":1,\"103\":1,\"206\":1},\"1\":{\"84\":4,\"103\":1,\"104\":2,\"106\":1}}],[\"acts\",{\"1\":{\"42\":1}}],[\"actual\",{\"1\":{\"40\":1}}],[\"active\",{\"1\":{\"17\":1}}],[\"acknowledge\",{\"1\":{\"17\":1}}],[\"adam\",{\"1\":{\"67\":2}}],[\"adjust\",{\"1\":{\"37\":3,\"40\":1}}],[\"additional\",{\"1\":{\"43\":1}}],[\"adding\",{\"1\":{\"30\":1,\"39\":2,\"42\":1}}],[\"added\",{\"1\":{\"39\":2,\"40\":3,\"42\":1,\"43\":1}}],[\"add\",{\"1\":{\"39\":1}}],[\"adds\",{\"1\":{\"34\":1}}],[\"adoption\",{\"1\":{\"17\":1}}],[\"advantage\",{\"0\":{\"107\":1},\"1\":{\"84\":1,\"106\":1}}],[\"advantages\",{\"1\":{\"17\":1}}],[\"advanced\",{\"1\":{\"43\":1}}],[\"advancement\",{\"1\":{\"17\":1}}],[\"advancing\",{\"0\":{\"16\":1,\"17\":1}}],[\"aspects\",{\"0\":{\"43\":1}}],[\"ask\",{\"1\":{\"40\":1}}],[\"assigning\",{\"1\":{\"33\":1}}],[\"assigned\",{\"1\":{\"33\":2}}],[\"associate\",{\"1\":{\"33\":1}}],[\"assuming\",{\"0\":{\"20\":1}}],[\"as\",{\"1\":{\"17\":3,\"34\":2,\"36\":2,\"39\":3,\"42\":1,\"43\":3}}],[\"about\",{\"1\":{\"17\":1}}],[\"ability\",{\"1\":{\"2\":1,\"43\":1}}],[\"a\",{\"0\":{\"16\":1,\"17\":1,\"35\":1,\"41\":1,\"86\":1,\"147\":1},\"1\":{\"14\":3,\"17\":14,\"20\":2,\"33\":11,\"34\":3,\"36\":6,\"37\":13,\"38\":2,\"39\":13,\"40\":5,\"41\":6,\"42\":9,\"43\":7,\"56\":1,\"61\":2,\"82\":1,\"103\":1,\"106\":1,\"107\":5,\"111\":2,\"112\":1,\"113\":1,\"114\":2,\"148\":1,\"149\":1,\"151\":2,\"153\":1,\"155\":2,\"156\":1,\"157\":4,\"159\":4}}],[\"alignment\",{\"1\":{\"135\":1}}],[\"alphazero\",{\"1\":{\"86\":1}}],[\"alphago\",{\"1\":{\"86\":1}}],[\"almost\",{\"1\":{\"36\":1}}],[\"always\",{\"1\":{\"34\":1}}],[\"also\",{\"1\":{\"34\":1,\"39\":1,\"41\":1,\"43\":1}}],[\"aleatoric\",{\"1\":{\"19\":1}}],[\"already\",{\"1\":{\"17\":1}}],[\"algorithm=aws4\",{\"1\":{\"79\":1}}],[\"algorithm\",{\"1\":{\"10\":1,\"37\":1,\"91\":1}}],[\"allow\",{\"1\":{\"43\":1}}],[\"allowing\",{\"1\":{\"43\":1}}],[\"allows\",{\"1\":{\"41\":1,\"42\":1,\"43\":1}}],[\"all\",{\"0\":{\"10\":1,\"20\":1},\"1\":{\"34\":2,\"40\":1,\"41\":1,\"43\":1}}],[\"ai=j​\",{\"1\":{\"157\":1}}],[\"aij​=∑k=1n​exp\",{\"1\":{\"148\":1}}],[\"ai\",{\"1\":{\"4\":1,\"8\":1,\"9\":1,\"90\":2}}],[\"api\",{\"1\":{\"4\":1}}],[\"applying\",{\"1\":{\"178\":1}}],[\"application\",{\"1\":{\"17\":1}}],[\"applications\",{\"1\":{\"2\":2,\"4\":1,\"8\":1,\"17\":1}}],[\"approach\",{\"1\":{\"37\":1}}],[\"appropriate\",{\"1\":{\"17\":3}}],[\"approximations\",{\"1\":{\"17\":1}}],[\"approximation是一种实用的近似方法\",{\"1\":{\"14\":1}}],[\"approximation是一种在统计学中常用的近似方法\",{\"1\":{\"14\":1}}],[\"approximation主要关注的是分布的峰值附近的信息\",{\"1\":{\"14\":1}}],[\"approximation可能无法提供一个准确的近似\",{\"1\":{\"14\":1}}],[\"approximation可能无法提供一个好的近似\",{\"1\":{\"14\":1}}],[\"approximation可以用于long\",{\"1\":{\"14\":1}}],[\"approximation的基本思想是在分布的最大值\",{\"1\":{\"14\":1}}],[\"approximation的计算效率高\",{\"1\":{\"14\":1}}],[\"approximation的方法用于神经网络的训练\",{\"1\":{\"14\":1}}],[\"approximation的具体实现\",{\"1\":{\"14\":1}}],[\"approximation\",{\"0\":{\"14\":1},\"1\":{\"10\":1,\"43\":1}}],[\"app\",{\"1\":{\"4\":3}}],[\"answer\",{\"1\":{\"174\":2}}],[\"animal\",{\"1\":{\"131\":5}}],[\"anchor\",{\"1\":{\"90\":1,\"93\":1}}],[\"any\",{\"1\":{\"36\":1,\"39\":1}}],[\"analogue\",{\"0\":{\"103\":1}}],[\"analogy\",{\"1\":{\"33\":1}}],[\"analysis\",{\"1\":{\"73\":1}}],[\"analyzes\",{\"1\":{\"41\":1,\"42\":1}}],[\"analyzed\",{\"1\":{\"17\":1}}],[\"analyzation\",{\"1\":{\"4\":1}}],[\"an\",{\"0\":{\"39\":1},\"1\":{\"33\":6,\"35\":1,\"36\":1,\"37\":1,\"39\":1,\"42\":3,\"43\":5}}],[\"and\",{\"0\":{\"13\":1,\"49\":1,\"94\":1},\"1\":{\"4\":3,\"17\":13,\"30\":2,\"33\":1,\"34\":2,\"35\":1,\"36\":2,\"37\":3,\"38\":2,\"39\":1,\"40\":6,\"41\":6,\"42\":4,\"43\":8,\"178\":1}}],[\"azure\",{\"1\":{\"2\":1,\"4\":4,\"8\":1}}],[\"aaronlzm\",{\"1\":{\"1\":1}}],[\"aaron\",{\"1\":{\"1\":1}}],[\"e2\",{\"1\":{\"152\":1}}],[\"equation\",{\"1\":{\"110\":2}}],[\"eqϕ​​\",{\"1\":{\"25\":1}}],[\"ema\",{\"1\":{\"97\":1}}],[\"embeddings\",{\"1\":{\"30\":1,\"43\":1}}],[\"embedding\",{\"0\":{\"177\":1},\"1\":{\"30\":1,\"43\":2,\"95\":1,\"161\":1,\"176\":1,\"184\":1}}],[\"ez∼p​\",{\"1\":{\"47\":1}}],[\"ez∼q​\",{\"1\":{\"46\":1,\"47\":1}}],[\"east\",{\"1\":{\"79\":1}}],[\"early\",{\"1\":{\"69\":1}}],[\"earlier\",{\"1\":{\"43\":1}}],[\"each\",{\"1\":{\"39\":2,\"40\":1}}],[\"elegant\",{\"1\":{\"38\":1}}],[\"elbo不仅仅是包含kl散度\",{\"1\":{\"48\":1}}],[\"elbo的核心地位\",{\"1\":{\"48\":1}}],[\"elbo的定义为\",{\"1\":{\"18\":1}}],[\"elbo就越大\",{\"1\":{\"46\":1}}],[\"elbo是证据\",{\"1\":{\"46\":1}}],[\"elbo定义为\",{\"1\":{\"12\":1}}],[\"elbo\",{\"1\":{\"12\":2,\"13\":1,\"18\":1,\"22\":1,\"44\":1,\"46\":1}}],[\"effectively\",{\"1\":{\"37\":1,\"43\":1}}],[\"effective\",{\"1\":{\"37\":1,\"43\":1}}],[\"exceptionally\",{\"1\":{\"43\":1}}],[\"exactly\",{\"1\":{\"39\":1,\"40\":1}}],[\"example\",{\"1\":{\"39\":1}}],[\"examples\",{\"1\":{\"33\":1}}],[\"exp\",{\"1\":{\"111\":1,\"148\":1,\"187\":2}}],[\"exploitation\",{\"1\":{\"103\":1}}],[\"exploration\",{\"1\":{\"103\":1}}],[\"expires=300\",{\"1\":{\"79\":1}}],[\"expansive\",{\"1\":{\"43\":1}}],[\"exponential\",{\"1\":{\"34\":1,\"97\":1}}],[\"expensive\",{\"1\":{\"17\":1}}],[\"expectation\",{\"1\":{\"10\":1}}],[\"expertise\",{\"0\":{\"8\":1}}],[\"experience\",{\"0\":{\"3\":1},\"1\":{\"2\":1}}],[\"ebm\",{\"0\":{\"199\":1},\"1\":{\"33\":1,\"35\":1}}],[\"ebms\",{\"0\":{\"32\":1},\"1\":{\"32\":1,\"34\":1,\"36\":1}}],[\"ever\",{\"1\":{\"37\":1}}],[\"every\",{\"1\":{\"33\":1,\"34\":1,\"39\":1,\"43\":1}}],[\"evaluate\",{\"1\":{\"34\":1}}],[\"evidence\",{\"1\":{\"12\":1,\"22\":1,\"45\":1}}],[\"epsilon\",{\"1\":{\"106\":1}}],[\"ep\",{\"1\":{\"24\":1}}],[\"epistemic\",{\"1\":{\"19\":1}}],[\"essence\",{\"1\":{\"41\":1}}],[\"essential\",{\"1\":{\"17\":1,\"43\":1}}],[\"estimator\",{\"1\":{\"25\":1}}],[\"estimation\",{\"1\":{\"18\":1,\"37\":1}}],[\"especially\",{\"1\":{\"17\":1,\"34\":1}}],[\"e\",{\"1\":{\"12\":2,\"33\":2,\"34\":11,\"36\":1,\"37\":5,\"39\":1,\"175\":1}}],[\"education\",{\"0\":{\"5\":1}}],[\"end\",{\"1\":{\"178\":1}}],[\"entropy\",{\"1\":{\"104\":2}}],[\"entirely\",{\"1\":{\"41\":1}}],[\"entire\",{\"1\":{\"33\":1,\"38\":1}}],[\"enabling\",{\"1\":{\"43\":1}}],[\"enhancements\",{\"1\":{\"43\":1}}],[\"ensuring\",{\"1\":{\"43\":1}}],[\"ensures\",{\"1\":{\"34\":1}}],[\"engine\",{\"1\":{\"42\":1}}],[\"engineering\",{\"1\":{\"4\":1}}],[\"engineer\",{\"1\":{\"2\":1}}],[\"energy\",{\"0\":{\"33\":1,\"36\":1},\"1\":{\"32\":1,\"33\":12,\"34\":6,\"35\":1,\"36\":4,\"37\":13}}],[\"encoding\",{\"1\":{\"30\":2}}],[\"encoder\",{\"0\":{\"29\":1},\"1\":{\"28\":1,\"29\":3,\"30\":5,\"43\":7,\"92\":2,\"97\":2,\"135\":1,\"160\":3}}],[\"environments\",{\"1\":{\"4\":1}}],[\"🎓\",{\"0\":{\"5\":1}}],[\"🎯\",{\"0\":{\"2\":1}}],[\"🌐\",{\"1\":{\"1\":1}}],[\"llama\",{\"1\":{\"172\":1}}],[\"llm\",{\"1\":{\"4\":1}}],[\"lstm\",{\"0\":{\"77\":1}}],[\"lsi\",{\"1\":{\"74\":1}}],[\"lsa\",{\"1\":{\"74\":1}}],[\"lr=1e\",{\"1\":{\"67\":1}}],[\"l2同等重要\",{\"1\":{\"70\":1}}],[\"l2相辅相成\",{\"1\":{\"69\":1}}],[\"l2正则化几乎等同于\",{\"1\":{\"67\":1}}],[\"l2正则化\",{\"0\":{\"67\":1},\"1\":{\"63\":1,\"64\":2,\"70\":1}}],[\"l2\",{\"0\":{\"58\":1},\"1\":{\"55\":2,\"56\":1,\"58\":1,\"60\":2,\"61\":5,\"62\":4,\"63\":3,\"65\":1,\"69\":1}}],[\"l1和l2正则化的思想是深度学习正则化技术的基石\",{\"1\":{\"70\":1}}],[\"l1在深度学习中也同样可用\",{\"1\":{\"68\":1}}],[\"l1正则化强度\",{\"1\":{\"68\":1}}],[\"l1正则化可以帮助凸显出最重要的输入路径\",{\"1\":{\"68\":1}}],[\"l1正则化\",{\"0\":{\"68\":1},\"1\":{\"63\":1,\"64\":1,\"70\":1}}],[\"l1\",{\"0\":{\"59\":1},\"1\":{\"55\":2,\"56\":2,\"59\":1,\"60\":2,\"61\":6,\"62\":4,\"63\":3,\"64\":1,\"65\":1,\"68\":5}}],[\"ln则是如左侧所示\",{\"1\":{\"174\":1}}],[\"ln和bn不同点是归一化的维度是互相垂直的\",{\"1\":{\"174\":1}}],[\"ln更多的应用在自然语言处理领域\",{\"1\":{\"174\":1}}],[\"ln是对单个样本进行归一化\",{\"1\":{\"174\":1}}],[\"ln\",{\"1\":{\"53\":1,\"174\":1}}],[\"literature\",{\"0\":{\"79\":1}}],[\"little\",{\"1\":{\"39\":1}}],[\"lies\",{\"1\":{\"36\":1}}],[\"link\",{\"1\":{\"43\":1}}],[\"linkedin\",{\"1\":{\"1\":1}}],[\"linearδ​\",{\"1\":{\"113\":1}}],[\"linear\",{\"1\":{\"30\":1,\"100\":1}}],[\"likely\",{\"1\":{\"33\":1}}],[\"likelihood\",{\"1\":{\"18\":1,\"37\":1,\"45\":1}}],[\"like\",{\"1\":{\"17\":2,\"33\":1,\"34\":1,\"36\":1,\"37\":1,\"39\":1,\"43\":2}}],[\"l\",{\"1\":{\"13\":1,\"18\":2,\"112\":1,\"115\":1,\"154\":1,\"157\":5,\"175\":1}}],[\"left\",{\"1\":{\"157\":2}}],[\"level\",{\"1\":{\"43\":6}}],[\"levels\",{\"1\":{\"33\":1}}],[\"less\",{\"1\":{\"41\":1,\"42\":1}}],[\"let\",{\"1\":{\"32\":1,\"34\":1,\"37\":1,\"42\":1}}],[\"learn\",{\"1\":{\"33\":2,\"38\":2}}],[\"learns\",{\"1\":{\"33\":1,\"37\":1,\"41\":1}}],[\"learning与深度神经网络结合\",{\"1\":{\"82\":1}}],[\"learning\",{\"0\":{\"16\":1,\"17\":1,\"40\":1,\"64\":1,\"194\":1,\"203\":1},\"1\":{\"6\":1,\"8\":1,\"17\":8,\"33\":1,\"36\":1,\"39\":1,\"40\":1,\"42\":1,\"82\":2,\"87\":1,\"88\":1,\"135\":1,\"190\":1}}],[\"lead\",{\"1\":{\"17\":1,\"43\":1}}],[\"leibler\",{\"1\":{\"12\":1,\"53\":1}}],[\"loop\",{\"1\":{\"114\":1}}],[\"looking\",{\"1\":{\"38\":1,\"41\":1}}],[\"local\",{\"1\":{\"103\":1}}],[\"locations\",{\"1\":{\"43\":1}}],[\"loss\",{\"1\":{\"43\":1,\"62\":3,\"68\":6,\"187\":2}}],[\"low\",{\"1\":{\"33\":4,\"37\":3,\"43\":1}}],[\"lower\",{\"1\":{\"12\":2,\"22\":1,\"37\":1,\"43\":1,\"46\":1}}],[\"long\",{\"1\":{\"14\":1,\"43\":1}}],[\"logq\",{\"1\":{\"46\":5}}],[\"logp\",{\"1\":{\"13\":1,\"18\":1,\"46\":13,\"48\":1}}],[\"log\",{\"1\":{\"12\":2,\"19\":2}}],[\"labels\",{\"1\":{\"186\":1}}],[\"latex\",{\"0\":{\"178\":1}}],[\"latent\",{\"1\":{\"73\":1}}],[\"lambda\",{\"1\":{\"68\":2}}],[\"lasso回归适用于深度学习\",{\"1\":{\"70\":1}}],[\"lasso回归\",{\"1\":{\"64\":1,\"65\":1,\"70\":1}}],[\"lasso\",{\"1\":{\"62\":2}}],[\"langle\",{\"1\":{\"178\":3}}],[\"langevin\",{\"1\":{\"37\":1}}],[\"languages\",{\"1\":{\"8\":1}}],[\"landscape\",{\"1\":{\"37\":2}}],[\"layernorm通过两个步骤实现这一目标\",{\"1\":{\"169\":1}}],[\"layernorm的成功主要来自于缩放操作\",{\"1\":{\"170\":1}}],[\"layernorm的痛点与rmsnorm的解决方案\",{\"0\":{\"168\":1}}],[\"layernorm的计算是\",{\"1\":{\"167\":1}}],[\"layernorm\",{\"1\":{\"161\":1,\"167\":1,\"172\":1}}],[\"layeroutput=layernorm\",{\"1\":{\"128\":1}}],[\"layers\",{\"0\":{\"208\":1},\"1\":{\"36\":1,\"43\":5}}],[\"layer\",{\"0\":{\"174\":1},\"1\":{\"30\":1,\"36\":1,\"164\":1,\"174\":4,\"175\":1}}],[\"large\",{\"1\":{\"17\":1}}],[\"laplace\",{\"0\":{\"14\":1},\"1\":{\"10\":1,\"14\":5}}],[\"lzm\",{\"1\":{\"1\":1}}],[\"|5\",{\"1\":{\"61\":1}}],[\"|10\",{\"1\":{\"61\":1}}],[\"|1\",{\"1\":{\"61\":2}}],[\"|4|\",{\"1\":{\"59\":1}}],[\"|3|\",{\"1\":{\"59\":1}}],[\"|vₙ|\",{\"1\":{\"59\":1}}],[\"|v₃|\",{\"1\":{\"59\":1}}],[\"|v₂|\",{\"1\":{\"59\":1}}],[\"|v₁|\",{\"1\":{\"59\":1}}],[\"||v||₁\",{\"1\":{\"59\":1}}],[\"||v||₂\",{\"1\":{\"58\":1}}],[\"||p\",{\"1\":{\"12\":2}}],[\"|\",{\"0\":{\"4\":1},\"1\":{\"1\":2,\"6\":1,\"7\":1}}],[\"ce\",{\"1\":{\"187\":1}}],[\"centering\",{\"1\":{\"169\":1,\"173\":1}}],[\"central\",{\"1\":{\"33\":1}}],[\"c表示通道轴\",{\"1\":{\"174\":1}}],[\"ck​\",{\"1\":{\"114\":1}}],[\"ck​=linearc​\",{\"1\":{\"113\":1}}],[\"cb\",{\"1\":{\"112\":1}}],[\"c∈r1×n\",{\"1\":{\"110\":1}}],[\"cc=hk\",{\"1\":{\"79\":1}}],[\"c1c2\",{\"1\":{\"79\":1}}],[\"cv\",{\"0\":{\"198\":1},\"1\":{\"74\":1}}],[\"curve\",{\"1\":{\"39\":1}}],[\"current\",{\"1\":{\"43\":1}}],[\"currently\",{\"1\":{\"17\":1,\"37\":1}}],[\"curriculum\",{\"0\":{\"1\":1}}],[\"clipping\",{\"1\":{\"106\":1}}],[\"clipped\",{\"0\":{\"106\":1}}],[\"cleaner\",{\"1\":{\"41\":1}}],[\"clean\",{\"1\":{\"40\":2,\"41\":1,\"42\":1}}],[\"clear\",{\"1\":{\"39\":2,\"41\":1}}],[\"cls\",{\"1\":{\"29\":1}}],[\"classic\",{\"0\":{\"102\":1}}],[\"classification\",{\"1\":{\"30\":1}}],[\"class\",{\"1\":{\"29\":4,\"30\":7,\"31\":1}}],[\"cloud\",{\"1\":{\"2\":2,\"4\":1,\"6\":1,\"8\":1}}],[\"critic或者更广泛的强化学习\",{\"1\":{\"103\":1}}],[\"critic中的\",{\"0\":{\"103\":1}}],[\"critic中这个问题的具体名称和表现形式有所不同\",{\"1\":{\"101\":1}}],[\"critic的实现\",{\"1\":{\"84\":1}}],[\"critic算法\",{\"1\":{\"84\":1,\"107\":1}}],[\"critic\",{\"0\":{\"84\":1,\"206\":1},\"1\":{\"84\":4,\"103\":1,\"104\":3}}],[\"criticality\",{\"1\":{\"17\":1}}],[\"critical\",{\"1\":{\"17\":1,\"42\":1,\"43\":1}}],[\"credential=asiaq3phcvtytzlo5zx7\",{\"1\":{\"79\":1}}],[\"create\",{\"1\":{\"37\":2,\"38\":2}}],[\"creating\",{\"1\":{\"17\":1}}],[\"cross\",{\"1\":{\"43\":2,\"131\":1}}],[\"crucial\",{\"1\":{\"34\":1,\"43\":1}}],[\"choices\",{\"1\":{\"17\":1}}],[\"choice\",{\"1\":{\"17\":3,\"43\":1}}],[\"chosen\",{\"1\":{\"17\":1,\"36\":1}}],[\"channels\",{\"1\":{\"43\":1}}],[\"characteristics\",{\"1\":{\"41\":1}}],[\"challenges\",{\"1\":{\"17\":2}}],[\"challenge\",{\"1\":{\"17\":5,\"34\":1}}],[\"chain\",{\"0\":{\"15\":1},\"1\":{\"10\":1,\"13\":2,\"17\":1,\"37\":1}}],[\"c\",{\"1\":{\"14\":1,\"30\":5,\"112\":1,\"113\":4,\"115\":1,\"175\":1}}],[\"ca2b\",{\"1\":{\"112\":1}}],[\"cab\",{\"1\":{\"112\":1}}],[\"case\",{\"0\":{\"102\":1}}],[\"cases\",{\"1\":{\"4\":1}}],[\"capture\",{\"1\":{\"36\":1,\"43\":2}}],[\"cal−1b\",{\"1\":{\"112\":1}}],[\"call\",{\"1\":{\"37\":1}}],[\"called\",{\"1\":{\"17\":1,\"37\":1,\"43\":1}}],[\"calculate\",{\"1\":{\"37\":3}}],[\"calculating\",{\"1\":{\"34\":1}}],[\"cat\",{\"1\":{\"33\":1,\"37\":1,\"39\":1}}],[\"cats\",{\"1\":{\"33\":1}}],[\"can\",{\"1\":{\"17\":7,\"33\":1,\"36\":2,\"37\":2,\"38\":1,\"39\":1,\"40\":2,\"41\":1,\"43\":2}}],[\"carlo方法等\",{\"1\":{\"14\":1}}],[\"carlo等\",{\"1\":{\"13\":1,\"15\":1}}],[\"carlo\",{\"0\":{\"15\":1},\"1\":{\"10\":1,\"13\":2,\"17\":1,\"37\":1}}],[\"cdot\",{\"1\":{\"157\":1}}],[\"cd\",{\"1\":{\"8\":1}}],[\"circ\",{\"1\":{\"157\":2}}],[\"circle\",{\"1\":{\"60\":1}}],[\"ci\",{\"1\":{\"8\":1}}],[\"cnn等传统检测器\",{\"1\":{\"93\":1}}],[\"cnn主干网络\",{\"1\":{\"92\":1}}],[\"cnn\",{\"1\":{\"6\":1,\"28\":1,\"31\":3,\"36\":1}}],[\"cos\",{\"1\":{\"178\":2}}],[\"cost\",{\"1\":{\"17\":2}}],[\"covariate\",{\"1\":{\"169\":1,\"175\":1}}],[\"collapsing\",{\"0\":{\"105\":1}}],[\"collapse\",{\"0\":{\"101\":1},\"1\":{\"101\":1}}],[\"coherent\",{\"1\":{\"40\":1,\"41\":1,\"42\":1}}],[\"countless\",{\"1\":{\"40\":1}}],[\"could\",{\"1\":{\"36\":1,\"39\":1}}],[\"course\",{\"1\":{\"32\":1}}],[\"courses\",{\"1\":{\"6\":1}}],[\"corrupted\",{\"1\":{\"39\":1}}],[\"corresponding\",{\"1\":{\"41\":1,\"43\":1}}],[\"correspond\",{\"1\":{\"37\":1}}],[\"correct\",{\"1\":{\"33\":1}}],[\"core\",{\"0\":{\"33\":1},\"1\":{\"34\":1,\"38\":1,\"42\":1}}],[\"conditioned\",{\"1\":{\"43\":2}}],[\"conditioning\",{\"1\":{\"43\":2}}],[\"convergence\",{\"1\":{\"103\":1}}],[\"converted\",{\"1\":{\"33\":1,\"43\":1}}],[\"convolutions\",{\"1\":{\"43\":1}}],[\"convolutional\",{\"1\":{\"36\":2,\"43\":2}}],[\"configurations\",{\"1\":{\"34\":1}}],[\"configuration\",{\"1\":{\"33\":1}}],[\"connections\",{\"1\":{\"43\":5}}],[\"connection\",{\"1\":{\"30\":1}}],[\"consists\",{\"1\":{\"43\":1}}],[\"constrained\",{\"1\":{\"17\":1,\"36\":1}}],[\"consequences\",{\"1\":{\"17\":1}}],[\"concat=\",{\"1\":{\"126\":1}}],[\"concat\",{\"1\":{\"121\":1,\"124\":2}}],[\"concept\",{\"0\":{\"33\":1}}],[\"concern\",{\"1\":{\"17\":1}}],[\"conclusion\",{\"1\":{\"17\":1}}],[\"conclusions\",{\"1\":{\"17\":1}}],[\"contain\",{\"1\":{\"43\":1}}],[\"contains\",{\"1\":{\"43\":1}}],[\"containerized\",{\"1\":{\"4\":1}}],[\"container\",{\"1\":{\"4\":1}}],[\"continue\",{\"1\":{\"41\":1}}],[\"continued\",{\"1\":{\"17\":1}}],[\"contracting\",{\"1\":{\"43\":1}}],[\"contrastive\",{\"1\":{\"37\":2}}],[\"controlled\",{\"1\":{\"39\":1}}],[\"contextual\",{\"1\":{\"43\":1}}],[\"context\",{\"1\":{\"17\":1,\"38\":1,\"43\":2,\"97\":1}}],[\"content\",{\"0\":{\"12\":1}}],[\"code\",{\"1\":{\"4\":1}}],[\"combine\",{\"1\":{\"43\":1}}],[\"comes\",{\"1\":{\"34\":1}}],[\"compression\",{\"1\":{\"68\":1,\"73\":1}}],[\"compressed\",{\"1\":{\"43\":2}}],[\"compare\",{\"1\":{\"40\":1}}],[\"compute\",{\"1\":{\"37\":1}}],[\"computationally\",{\"1\":{\"17\":2,\"34\":1}}],[\"computational\",{\"1\":{\"17\":4}}],[\"component\",{\"0\":{\"36\":1,\"37\":1},\"1\":{\"41\":1,\"42\":1}}],[\"components\",{\"1\":{\"35\":1}}],[\"completion\",{\"1\":{\"73\":1}}],[\"completely\",{\"1\":{\"39\":1}}],[\"complexity\",{\"1\":{\"17\":1}}],[\"complex\",{\"1\":{\"17\":2}}],[\"complicate\",{\"1\":{\"17\":1}}],[\"common\",{\"1\":{\"17\":1,\"37\":2}}],[\"com\",{\"1\":{\"1\":1,\"9\":2,\"77\":1,\"78\":1,\"79\":1,\"93\":1,\"174\":2}}],[\"✉️\",{\"1\":{\"1\":1}}]],\"version\":2}}")).map(([e,t])=>[e,jn(t,{fields:["h","t","c"],storeFields:["h","t","c"]})]));self.onmessage=({data:{type:e="all",query:t,locale:n,options:s,id:r}})=>{const o=An[n];e==="suggest"?self.postMessage([e,r,Et(t,o,s)]):e==="search"?self.postMessage([e,r,vt(t,o,s,"max")]):self.postMessage({suggestions:[e,r,Et(t,o,s)],results:[e,r,vt(t,o,s,__SLIMSEARCH_SORT_STRATEGY__)]})};
//# sourceMappingURL=index.js.map
