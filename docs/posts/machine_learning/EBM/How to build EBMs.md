---
 title: How to build EBMs 
---
Of course, let's break down Energy-Based Models (EBMs) step by step.

### Step 1: Understanding the Core Concept of Energy-Based Models

At its heart, an Energy-Based Model is a type of machine learning model that learns to associate a scalar "energy" value with every possible configuration of its input data. The central idea is inspired by physics:

*   **Low Energy = High Probability:** Data points that are similar to the data the model was trained on (i.e., they are "correct" or "plausible") are assigned a low energy score. In the analogy of a physical system, this is a stable, likely state.
*   **High Energy = Low Probability:** Data points that are unlike the training data (i.e., they are "incorrect" or "implausible") are assigned a high energy score. This represents an unstable, unlikely state.

Imagine you are training a model on images of cats. The EBM would learn an "energy function" that works like this:

*   **Input:** An image of a cat -> **Output:** Low energy
*   **Input:** An image of a dog -> **Output:** High energy
*   **Input:** An image of random static -> **Output:** Very high energy

The model isn't directly outputting a probability, but rather this energy score, which can then be converted into a probability. The key takeaway is that the model's entire job is to learn to distinguish between "good" and "bad" examples by assigning them different energy levels.

### Step 2: The "Theorem" - The Gibbs-Boltzmann Distribution

The "theorem" or mathematical principle that formalizes this relationship between energy and probability is the **Gibbs-Boltzmann distribution**, which comes from statistical mechanics. It states that the probability of a given data point `x`, denoted as `p(x)`, is related to its energy `E(x)` by the following formula:

**p(x) = (e ^ -E(x)) / Z**

Let's break down this formula:

*   **E(x):** This is the **energy function**. It's the core of your model, which takes a data point `x` and outputs its energy. As `E(x)` goes up, `e ^ -E(x)` goes down, meaning the probability `p(x)` also goes down.
*   **e ^ -E(x):** This is the exponential of the negative energy. This mathematical operation ensures that the resulting value is always positive.
*   **Z:** This is the **partition function**. It is the sum of `e ^ -E(x)` over *all possible* data configurations. Its purpose is to normalize the values so that the total probability of all possible inputs adds up to 1.

**The big challenge with EBMs is that calculating `Z` is often computationally impossible**, especially for high-dimensional data like images, because you would have to evaluate the energy of every possible image. This is a crucial point that dictates how we have to train these models.

### Step 3: How to Build a Model Based on This Principle

Building an EBM involves two main components: designing the energy function and then training it.

#### Component 1: Designing the Energy Function `E(x)`

The beauty of EBMs lies in their flexibility. The energy function `E(x)` can be almost any function that takes your data `x` as input and outputs a single scalar value (the energy). In modern deep learning, this function is typically a **neural network**.

*   **For Images:** You might use a Convolutional Neural Network (CNN). The input is an image, it passes through several convolutional layers, and the final output is a single number representing the energy.
*   **For Tabular Data:** A simple Multi-Layer Perceptron (MLP) could serve as the energy function.

The key is that you are not constrained by having to output a specific probability distribution (like in other models). The network's architecture can be chosen freely to best capture the features of your data.

#### Component 2: Training the Model

Since we can't calculate the partition function `Z`, we can't directly use maximum likelihood estimation to train the model. Instead, we train it using a **contrastive approach**. The goal is to shape the "energy landscape":

*   **Push Down** the energy of real data points (we want them to be low).
*   **Push Up** the energy of "fake" or "negative" data points (we want them to be high).

A very common and effective training algorithm for this is called **Contrastive Divergence**. Here’s a step-by-step intuition of how it works during one training step:

1.  **Pick a "Positive" Sample:** Take a real data point `x` from your training dataset (e.g., a real image of a cat).

2.  **Calculate its Energy:** Feed this real data point `x` through your neural network to get its energy, `E(x)`.

3.  **Adjust the Weights:** Using gradient descent, update the weights of your network to make `E(x)` lower. This is the "push down" part.

4.  **Generate a "Negative" Sample:** This is the tricky part. You need to create a "fake" sample that the model currently thinks is good (has low energy). A common way to do this is using a sampling technique like **Langevin Dynamics** or **Markov Chain Monte Carlo (MCMC)**. You start with a random data point and iteratively adjust it based on the model's own energy function until it settles into a low-energy state. Let's call this generated sample `x'`.

5.  **Calculate its Energy:** Feed this generated negative sample `x'` through your network to get its energy, `E(x')`.

6.  **Adjust the Weights Again:** Using gradient descent, update the weights of your network to make `E(x')` higher. This is the "push up" part.

By repeatedly performing these two steps—pulling the energy of real data down and pushing the energy of generated data up—the model learns to create an energy landscape where the valleys correspond to the real data distribution. This process effectively trains the model without ever needing to compute the intractable partition function `Z`.