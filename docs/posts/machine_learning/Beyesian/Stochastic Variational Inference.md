---
 title: Stochastic Variational Inference 
---
#Bayes 

使用随机变分推断（Stochastic Variational Inference, SVI）来更新深度学习中神经网络权重的过程涉及到几个步骤。以下是详细的步骤，包括相关的数学公式：

1. 初始化：首先，我们需要初始化网络的权重。对于一个具有$D$个权重的网络，我们可以初始化权重向量$\mathbf{w}$和变分分布的参数。这个变分分布通常是一个高斯分布$q(\mathbf{w};\mu,\sigma)$，其均值向量$\mu$和协方差矩阵$\sigma$也需要初始化。
    
2. 定义优化目标：在变分推断中，我们的目标是最大化证据下界（ELBO）。对于一个有$N$个样本的数据集$\mathcal{D}$，ELBO的定义为：
    
    $[ \mathcal{L}(\mu, \sigma) = \mathbb{E}_{q(\mathbf{w};\mu,\sigma)}[\log p(\mathcal{D}|\mathbf{w})] - KL(q(\mathbf{w};\mu,\sigma)||p(\mathbf{w}))] ]$
    
    其中，$p(\mathcal{D}|\mathbf{w})$是数据的似然函数，$p(\mathbf{w})$是权重的先验分布，$KL(\cdot||\cdot)$是KL散度，用来衡量变分分布和先验分布的相似程度。
    
3. 采样和前向传播：在每个训练步骤中，我们从变分分布$q(\mathbf{w};\mu,\sigma)$中采样权重$\mathbf{w}$，并使用这些权重来执行前向传播。具体地，我们使用重参数化技巧，即引入一个无参数的噪声向量$\epsilon$，使得$\mathbf{w} = \mu + \sigma \odot \epsilon$，其中$\odot$表示元素级的乘法。
    
4. 计算梯度：然后我们计算损失函数（即负ELBO）关于变分分布参数的梯度。具体地，我们计算$\frac{\partial (-\mathcal{L})}{\partial \mu}$和$\frac{\partial (-\mathcal{L})}{\partial \sigma}$。由于我们使用了重参数化技巧，所以可以直接通过反向传播来计算这些梯度。
    
5. 更新权重：最后，我们用计算出的梯度来更新变分分布的参数，通常使用随机梯度下降（SGD）或其变体。具体地，我们有：
    
    $[ \mu \leftarrow \mu - \alpha \frac{\partial (-\mathcal{L})}{\partial \mu} ] [ \sigma \leftarrow \sigma - \alpha \frac{\partial (-\mathcal{L})}{\partial \sigma} ]$
    
    其中，$\alpha$是学习率。
    
6. 重复步骤：重复步骤3-5直到满足停止条件，例如达到预定的迭代次数，或者ELBO收敛到一个稳定值。
    

以上就是在深度学习中使用SVI来更新神经网络权重的详细步骤和相关的数学公式。

---

在随机变分推断中，变分分布和权重的先验分布都是关于神经网络权重的概率分布，但它们的含义和作用是不同的。

1. 权重的先验分布：这是我们对权重在看到任何数据之前的知识或假设。它通常是由问题的背景知识和经验选择的。例如，我们可能会假设权重来自一个以零为中心的正态分布，这反映了我们的假设，即权重应该接近零但可以在两个方向上变化。在贝叶斯推断中，先验分布是必需的，因为它与似然函数相结合，形成了后验分布。
    
2. 变分分布：这是我们在变分推断中引入的一个新的分布，用来近似权重的真实后验分布。它是一个参数化的分布，其参数（例如均值和方差）由神经网络（推断网络）输出，这些参数在训练过程中会被优化。最优的变分分布将在KL散度意义下尽可能接近真实的后验分布。
    

在训练神经网络时，我们的目标是找到最优的变分分布，使得其最大化了ELBO（证据下界）。这意味着我们需要找到一组权重，使得它们能够在拟合数据（最大化数据的似然）和遵循先验（最小化与先验分布的KL散度）之间找到一个好的平衡。

---

似然函数是一个在统计学和机器学习中非常重要的概念。它描述了在给定模型参数下，观测到当前数据的概率。

假设我们有一个统计模型，模型参数为θ，观测数据为X。似然函数L(θ|X)是一个关于θ的函数，表示在给定参数θ的情况下，观测到数据X的概率。在许多情况下，我们可以将似然函数视为参数θ的函数，数据X是已知的。

例如，假设我们有一个从正态分布抽取的数据集X，正态分布的参数是均值μ和标准差σ。在这种情况下，似然函数是：

L(μ, σ|X) = Π_{i=1}{n} (1/σ√(2π)) e{-(x_i-μ)2 / (2σ2)}

这个公式表示了在给定均值μ和标准差σ的情况下，观测到数据集X的概率。

似然函数在参数估计中起着重要的作用。我们通常想要找到一组参数，使得似然函数最大，这就是最大似然估计（Maximum Likelihood Estimation, MLE）。在贝叶斯推断中，似然函数与先验分布相结合，形成了后验分布，这个后验分布描述了在观测到数据之后，参数的不确定性。

需要注意的是，虽然似然函数和概率密度函数看起来非常相似，但它们的角色和含义是不同的。概率密度函数描述了在给定参数的情况下，某个具体的观测值发生的概率，是对观测值的函数；而似然函数则是描述在给定观测值的情况下，参数取某个值的可能性，是对参数的函数。

---

