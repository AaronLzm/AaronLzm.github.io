---
 title: Bayes by backprop - naive variational inference 
---
#Bayes 
### GPT content:
"Bayes by Backprop"的核心是使用变分推理来近似权重的后验分布。这一过程涉及到一些复杂的数学，但我会尽力将其分解为详细的步骤。

假设我们的神经网络的权重是 w，我们的数据是 D，那么在贝叶斯推理中，我们希望计算权重的后验分布 p(w|D)。然而，这通常是非常困难的，因为这需要计算一个涉及到所有可能权重值的积分，这在实际中是不可行的。因此，我们使用变分推理来近似这个后验分布。

在变分推理中，我们引入一个变分分布 q(w|θ)，这个分布是由参数θ描述的，通常我们假设这个分布是高斯分布。我们的目标是找到最优的参数θ*，使得q(w|θ*)最接近真实的后验分布p(w|D)。

为了找到这个最优的参数θ*，我们需要优化一个叫做变分下界（Variational Lower Bound）或者证据下界（Evidence Lower Bound，ELBO）的量。ELBO定义为：

ELBO(θ) = E_q[log p(D|w)] - KL(q(w|θ)||p(w))

其中，E_q[log p(D|w)]是在变分分布q(w|θ)下，模型对数据的对数似然的期望，KL(q(w|θ)||p(w))是变分分布q(w|θ)和先验分布p(w)之间的KL散度（Kullback-Leibler Divergence）。

我们的目标是最大化这个ELBO。为了实现这个，我们需要计算ELBO关于θ的梯度，然后使用一个优化算法（比如随机梯度下降）来更新θ。计算梯度的过程就是所谓的反向传播。

在计算了梯度并更新了θ后，我们就得到了新的权重分布q(w|θ)。然后，我们可以在下一次前向传播时，从这个新的分布中采样权重。

这个过程会反复进行，每次迭代都会使得我们的权重分布更接近真实的后验分布，也就是使得ELBO更大。最终，我们得到的不仅是一组能使得预测性能最优的权重，还有一个描述权重不确定性的分布，这对于理解模型的不确定性，防止过拟合，以及在面对新的、未见过的数据时提供有用的不确定性信息是非常有价值的。

