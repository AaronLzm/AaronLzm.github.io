---
 title: What is uncertainty in neural networks 
---
#Bayes 

在深度神经网络中，不确定性通常分为两种类型：模型不确定性（或称为认知不确定性）和数据不确定性（或称为偶发不确定性）。

1. 模型不确定性（Epistemic Uncertainty）: 这是由于模型参数的不确定性导致的。例如，当我们的模型在训练数据上过拟合时，模型对于新的、未见过的数据可能会有很大的不确定性。这种不确定性可以通过收集更多的数据来减小。
    
    一个常用的方法来量化模型不确定性是贝叶斯神经网络（Bayesian Neural Networks），它将每个权重视为随机变量，从而引入了权重的分布。对于一个具有权重w的神经网络，后验分布可以表示为P(w|D)，其中D是训练数据。这个后验分布可以通过贝叶斯定理计算：
    
    P(w|D) = P(D|w)P(w) / P(D)
    
    在这个公式中，P(w|D)是权重w的后验分布，P(D|w)是在给定权重w的情况下数据D的似然性，P(w)是权重w的先验分布，P(D)是数据的证据（也称为边缘似然性）。我们可以使用随机变分推断或MCMC等方法来近似这个后验分布。
    
2. 数据不确定性（Aleatoric Uncertainty）: 这是由于数据本身的噪声或测量误差导致的。这种不确定性是固有的，不能通过收集更多的数据来减小。
    
    数据不确定性可以通过模型的预测分布来表示。例如，对于回归问题，我们可以假设预测值y遵循一个以f(x)为均值、σ2为方差的高斯分布。这里的σ2可以看作是数据不确定性的度量，通过最小化对数似然性来估计：
    
    log P(D|w) = Σ_{i} log P(y_i|f(x_i), w)
    
    在这个公式中，P(y_i|f(x_i), w)是在给定输入x_i和权重w的情况下，输出y_i的概率分布。我们可以通过最小化这个对数似然性来估计模型的参数和数据的不确定性。