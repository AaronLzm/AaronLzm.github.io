---
date: 2025-09-10 20:49:27
title: Norm 范数
category: Machine Learning
tags: [Machine Learning,mathematics]

---
好的，我们来一步一步地理解 L1 和 L2 Norm（范数）这两个非常重要且基础的概念。

简单来说，**Norm（范数）** 是一个数学工具，用来**衡量向量的大小或长度**。你可以把它想象成一把测量“多维空间中的距离”的尺子。L1 和 L2 是这把尺子上两种最常见的刻度。

---

### 1. 从直观理解开始：从点到点的路径

想象一下你在一个城市里，要从点 A 走到点 B。

*   **L2 Norm (欧几里得距离)**：就像一只鸟**直线飞过去**的距离。这是我们所认为的“最短距离”。
    *   在城市地图上，这就是连接两点的直线长度。
*   **L1 Norm (曼哈顿距离)**：就像你**沿着城市的街道网格走路**，只能直角转弯。你走过的总路程就是 L1 距离。
    *   它得名于纽约曼哈顿的棋盘式街道布局，你不能斜着穿过大楼，只能沿着街道走。



---

### 2. 数学定义

现在我们用数学公式来精确地定义它们。假设我们有一个向量 `v = [v₁, v₂, v₃, ..., vₙ]`，它有 n 个维度。

#### L2 Norm (欧几里得范数)
这是我们最熟悉的“长度”公式，是勾股定理在多维空间的扩展。
**计算公式：**
`||v||₂ = √(v₁² + v₂² + v₃² + ... + vₙ²)`

**计算示例：**
向量 `v = [3, 4]`
它的 L2 Norm 是：`√(3² + 4²) = √(9 + 16) = √25 = 5`
这正好是一个 3-4-5 直角三角形的斜边长度。

#### L1 Norm (曼哈顿范数)
这是向量所有维度**绝对值**的总和。
**计算公式：**
`||v||₁ = |v₁| + |v₂| + |v₃| + ... + |vₙ|`

**计算示例：**
同样向量 `v = [3, 4]`
它的 L1 Norm 是：`|3| + |4| = 3 + 4 = 7`
这就是在网格上从 (0,0) 走到 (3,4) 需要走的最短路径（例如，先向东走 3 格，再向北走 4 格）。

---

### 3. 几何意义：单位圆

一个非常好的方式来可视化 L1 和 L2 的区别是看它们的**单位圆（Unit Circle）**——即所有“长度为 1”的点构成的图形。

*   **L2 单位圆**：所有到原点距离为 1 的点。这形成了一个我们熟悉的、光滑的圆形（在 3D 中是球体）。
*   **L1 单位圆**：所有到原点曼哈顿距离为 1 的点。这形成了一个菱形（或正菱形）。

从下图可以清晰地看到两者的几何差异：



---

### 4. 关键特性与对比

| 特性 | L1 Norm (曼哈顿距离) | L2 Norm (欧几里得距离) |
| :--- | :--- | :--- |
| **计算方式** | 绝对值之和 | 平方和的平方根 |
| **几何形状** | 菱形（有尖点） | 圆形（光滑） |
| **稳健性** | **对异常值更不敏感（更稳健）** | 对异常值**非常敏感** |
| **稀疏性** | **倾向于产生稀疏解**（很多系数为0） | 倾向于产生**稠密解**（系数都很小但非零） |
| **计算效率** | 计算简单（只需加绝对值） | 计算涉及平方和开方，稍复杂 |

**为什么 L1 对异常值更不敏感？**
想象两个点：
*   A: (1, 1)
*   B: (1, 5)

*   **L2 距离**：`√((1-1)² + (5-1)²) = √(0 + 16) = 4`
*   **L1 距离**：`|1-1| + |5-1| = 0 + 4 = 4`

现在把 B 点变成一个异常值：B: (1, 10)
*   **L2 距离**：`√((1-1)² + (10-1)²) = √(0 + 81) = 9`  （变化了 5）
*   **L1 距离**：`|1-1| + |10-1| = 0 + 9 = 9`  （变化了 5）

看起来一样？我们再夸张一点，把 A 点放在原点 (0,0)，B 点放在 (1, 100)：
*   **L2 距离**：`√(1² + 100²) ≈ 100.005` **（距离几乎完全由巨大的异常值 100 决定）**
*   **L1 距离**：`1 + 100 = 101` **（巨大值 100 和 小值 1 的贡献是线性叠加的）**

L2 因为计算了平方，会极大地放大异常值的影响。而 L1 只是线性地处理每个维度，所以某个维度上的巨大值不会完全主导整个距离的计算。

---

### 5. 在机器学习和数据科学中的应用

正是基于以上特性，L1 和 L2 在不同的场景下各有千秋。

1.  **作为损失函数 (Loss Function)**
    *   **L2 Loss (MSE, 均方误差)**：最常用。它的光滑性使得求解梯度更容易，便于使用梯度下降法等优化算法。但对异常值敏感。
    *   **L1 Loss (MAE, 平均绝对误差)**：当你希望模型对数据中的异常值不敏感时使用（例如，金融数据中偶尔出现的极端波动）。

2.  **作为正则化器 (Regularizer)**
    这是两者最重要的应用之一，用于防止模型过拟合。
    *   **L2 正则化 (岭回归, Ridge Regression)**：在损失函数中加入模型权重的 **L2 Norm** 作为惩罚项。它倾向于让**所有的权重都变小**，但不会让任何一个权重直接变为 0。模型会更平滑、更稳定。
    *   **L1 正则化 (Lasso 回归, Lasso Regression)**：在损失函数中加入模型权重的 **L1 Norm** 作为惩罚项。它强大的特性是**倾向于产生稀疏解**，即它会自动进行“特征选择”，把不重要的特征的权重**直接压缩到 0**。这对于处理高维数据非常有用。

### 总结

| | L1 Norm | L2 Norm |
| :--- | :--- | :--- |
| **绰号** | 曼哈顿距离、 Taxicab Norm | 欧几里得距离、 “普通”距离 |
| **计算** | 绝对值之和 | 平方和的平方根 |
| **形状** | 菱形 | 圆形 |
| **特点** | 对异常值稳健、**产生稀疏性** | 对异常值敏感、解更平滑 |
| **主要用途**| L1 损失(MAE)、**L1正则化(特征选择)** | L2 损失(MSE)、**L2正则化(防止过拟合)** |

希望这个从直观到数学的解释能帮助你彻底理解 L1 和 L2 Norm 的区别！� L1 和 L2 Norm 的区别！