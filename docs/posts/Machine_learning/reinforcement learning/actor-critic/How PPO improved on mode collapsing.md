---
date: 2025-09-10 20:49:27
title: How PPO improved on mode collapsing
category: Machine Learning
tags: [Machine Learning,actor-critic]

---
非常好，这个问题直击了PPO算法设计的精髓。PPO通过其核心机制**间接地、但非常有效地**应对了策略上的“模式坍塌”（即过早收敛到局部最优）。

它并非像熵正则化那样**直接**奖励多样性，而是通过一种更根本的方式——**保证优化过程的稳定性**——来避免策略过早地变得狭隘和确定。

PPO主要通过以下两个相互关联的机制来做到这一点：

---

### 1. 核心机制：截断的替代目标 (Clipped Surrogate Objective)

这是PPO的灵魂。它的目标是：**在不让新策略偏离旧策略太远的前提下，尽可能地提升策略。**

让我们把它拆解开来看：

*   **策略比例 (Probability Ratio) `r_t(θ)`**：
    这个值的计算方法是：`r_t = π_θ(a|s) / π_θ_old(a|s)`。
    *   `π_θ(a|s)` 是**新策略**（正在优化的策略）在状态 `s` 下选择动作 `a` 的概率。
    *   `π_θ_old(a|s)` 是**旧策略**（本次更新开始前的策略）的概率。
    *   如果 `r_t > 1`，说明这个动作在新策略下变得更可能被选中了。
    *   如果 `r_t < 1`，说明这个动作在新策略下变得更不可能被选中了。

*   **截断 (Clipping)**：
    PPO设置了一个超参数 `ε` (epsilon，通常是0.1或0.2)。然后，它将策略比例 `r_t` 限制在一个**安全区间**内： `[1 - ε, 1 + ε]`。
    *   这意味着，即使某个动作的“优势”（Advantage）非常高，PPO也不允许新策略将选中该动作的概率无限拔高。最多只能提升到旧策略的 `1 + ε` 倍。
    *   反之，即使一个动作的优势非常差，PPO也不允许新策略过度惩罚它，最多将概率降低到旧策略的 `1 - ε` 倍。

**这个“截断”机制如何防止模式坍塌？**

想象一下，Actor（演员）偶然发现了一个“还不错”的动作，这个动作带来了比预期要好得多的奖励（即高优势）。

*   **没有PPO的普通策略梯度**：会像打了兴奋剂一样，疯狂地增加这个动作被选中的概率，可能在一次更新后，策略就从“50%概率选A”变成了“99%概率选A”。策略迅速收敛，**探索性急剧下降**，这就是模式坍塌。它再也没有机会去发现可能比A更好的动作B了。

*   **有PPO**：当新策略试图将动作A的概率大幅提升时，`r_t` 会迅速增长并超过 `1 + ε`。此时，**截断机制会介入**，将`r_t`强行“按回”到 `1 + ε`。这意味着，本次更新对这个动作的奖励是**有上限的**。策略只能从“50%概率选A”温和地变成，比如说，“60%概率选A”。

**结论：PPO通过限制单次更新的步长，强制策略进行“小步慢跑”式的优化，而不是“百米冲刺”。这种温和的更新方式，使得策略在更长的时间内保持其随机性和探索性，大大降低了过早陷入局部最优（模式坍塌）的风险。**

---

### 2. 优势函数 (Advantage Function) 的使用

PPO作为一种Actor-Critic算法，它不只是看奖励的绝对值，而是使用**优势函数 A(s, a)** 来指导策略更新。

*   `A(s, a) = Q(s, a) - V(s)`
*   它衡量的不是“动作a好不好”，而是“**在状态s下，动作a比平均水平好多少**”。

这提供了一个更稳定、方差更低的基线。它避免了智能体仅仅因为某个状态本身价值很高，就错误地加强在该状态下采取的所有动作。通过关注“超出预期的好坏”，优势函数为策略更新提供了更精确的信号，这有助于引导Actor进行更有效的探索，而不是简单地固化任何能带来正奖励的行为。

### 总结：PPO与模式坍塌的关系

我们可以用一个比喻来理解：

*   一个**激进的登山者（普通策略梯度）**，一旦发现一条看起来向上的路，就会不顾一切地冲上去，结果很可能被困在一个小山峰上（局部最优），错过了通往主峰（全局最优）的真正路径。

*   一个**谨慎的登山者（PPO）**，他每次只向上走一小段固定的距离（由`ε`限制）。即使他发现了一条看似很好的路，他也会小步前进，并不断地重新评估周围的环境。这种谨慎的策略让他不容易被困住，有更多的机会发现并转向那条通往真正顶峰的道路。

因此，PPO通过其核心的**截断机制**来维持训练的**稳定性**，而这种稳定性又**间接地保留了策略的探索性**，从而成为了对抗策略“模式坍塌”的一道坚固防线。在实际应用中，PPO的目标函数里通常还会**显式地加入熵正则化项**，双管齐下，进一步鼓励探索，让策略更加稳健。��，进一步鼓励探索，让策略更加稳健。